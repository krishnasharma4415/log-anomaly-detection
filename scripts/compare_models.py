"""
Model Comparison: Hierarchical Transformer vs Federated Contrastive Learning

This script provides a side-by-side comparison of the two models
to help understand their differences and choose the right one.
"""

import sys
from pathlib import Path

def print_comparison():
    """Print detailed comparison of both models"""
    
    print("="*100)
    print(" " * 30 + "MODEL COMPARISON")
    print("="*100)
    
    print("\n" + "="*100)
    print("1. ARCHITECTURE COMPARISON")
    print("="*100)
    
    print("\nâ”Œâ”€ Hierarchical Transformer (HLogFormer)")
    print("â”‚")
    print("â”‚  Raw Log Text")
    print("â”‚      â†“")
    print("â”‚  [BERT Encoder] â† Frozen first 6 layers")
    print("â”‚      â†“")
    print("â”‚  [Template Extraction] â† Drain3 algorithm")
    print("â”‚      â†“")
    print("â”‚  [Template Embeddings] â† Learnable embeddings")
    print("â”‚      â†“")
    print("â”‚  [Template-Aware Attention] â† Multi-head attention")
    print("â”‚      â†“")
    print("â”‚  [Temporal LSTM] â† Bidirectional, 2 layers")
    print("â”‚      â†“")
    print("â”‚  [Source Adapters] â† Domain-specific adaptation")
    print("â”‚      â†“")
    print("â”‚  [Classification Head] â† Binary classification")
    print("â”‚")
    print("â”‚  Auxiliary Tasks:")
    print("â”‚  â”œâ”€ [Source Discriminator] â† Adversarial training")
    print("â”‚  â””â”€ [Template Classifier] â† Template prediction")
    print("â”‚")
    print("â””â”€ Output: Anomaly prediction + learned representations")
    
    print("\nâ”Œâ”€ Federated Contrastive Learning (FedLogCL)")
    print("â”‚")
    print("â”‚  Raw Log Text (Multiple Clients)")
    print("â”‚      â†“")
    print("â”‚  [Template Extraction] â† Drain3 algorithm")
    print("â”‚      â†“")
    print("â”‚  [Contrastive Pair Generation]")
    print("â”‚      â”œâ”€ Positive pairs (same label)")
    print("â”‚      â”œâ”€ Negative pairs (different labels)")
    print("â”‚      â”œâ”€ Template-based pairs")
    print("â”‚      â””â”€ Minority augmentation")
    print("â”‚      â†“")
    print("â”‚  [BERT Encoder] â† Fine-tuned")
    print("â”‚      â†“")
    print("â”‚  [Projection Head] â† Contrastive projection")
    print("â”‚      â†“")
    print("â”‚  [Template-Aware Attention] â† Multi-head attention")
    print("â”‚      â†“")
    print("â”‚  [Contrastive Learning] â† InfoNCE + alignment")
    print("â”‚      â†“")
    print("â”‚  [Classification Head] â† Binary classification")
    print("â”‚      â†“")
    print("â”‚  [Federated Aggregation] â† Weighted averaging")
    print("â”‚")
    print("â””â”€ Output: Global model + client embeddings")
    
    print("\n" + "="*100)
    print("2. FEATURE ENGINEERING COMPARISON")
    print("="*100)
    
    comparison_table = [
        ("Feature", "Hierarchical Transformer", "Federated Contrastive"),
        ("-" * 30, "-" * 30, "-" * 35),
        ("Template Extraction", "âœ“ Drain3", "âœ“ Drain3"),
        ("Template Embeddings", "âœ“ Learnable", "âœ“ Learnable"),
        ("Timestamp Features", "âœ“ Normalized + LSTM", "âœ— Not used"),
        ("Contrastive Pairs", "âœ— Not used", "âœ“ Positive/Negative/Template"),
        ("Source Features", "âœ“ Source ID + Adapters", "âœ“ Client-specific"),
        ("Minority Augmentation", "âœ— Not used", "âœ“ 3x oversampling"),
        ("Temporal Modeling", "âœ“ Bidirectional LSTM", "âœ— Not used"),
        ("Domain Adaptation", "âœ“ Source adapters", "âœ“ Federated aggregation"),
    ]
    
    for row in comparison_table:
        print(f"{row[0]:<30} | {row[1]:<30} | {row[2]:<35}")
    
    print("\n" + "="*100)
    print("3. TRAINING STRATEGY COMPARISON")
    print("="*100)
    
    training_table = [
        ("Aspect", "Hierarchical Transformer", "Federated Contrastive"),
        ("-" * 30, "-" * 30, "-" * 35),
        ("Training Mode", "Centralized", "Federated (multi-client)"),
        ("Data Sharing", "All data in one place", "No raw data sharing"),
        ("Privacy", "Standard", "Privacy-preserving"),
        ("Batch Processing", "Standard batches", "Contrastive pairs"),
        ("Optimization", "Single optimizer", "Per-client optimizers"),
        ("Aggregation", "N/A", "Weighted by size/templates/imbalance"),
        ("Learning Rate", "Single LR: 2e-5", "Dual LR: 2e-5 (encoder), 1e-3 (head)"),
        ("Epochs", "5 epochs", "10 rounds Ã— 1 local epoch"),
        ("Early Stopping", "âœ“ Patience=3", "âœ“ Patience=3"),
    ]
    
    for row in training_table:
        print(f"{row[0]:<30} | {row[1]:<30} | {row[2]:<35}")
    
    print("\n" + "="*100)
    print("4. LOSS FUNCTION COMPARISON")
    print("="*100)
    
    print("\nâ”Œâ”€ Hierarchical Transformer Loss")
    print("â”‚")
    print("â”‚  Total Loss = Î±â‚Â·L_classification + Î±â‚‚Â·L_template + Î±â‚ƒÂ·L_temporal + Î±â‚„Â·L_source")
    print("â”‚")
    print("â”‚  where:")
    print("â”‚  â€¢ L_classification = Focal Loss (handles imbalance)")
    print("â”‚  â€¢ L_template = Cross-Entropy (template prediction)")
    print("â”‚  â€¢ L_temporal = Consistency Loss (smooth transitions)")
    print("â”‚  â€¢ L_source = Cross-Entropy (adversarial source prediction)")
    print("â”‚")
    print("â”‚  Weights: Î±â‚=1.0, Î±â‚‚=0.3, Î±â‚ƒ=0.2, Î±â‚„=0.1")
    print("â”‚")
    print("â””â”€ Multi-task learning with 4 objectives")
    
    print("\nâ”Œâ”€ Federated Contrastive Loss")
    print("â”‚")
    print("â”‚  Total Loss = Î»â‚Â·L_contrastive + Î»â‚‚Â·L_focal + Î»â‚ƒÂ·L_template")
    print("â”‚")
    print("â”‚  where:")
    print("â”‚  â€¢ L_contrastive = InfoNCE + Alignment (representation learning)")
    print("â”‚  â€¢ L_focal = Focal Loss (handles imbalance)")
    print("â”‚  â€¢ L_template = BCE (template alignment)")
    print("â”‚")
    print("â”‚  Weights: Î»â‚=0.5, Î»â‚‚=0.3, Î»â‚ƒ=0.2")
    print("â”‚")
    print("â””â”€ Contrastive learning with 3 objectives")
    
    print("\n" + "="*100)
    print("5. PERFORMANCE CHARACTERISTICS")
    print("="*100)
    
    perf_table = [
        ("Metric", "Hierarchical Transformer", "Federated Contrastive"),
        ("-" * 30, "-" * 30, "-" * 35),
        ("Training Time (Test)", "~5-10 minutes", "~10-15 minutes"),
        ("Training Time (Full)", "~2-4 hours", "~4-8 hours"),
        ("GPU Memory", "~6-8 GB", "~6-8 GB"),
        ("Model Parameters", "~110M", "~110M"),
        ("Inference Speed", "Fast (single forward)", "Fast (single forward)"),
        ("Scalability", "Single machine", "Distributed clients"),
        ("Best For", "Single deployment", "Multi-source privacy"),
    ]
    
    for row in perf_table:
        print(f"{row[0]:<30} | {row[1]:<30} | {row[2]:<35}")
    
    print("\n" + "="*100)
    print("6. USE CASE RECOMMENDATIONS")
    print("="*100)
    
    print("\nâœ“ Choose HIERARCHICAL TRANSFORMER when:")
    print("  â€¢ You have centralized access to all data")
    print("  â€¢ Temporal patterns are important (time-series logs)")
    print("  â€¢ You need source-specific adaptation")
    print("  â€¢ You want adversarial domain adaptation")
    print("  â€¢ Privacy is not a primary concern")
    print("  â€¢ You have logs from multiple sources but can combine them")
    print("\n  Example: Enterprise monitoring with centralized log aggregation")
    
    print("\nâœ“ Choose FEDERATED CONTRASTIVE when:")
    print("  â€¢ Data is distributed across multiple clients")
    print("  â€¢ Privacy is a requirement (can't share raw logs)")
    print("  â€¢ You want to learn from multiple organizations")
    print("  â€¢ You need strong representation learning")
    print("  â€¢ Class imbalance is severe")
    print("  â€¢ You want to leverage contrastive learning benefits")
    print("\n  Example: Multi-organization collaboration without data sharing")
    
    print("\n" + "="*100)
    print("7. HYPERPARAMETER SUMMARY")
    print("="*100)
    
    print("\nâ”Œâ”€ Hierarchical Transformer")
    print("â”‚  MAX_SEQ_LEN = 128")
    print("â”‚  BATCH_SIZE = 16")
    print("â”‚  NUM_EPOCHS = 5")
    print("â”‚  LEARNING_RATE = 2e-5")
    print("â”‚  FREEZE_BERT_LAYERS = 6")
    print("â”‚  ALPHA_CLASSIFICATION = 1.0")
    print("â”‚  ALPHA_TEMPLATE = 0.3")
    print("â”‚  ALPHA_TEMPORAL = 0.2")
    print("â”‚  ALPHA_SOURCE = 0.1")
    print("â””â”€")
    
    print("\nâ”Œâ”€ Federated Contrastive")
    print("â”‚  MAX_LENGTH = 64")
    print("â”‚  BATCH_SIZE = 32")
    print("â”‚  NUM_ROUNDS = 10")
    print("â”‚  LOCAL_EPOCHS = 1")
    print("â”‚  LR_ENCODER = 2e-5")
    print("â”‚  LR_HEAD = 1e-3")
    print("â”‚  PROJECTION_DIM = 128")
    print("â”‚  LAMBDA_CONTRASTIVE = 0.5")
    print("â”‚  LAMBDA_FOCAL = 0.3")
    print("â”‚  LAMBDA_TEMPLATE = 0.2")
    print("â”‚  TEMPERATURE = 0.07")
    print("â””â”€")
    
    print("\n" + "="*100)
    print("8. QUICK START COMMANDS")
    print("="*100)
    
    print("\n# Test Hierarchical Transformer (5-10 min)")
    print("python demo/demo_hierarchical_transformer.py")
    
    print("\n# Test Federated Contrastive (10-15 min)")
    print("python demo/demo_federated_contrastive.py")
    
    print("\n# For full training, edit the scripts and set:")
    print("TEST_MODE = False")
    
    print("\n" + "="*100)
    print("9. OUTPUT FILES")
    print("="*100)
    
    print("\nâ”Œâ”€ Hierarchical Transformer")
    print("â”‚  results/demo_hlogformer/")
    print("â”‚  â”œâ”€â”€ demo_results_TIMESTAMP.pkl")
    print("â”‚  models/demo_hlogformer/")
    print("â”‚  â””â”€â”€ best_model.pt")
    print("â””â”€")
    
    print("\nâ”Œâ”€ Federated Contrastive")
    print("â”‚  results/demo_fedlogcl/")
    print("â”‚  â”œâ”€â”€ demo_results_TIMESTAMP.pkl")
    print("â”‚  â”œâ”€â”€ test_embeddings_TIMESTAMP.npy")
    print("â”‚  models/demo_fedlogcl/")
    print("â”‚  â””â”€â”€ best_model.pt")
    print("â””â”€")
    
    print("\n" + "="*100)
    print("10. KEY TAKEAWAYS")
    print("="*100)
    
    print("\nğŸ“Š Hierarchical Transformer:")
    print("   âœ“ Best for centralized deployment")
    print("   âœ“ Strong temporal modeling")
    print("   âœ“ Source-specific adaptation")
    print("   âœ“ Multi-task learning")
    print("   âœ— Requires centralized data")
    
    print("\nğŸ” Federated Contrastive:")
    print("   âœ“ Privacy-preserving")
    print("   âœ“ Distributed training")
    print("   âœ“ Strong representation learning")
    print("   âœ“ Handles severe imbalance")
    print("   âœ— More complex setup")
    
    print("\n" + "="*100)
    print("For detailed documentation, see: demo/README_HIERARCHICAL_FEDERATED.md")
    print("="*100 + "\n")

if __name__ == "__main__":
    print_comparison()
