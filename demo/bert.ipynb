{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a0ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456345cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "MODELS_PATH = ROOT / \"models\" / \"bert_models\"\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}\n",
    "\n",
    "BERT_CONFIG = {\n",
    "    'max_length': 128,\n",
    "    'batch_size': 32,\n",
    "    'model_name': 'bert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd21e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log(text):\n",
    "    \"\"\"Preprocess log text to normalize patterns\"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Replace common patterns\n",
    "    text = re.sub(r'[0-9a-f]{8,}', '<HEX>', text)  # Hex IDs\n",
    "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '<IP>', text)  # IP addresses\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # Dates\n",
    "    text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', '<TIME>', text)  # Times\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Numbers\n",
    "    text = re.sub(r'[^\\w\\s<>]', ' ', text)  # Remove special chars except <>\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7782442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_logs(log_texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize log texts using BERT tokenizer\n",
    "    \n",
    "    Args:\n",
    "        log_texts: List of log messages\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        input_ids, attention_masks\n",
    "    \"\"\"\n",
    "    print(f\"Tokenizing {len(log_texts)} log entries...\")\n",
    "    \n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_log(text) for text in log_texts]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        processed_texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encodings['input_ids'], encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa74984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(model_type='logbert'):\n",
    "    \"\"\"\n",
    "    Load trained BERT model\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of BERT model ('logbert', 'dapt_bert', 'deberta_v3', 'mpnet')\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    model_file = MODELS_PATH / f\"{model_type}_best_model.pt\"\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        print(f\"Warning: Model file not found: {model_file}\")\n",
    "        print(\"Using pre-trained BERT for demo purposes...\")\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer, None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    \n",
    "    print(f\"Loaded BERT model: {model_type.upper()}\")\n",
    "    print(f\"Training F1-Macro: {checkpoint.get('best_f1', 'N/A')}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    model_name = checkpoint.get('model_name', 'bert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Note: Full model loading requires the exact architecture\n",
    "    # For demo, we'll use the pre-trained model\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5b1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_bert(log_texts, model, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions using BERT model\n",
    "    \n",
    "    Args:\n",
    "        log_texts: List of log messages\n",
    "        model: BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        predictions, probabilities, confidence, embeddings\n",
    "    \"\"\"\n",
    "    print(\"Making predictions with BERT...\")\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids, attention_masks = tokenize_logs(log_texts, tokenizer, max_length)\n",
    "    \n",
    "    # Get embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size].to(device)\n",
    "            batch_attention_masks = attention_masks[i:i+batch_size].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_masks,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Use [CLS] token embedding\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    print(f\"Extracted BERT embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    # Simple heuristic-based classification for demo\n",
    "    # In production, use the trained classifier head\n",
    "    print(\"\\nWarning: Using heuristic-based classification for demo\")\n",
    "    print(\"For production, load the trained classifier head\\n\")\n",
    "    \n",
    "    # Calculate anomaly scores based on embedding statistics\n",
    "    embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "    embedding_mean = embeddings.mean(axis=0)\n",
    "    distances_from_mean = np.linalg.norm(embeddings - embedding_mean, axis=1)\n",
    "    \n",
    "    # Normalize scores\n",
    "    anomaly_scores = (distances_from_mean - distances_from_mean.min()) / (distances_from_mean.max() - distances_from_mean.min() + 1e-8)\n",
    "    \n",
    "    # Check for error keywords in original text\n",
    "    error_keywords = ['error', 'critical', 'fail', 'exception', 'timeout', 'crash']\n",
    "    keyword_scores = np.array([\n",
    "        sum(1 for keyword in error_keywords if keyword in text.lower()) / len(error_keywords)\n",
    "        for text in log_texts\n",
    "    ])\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = 0.6 * anomaly_scores + 0.4 * keyword_scores\n",
    "    combined_scores = np.clip(combined_scores, 0, 1)\n",
    "    \n",
    "    predictions = (combined_scores > 0.5).astype(int)\n",
    "    probabilities = np.column_stack([1 - combined_scores, combined_scores])\n",
    "    confidence = np.max(probabilities, axis=1)\n",
    "    \n",
    "    return predictions, probabilities[:, 1], confidence, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ad0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(log_data, predictions, probabilities, confidence, \n",
    "                   content_column='Content', top_n=10):\n",
    "    \"\"\"Display prediction results\"\"\"\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    df['Prediction'] = predictions\n",
    "    df['Prediction_Label'] = df['Prediction'].map(LABEL_MAP)\n",
    "    df['Anomaly_Probability'] = probabilities\n",
    "    df['Confidence'] = confidence\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total logs analyzed: {len(df)}\")\n",
    "    print(f\"Normal logs: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anomalous logs: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {confidence.mean():.3f}\")\n",
    "    \n",
    "    if (predictions == 1).sum() > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TOP {min(top_n, (predictions == 1).sum())} ANOMALIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        anomalies = df[df['Prediction'] == 1].sort_values('Anomaly_Probability', ascending=False).head(top_n)\n",
    "        \n",
    "        for idx, row in anomalies.iterrows():\n",
    "            print(f\"\\n[{idx}] Probability: {row['Anomaly_Probability']:.3f}, Confidence: {row['Confidence']:.3f}\")\n",
    "            print(f\"Log: {row[content_column][:200]}...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef64483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_bert_prediction(custom_logs, content_column='Content', model_type='logbert',\n",
    "                        max_length=128, batch_size=32, show_top_n=10):\n",
    "    \"\"\"\n",
    "    Main demo function for BERT model prediction\n",
    "    \n",
    "    Args:\n",
    "        custom_logs: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        model_type: Type of BERT model ('logbert', 'dapt_bert', 'deberta_v3', 'mpnet')\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for inference\n",
    "        show_top_n: Number of top anomalies to display\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and probabilities\n",
    "        embeddings: BERT embeddings for the logs\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"BERT MODEL ANOMALY DETECTION DEMO ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert to list if needed\n",
    "    if isinstance(custom_logs, pd.DataFrame):\n",
    "        log_texts = custom_logs[content_column].tolist()\n",
    "    else:\n",
    "        log_texts = custom_logs\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer, checkpoint = load_bert_model(model_type)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, probabilities, confidence, embeddings = predict_with_bert(\n",
    "        log_texts, model, tokenizer, max_length, batch_size\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    results_df = display_results(\n",
    "        custom_logs, predictions, probabilities, confidence, \n",
    "        content_column, show_top_n\n",
    "    )\n",
    "    \n",
    "    return results_df, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a5ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE: Predicting on custom log messages with BERT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Testing with LOGBERT model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BERT MODEL ANOMALY DETECTION DEMO (LOGBERT)\n",
      "================================================================================\n",
      "Warning: Model file not found: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\bert_models\\logbert_best_model.pt\n",
      "Using pre-trained BERT for demo purposes...\n",
      "Making predictions with BERT...\n",
      "Tokenizing 15 log entries...\n",
      "Extracted BERT embeddings: (15, 768)\n",
      "\n",
      "Warning: Using heuristic-based classification for demo\n",
      "For production, load the trained classifier head\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 15\n",
      "Normal logs: 12 (80.0%)\n",
      "Anomalous logs: 3 (20.0%)\n",
      "Average confidence: 0.724\n",
      "\n",
      "================================================================================\n",
      "TOP 3 ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "[13] Probability: 0.733, Confidence: 0.733\n",
      "Log: ERROR: Failed to parse configuration file - invalid JSON...\n",
      "\n",
      "[5] Probability: 0.586, Confidence: 0.586\n",
      "Log: ERROR: Null pointer exception in module UserService.processRequest...\n",
      "\n",
      "[14] Probability: 0.535, Confidence: 0.535\n",
      "Log: INFO: Service health check passed...\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\bert\\bert_logbert_predictions.csv\n",
      "✓ Embeddings saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\bert_logbert_embeddings.npy\n",
      "\n",
      "Embedding statistics:\n",
      "  Shape: (15, 768)\n",
      "  Mean: -0.0090\n",
      "  Std: 0.5159\n",
      "  Min: -8.0076\n",
      "  Max: 3.8513\n",
      "\n",
      "================================================================================\n",
      "Testing with DAPT_BERT model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BERT MODEL ANOMALY DETECTION DEMO (DAPT_BERT)\n",
      "================================================================================\n",
      "Warning: Model file not found: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\bert_models\\dapt_bert_best_model.pt\n",
      "Using pre-trained BERT for demo purposes...\n",
      "Making predictions with BERT...\n",
      "Tokenizing 15 log entries...\n",
      "Extracted BERT embeddings: (15, 768)\n",
      "\n",
      "Warning: Using heuristic-based classification for demo\n",
      "For production, load the trained classifier head\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 15\n",
      "Normal logs: 12 (80.0%)\n",
      "Anomalous logs: 3 (20.0%)\n",
      "Average confidence: 0.724\n",
      "\n",
      "================================================================================\n",
      "TOP 3 ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "[13] Probability: 0.733, Confidence: 0.733\n",
      "Log: ERROR: Failed to parse configuration file - invalid JSON...\n",
      "\n",
      "[5] Probability: 0.586, Confidence: 0.586\n",
      "Log: ERROR: Null pointer exception in module UserService.processRequest...\n",
      "\n",
      "[14] Probability: 0.535, Confidence: 0.535\n",
      "Log: INFO: Service health check passed...\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\bert\\bert_dapt_bert_predictions.csv\n",
      "✓ Embeddings saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\bert_dapt_bert_embeddings.npy\n",
      "\n",
      "Embedding statistics:\n",
      "  Shape: (15, 768)\n",
      "  Mean: -0.0090\n",
      "  Std: 0.5159\n",
      "  Min: -8.0076\n",
      "  Max: 3.8513\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: Predicting on custom log messages with BERT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_logs = [\n",
    "        \"INFO: Application started successfully at port 8080\",\n",
    "        \"ERROR: Connection timeout after 30 seconds to database server\",\n",
    "        \"WARNING: Memory usage at 85% threshold exceeded\",\n",
    "        \"CRITICAL: Database connection failed - max retries reached\",\n",
    "        \"INFO: User authentication successful for user john.doe\",\n",
    "        \"ERROR: Null pointer exception in module UserService.processRequest\",\n",
    "        \"INFO: Data processing completed in 2.5 seconds\",\n",
    "        \"ALERT: Disk space critically low - only 5% remaining\",\n",
    "        \"INFO: HTTP request processed successfully in 120ms\",\n",
    "        \"ERROR: Authentication failed for user admin - invalid credentials\",\n",
    "        \"WARNING: High CPU usage detected - 95% utilization\",\n",
    "        \"INFO: Scheduled backup completed successfully\",\n",
    "        \"CRITICAL: Out of memory error in worker thread\",\n",
    "        \"ERROR: Failed to parse configuration file - invalid JSON\",\n",
    "        \"INFO: Service health check passed\"\n",
    "    ]\n",
    "    \n",
    "    # Test with different BERT models\n",
    "    for model_type in ['logbert', 'dapt_bert']:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Testing with {model_type.upper()} model\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results, embeddings = demo_bert_prediction(\n",
    "            sample_logs, \n",
    "            content_column='Content',\n",
    "            model_type=model_type,\n",
    "            max_length=128,\n",
    "            batch_size=8,\n",
    "            show_top_n=5\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        output_file = ROOT / \"demo\" / \"results\" / \"bert\" / f\"bert_{model_type}_predictions.csv\"\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        results.to_csv(output_file, index=False)\n",
    "        print(f\"\\n✓ Results saved to: {output_file}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_file = ROOT / \"demo\" / f\"bert_{model_type}_embeddings.npy\"\n",
    "        np.save(embeddings_file, embeddings)\n",
    "        print(f\"✓ Embeddings saved to: {embeddings_file}\")\n",
    "        \n",
    "        print(f\"\\nEmbedding statistics:\")\n",
    "        print(f\"  Shape: {embeddings.shape}\")\n",
    "        print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "        print(f\"  Std: {embeddings.std():.4f}\")\n",
    "        print(f\"  Min: {embeddings.min():.4f}\")\n",
    "        print(f\"  Max: {embeddings.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9914d1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_Label</th>\n",
       "      <th>Anomaly_Probability</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO: Application started successfully at port...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.282889</td>\n",
       "      <td>0.717111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERROR: Connection timeout after 30 seconds to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.364187</td>\n",
       "      <td>0.635813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING: Memory usage at 85% threshold exceeded</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.118079</td>\n",
       "      <td>0.881921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITICAL: Database connection failed - max ret...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.350850</td>\n",
       "      <td>0.649150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO: User authentication successful for user ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.137551</td>\n",
       "      <td>0.862449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERROR: Null pointer exception in module UserSe...</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.586244</td>\n",
       "      <td>0.586244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFO: Data processing completed in 2.5 seconds</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.243837</td>\n",
       "      <td>0.756163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALERT: Disk space critically low - only 5% rem...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.305592</td>\n",
       "      <td>0.694408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFO: HTTP request processed successfully in 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.298031</td>\n",
       "      <td>0.701969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ERROR: Authentication failed for user admin - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.250551</td>\n",
       "      <td>0.749449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WARNING: High CPU usage detected - 95% utiliza...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INFO: Scheduled backup completed successfully</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.318197</td>\n",
       "      <td>0.681803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CRITICAL: Out of memory error in worker thread</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.331486</td>\n",
       "      <td>0.668514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ERROR: Failed to parse configuration file - in...</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INFO: Service health check passed</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.534587</td>\n",
       "      <td>0.534587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Content  Prediction  \\\n",
       "0   INFO: Application started successfully at port...           0   \n",
       "1   ERROR: Connection timeout after 30 seconds to ...           0   \n",
       "2     WARNING: Memory usage at 85% threshold exceeded           0   \n",
       "3   CRITICAL: Database connection failed - max ret...           0   \n",
       "4   INFO: User authentication successful for user ...           0   \n",
       "5   ERROR: Null pointer exception in module UserSe...           1   \n",
       "6      INFO: Data processing completed in 2.5 seconds           0   \n",
       "7   ALERT: Disk space critically low - only 5% rem...           0   \n",
       "8   INFO: HTTP request processed successfully in 1...           0   \n",
       "9   ERROR: Authentication failed for user admin - ...           0   \n",
       "10  WARNING: High CPU usage detected - 95% utiliza...           0   \n",
       "11      INFO: Scheduled backup completed successfully           0   \n",
       "12     CRITICAL: Out of memory error in worker thread           0   \n",
       "13  ERROR: Failed to parse configuration file - in...           1   \n",
       "14                  INFO: Service health check passed           1   \n",
       "\n",
       "   Prediction_Label  Anomaly_Probability  Confidence  \n",
       "0            normal             0.282889    0.717111  \n",
       "1            normal             0.364187    0.635813  \n",
       "2            normal             0.118079    0.881921  \n",
       "3            normal             0.350850    0.649150  \n",
       "4            normal             0.137551    0.862449  \n",
       "5           anomaly             0.586244    0.586244  \n",
       "6            normal             0.243837    0.756163  \n",
       "7            normal             0.305592    0.694408  \n",
       "8            normal             0.298031    0.701969  \n",
       "9            normal             0.250551    0.749449  \n",
       "10           normal             0.000000    1.000000  \n",
       "11           normal             0.318197    0.681803  \n",
       "12           normal             0.331486    0.668514  \n",
       "13          anomaly             0.733333    0.733333  \n",
       "14          anomaly             0.534587    0.534587  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
