{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05639d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model on cuda...\n",
      "✓ BERT model loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from feature_extractor import extract_features_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ab235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout=0.3, num_classes=2):\n",
    "        super(FocalLossNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(prev_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64, hidden_dims=[256, 128]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(prev_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            recon, _, _ = self.forward(x)\n",
    "            error = torch.mean((x - recon) ** 2, dim=1)\n",
    "        return error\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm(x + self.dropout(attn_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN1DWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2, embed_dim=128, num_heads=4, dropout=0.3):\n",
    "        super(CNN1DWithAttention, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, embed_dim, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(embed_dim)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(16)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_dim * 16, 256)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class GhostBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GhostBatchNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            chunks = x.chunk(max(1, x.size(0) // self.virtual_batch_size), dim=0)\n",
    "            res = [self.bn(chunk) for chunk in chunks]\n",
    "            return torch.cat(res, dim=0)\n",
    "        else:\n",
    "            return self.bn(x)\n",
    "\n",
    "\n",
    "class TabNetEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_steps=3, n_shared=2, n_independent=2, \n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.initial_bn = GhostBatchNorm(input_dim, virtual_batch_size, momentum)\n",
    "        self.initial_fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        self.shared_layers = nn.ModuleList([\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "            for i in range(n_shared)\n",
    "        ])\n",
    "        \n",
    "        self.step_layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.Linear(output_dim, output_dim)\n",
    "                for j in range(n_independent)\n",
    "            ])\n",
    "            for _ in range(n_steps)\n",
    "        ])\n",
    "        \n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.Linear(output_dim, input_dim)\n",
    "            for _ in range(n_steps)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x_orig = self.initial_bn(x)\n",
    "        prior_scales = torch.ones_like(x_orig)\n",
    "        \n",
    "        outputs = []\n",
    "        for step in range(self.n_steps):\n",
    "            masked_x = x_orig * prior_scales\n",
    "            h = F.relu(self.initial_fc(masked_x))\n",
    "            \n",
    "            for layer in self.shared_layers:\n",
    "                h = F.relu(layer(h))\n",
    "            \n",
    "            for layer in self.step_layers[step]:\n",
    "                h = F.relu(layer(h))\n",
    "            \n",
    "            outputs.append(h)\n",
    "            \n",
    "            if step < self.n_steps - 1:\n",
    "                attn = self.attention_layers[step](h)\n",
    "                attn = torch.mul(attn, prior_scales)\n",
    "                attn = torch.softmax(attn, dim=-1)\n",
    "                prior_scales = torch.mul(prior_scales, (1 - attn))\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2, n_steps=3, n_shared=2, n_independent=2,\n",
    "                 output_dim=64, virtual_batch_size=128):\n",
    "        super(TabNet, self).__init__()\n",
    "        \n",
    "        self.encoder = TabNetEncoder(\n",
    "            input_dim, output_dim, n_steps, n_shared, n_independent, virtual_batch_size\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(output_dim * n_steps, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        logits = self.classifier(encoded)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StackedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):\n",
    "        super(StackedAutoencoder, self).__init__()\n",
    "        \n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            encoder_layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        decoder_layers = []\n",
    "        for hidden_dim in reversed(hidden_dims[:-1]):\n",
    "            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            decoder_layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "\n",
    "class StackedAEClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], num_classes=2, dropout=0.2):\n",
    "        super(StackedAEClassifier, self).__init__()\n",
    "        \n",
    "        self.autoencoder = StackedAutoencoder(input_dim, hidden_dims, dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_reconstruction=False):\n",
    "        decoded, encoded = self.autoencoder(x)\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        if return_reconstruction:\n",
    "            return logits, decoded\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2, d_model=128, nhead=8, \n",
    "                 num_layers=3, dim_feedforward=512, dropout=0.1, seq_len=16):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, d_model * seq_len)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * seq_len, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.input_projection(x)\n",
    "        x = x.view(batch_size, self.seq_len, self.d_model)\n",
    "        \n",
    "        x = x + self.pos_encoder\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.reshape(batch_size, -1)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class LogAnomalyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88bc212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "MODELS_PATH = ROOT / \"models\" / \"dl_models\"\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f58dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'flnn': FocalLossNN,\n",
    "    'vae': VAE,\n",
    "    'cnn': CNN1DWithAttention,\n",
    "    'cnn_attention': CNN1DWithAttention,\n",
    "    'tabnet': TabNet,\n",
    "    'stacked_ae': StackedAEClassifier,\n",
    "    'transformer': TransformerEncoder\n",
    "}\n",
    "\n",
    "def load_dl_model(model_name='flnn', input_dim=200):\n",
    "    \"\"\"\n",
    "    Load trained DL model with actual architecture and weights\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model ('flnn', 'vae', 'cnn', 'tabnet', 'stacked_ae', 'transformer')\n",
    "        input_dim: Input feature dimension (default: 200)\n",
    "    \n",
    "    Returns:\n",
    "        model, checkpoint (or None if not found)\n",
    "    \"\"\"\n",
    "    # Try different possible file locations\n",
    "    possible_paths = [\n",
    "        MODELS_PATH / f\"{model_name}_best_model.pt\",\n",
    "        MODELS_PATH / f\"{model_name}_checkpoint.pth\",\n",
    "        MODELS_PATH / \"deployment\" / f\"best_dl_model_{model_name}.pth\",\n",
    "        MODELS_PATH / \"deployment\" / f\"{model_name}_checkpoint.pth\",\n",
    "    ]\n",
    "    \n",
    "    model_file = None\n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            model_file = path\n",
    "            break\n",
    "    \n",
    "    if model_file is None:\n",
    "        print(f\"⚠️  Model file not found for '{model_name}'\")\n",
    "        print(f\"   Searched in:\")\n",
    "        for path in possible_paths:\n",
    "            print(f\"   - {path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    try:\n",
    "        checkpoint = torch.load(model_file, map_location=device)\n",
    "        print(f\"✓ Loaded checkpoint from: {model_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading checkpoint: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get model class\n",
    "    if model_name not in MODEL_CLASSES:\n",
    "        print(f\"❌ Unknown model name: {model_name}\")\n",
    "        print(f\"   Available models: {list(MODEL_CLASSES.keys())}\")\n",
    "        return None, None\n",
    "    \n",
    "    model_class = MODEL_CLASSES[model_name]\n",
    "    \n",
    "    # Initialize model with correct architecture\n",
    "    try:\n",
    "        if model_name == 'flnn':\n",
    "            model = model_class(input_dim=input_dim, hidden_dims=[512, 256, 128], dropout=0.3, num_classes=2)\n",
    "        elif model_name == 'vae':\n",
    "            model = model_class(input_dim=input_dim, latent_dim=64, hidden_dims=[256, 128])\n",
    "        elif model_name in ['cnn', 'cnn_attention']:\n",
    "            model = model_class(input_dim=input_dim, num_classes=2, embed_dim=128, num_heads=4, dropout=0.3)\n",
    "        elif model_name == 'tabnet':\n",
    "            model = model_class(input_dim=input_dim, num_classes=2, n_steps=3, n_shared=2, n_independent=2, output_dim=64)\n",
    "        elif model_name == 'stacked_ae':\n",
    "            model = model_class(input_dim=input_dim, hidden_dims=[256, 128, 64], num_classes=2, dropout=0.2)\n",
    "        elif model_name == 'transformer':\n",
    "            model = model_class(input_dim=input_dim, num_classes=2, d_model=128, nhead=8, num_layers=3, dropout=0.1, seq_len=16)\n",
    "        else:\n",
    "            model = model_class(input_dim=input_dim, num_classes=2)\n",
    "        \n",
    "        print(f\"✓ Initialized {model_name.upper()} architecture\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing model: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            # Checkpoint might be the state dict itself\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"✓ Loaded trained weights\")\n",
    "        print(f\"✓ Model ready for inference on {device}\")\n",
    "        \n",
    "        # Print model info if available\n",
    "        if 'val_f1' in checkpoint:\n",
    "            print(f\"   Validation F1: {checkpoint['val_f1']:.4f}\")\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"   Trained epochs: {checkpoint['epoch'] + 1}\")\n",
    "        \n",
    "        return model, checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading weights: {e}\")\n",
    "        print(f\"   This might be due to architecture mismatch\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def predict_with_dl_model(model, X, device, model_name='flnn'):\n",
    "    \"\"\"\n",
    "    Make predictions using actual trained DL model\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded PyTorch model\n",
    "        X: Feature matrix (numpy array)\n",
    "        device: torch device\n",
    "        model_name: Name of the model\n",
    "    \n",
    "    Returns:\n",
    "        predictions, probabilities, confidence\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"⚠️  No model provided, using heuristic predictions\")\n",
    "        # Fallback to heuristic\n",
    "        error_features = X[:, -5:] if X.shape[1] >= 5 else X\n",
    "        anomaly_scores = error_features.sum(axis=1) / max(error_features.shape[1], 1)\n",
    "        anomaly_scores = np.clip(anomaly_scores, 0, 1)\n",
    "        predictions = (anomaly_scores > 0.5).astype(int)\n",
    "        probabilities = np.column_stack([1 - anomaly_scores, anomaly_scores])\n",
    "        confidence = np.max(probabilities, axis=1)\n",
    "        return predictions, probabilities[:, 1], confidence\n",
    "    \n",
    "    # Ensure correct feature dimensions\n",
    "    expected_features = 200\n",
    "    if X.shape[1] < expected_features:\n",
    "        padding = np.zeros((X.shape[0], expected_features - X.shape[1]))\n",
    "        X_padded = np.hstack([X, padding])\n",
    "    elif X.shape[1] > expected_features:\n",
    "        X_padded = X[:, :expected_features]\n",
    "    else:\n",
    "        X_padded = X\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = LogAnomalyDataset(X_padded, np.zeros(len(X_padded)))\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(f\"Making predictions with {model_name.upper()} model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            if model_name == 'vae':\n",
    "                # VAE uses reconstruction error\n",
    "                errors = model.get_reconstruction_error(X_batch)\n",
    "                # Use threshold (95th percentile as default)\n",
    "                threshold = torch.quantile(errors, 0.95)\n",
    "                preds = (errors > threshold).long()\n",
    "                # Create pseudo-probabilities\n",
    "                normalized_errors = torch.clamp(errors / (threshold * 2), 0, 1)\n",
    "                probs = torch.stack([1 - normalized_errors, normalized_errors], dim=1)\n",
    "            elif model_name == 'stacked_ae':\n",
    "                # Stacked AE returns logits\n",
    "                logits = model(X_batch, return_reconstruction=False)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            else:\n",
    "                # Standard classification models\n",
    "                logits = model(X_batch)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Extract anomaly probabilities and confidence\n",
    "    anomaly_probs = all_probs[:, 1]\n",
    "    confidence = np.max(all_probs, axis=1)\n",
    "    \n",
    "    return all_preds, anomaly_probs, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3681f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalies(log_data, content_column='Content', timestamp_column=None,\n",
    "                     model_name='flnn', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict anomalies in custom log data using DL model with FULL feature extraction\n",
    "    \n",
    "    Args:\n",
    "        log_data: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        timestamp_column: Name of the column containing timestamps (optional)\n",
    "        model_name: Name of the DL model to use\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        predictions, probabilities, confidence\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTRACTING FEATURES USING FULL PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"This includes:\")\n",
    "    print(\"  ✓ BERT embeddings (768-dim)\")\n",
    "    print(\"  ✓ Drain3 template parsing\")\n",
    "    print(\"  ✓ Statistical features (rolling windows, outliers)\")\n",
    "    print(\"  ✓ Error pattern detection (15+ patterns)\")\n",
    "    print(\"  ✓ Temporal features\")\n",
    "    print(\"  ✓ Text complexity features\")\n",
    "    print(\"  ✓ Feature selection (top 200 features)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Extract features using FULL pipeline\n",
    "    X, scaler = extract_features_for_prediction(\n",
    "        log_data, \n",
    "        content_column, \n",
    "        timestamp_column,\n",
    "        feature_variant='selected_imbalanced'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {X.shape[1]} features (matching training pipeline)\")\n",
    "    \n",
    "    # Load model with actual architecture and weights\n",
    "    print(f\"\\nLoading {model_name.upper()} model...\")\n",
    "    model, checkpoint = load_dl_model(model_name, input_dim=X.shape[1])\n",
    "    \n",
    "    # Make predictions using actual model\n",
    "    predictions, probabilities, confidence = predict_with_dl_model(model, X, device, model_name)\n",
    "    \n",
    "    # Apply threshold\n",
    "    if threshold != 0.5:\n",
    "        predictions = (probabilities >= threshold).astype(int)\n",
    "        print(f\"Applied custom threshold: {threshold:.3f}\")\n",
    "    \n",
    "    return predictions, probabilities, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f48c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(log_data, predictions, probabilities, confidence, \n",
    "                   content_column='Content', top_n=10):\n",
    "    \"\"\"Display prediction results\"\"\"\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    df['Prediction'] = predictions\n",
    "    df['Prediction_Label'] = df['Prediction'].map(LABEL_MAP)\n",
    "    df['Anomaly_Probability'] = probabilities\n",
    "    df['Confidence'] = confidence\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total logs analyzed: {len(df)}\")\n",
    "    print(f\"Normal logs: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anomalous logs: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {confidence.mean():.3f}\")\n",
    "    \n",
    "    if (predictions == 1).sum() > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TOP {min(top_n, (predictions == 1).sum())} ANOMALIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        anomalies = df[df['Prediction'] == 1].sort_values('Anomaly_Probability', ascending=False).head(top_n)\n",
    "        \n",
    "        for idx, row in anomalies.iterrows():\n",
    "            print(f\"\\n[{idx}] Probability: {row['Anomaly_Probability']:.3f}, Confidence: {row['Confidence']:.3f}\")\n",
    "            print(f\"Log: {row[content_column][:200]}...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39998629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dl_prediction(custom_logs, content_column='Content', model_name='flnn',\n",
    "                      threshold=0.5, show_top_n=10):\n",
    "    \"\"\"\n",
    "    Main demo function for DL model prediction\n",
    "    \n",
    "    Args:\n",
    "        custom_logs: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        model_name: Name of the DL model ('flnn', 'vae', 'cnn', 'tabnet', 'stacked_ae', 'transformer')\n",
    "        threshold: Classification threshold\n",
    "        show_top_n: Number of top anomalies to display\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DEEP LEARNING MODEL ANOMALY DETECTION DEMO ({model_name.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    predictions, probabilities, confidence = predict_anomalies(\n",
    "        custom_logs, content_column, timestamp_column=None, \n",
    "        model_name=model_name, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    results_df = display_results(\n",
    "        custom_logs, predictions, probabilities, confidence, \n",
    "        content_column, show_top_n\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba515fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE: Predicting on custom log messages\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Testing with CNN_ATTENTION model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DEEP LEARNING MODEL ANOMALY DETECTION DEMO (CNN_ATTENTION)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 10 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (10, 9)\n",
      "✓ Error features: (10, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (10, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 10 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/10 logs\n",
      "✓ BERT embeddings: (10, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (10, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (10, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features (matching training pipeline)\n",
      "\n",
      "Loading CNN_ATTENTION model...\n",
      "✓ Loaded checkpoint from: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\deployment\\best_dl_model_cnn_attention.pth\n",
      "✓ Initialized CNN_ATTENTION architecture\n",
      "✓ Loaded trained weights\n",
      "✓ Model ready for inference on cuda\n",
      "Making predictions with CNN_ATTENTION model...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 10\n",
      "Normal logs: 9 (90.0%)\n",
      "Anomalous logs: 1 (10.0%)\n",
      "Average confidence: 1.000\n",
      "\n",
      "================================================================================\n",
      "TOP 1 ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "[3] Probability: 1.000, Confidence: 1.000\n",
      "Log: CRITICAL: Database connection failed...\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\dl\\dl_cnn_attention_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "Testing with FLNN model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DEEP LEARNING MODEL ANOMALY DETECTION DEMO (FLNN)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 10 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (10, 9)\n",
      "✓ Error features: (10, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (10, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 10 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/10 logs\n",
      "✓ BERT embeddings: (10, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (10, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (10, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features (matching training pipeline)\n",
      "\n",
      "Loading FLNN model...\n",
      "⚠️  Model file not found for 'flnn'\n",
      "   Searched in:\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\flnn_best_model.pt\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\flnn_checkpoint.pth\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\deployment\\best_dl_model_flnn.pth\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\deployment\\flnn_checkpoint.pth\n",
      "⚠️  No model provided, using heuristic predictions\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 10\n",
      "Normal logs: 10 (100.0%)\n",
      "Anomalous logs: 0 (0.0%)\n",
      "Average confidence: 0.803\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\dl\\dl_flnn_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "Testing with TABNET model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DEEP LEARNING MODEL ANOMALY DETECTION DEMO (TABNET)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 10 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (10, 9)\n",
      "✓ Error features: (10, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (10, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 10 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/10 logs\n",
      "✓ BERT embeddings: (10, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (10, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (10, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features (matching training pipeline)\n",
      "\n",
      "Loading TABNET model...\n",
      "⚠️  Model file not found for 'tabnet'\n",
      "   Searched in:\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\tabnet_best_model.pt\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\tabnet_checkpoint.pth\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\deployment\\best_dl_model_tabnet.pth\n",
      "   - C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\dl_models\\deployment\\tabnet_checkpoint.pth\n",
      "⚠️  No model provided, using heuristic predictions\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 10\n",
      "Normal logs: 10 (100.0%)\n",
      "Anomalous logs: 0 (0.0%)\n",
      "Average confidence: 0.803\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\dl\\dl_tabnet_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: Predicting on custom log messages\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_logs = [\n",
    "        \"INFO: Application started successfully\",\n",
    "        \"ERROR: Connection timeout after 30 seconds\",\n",
    "        \"WARNING: Memory usage at 85%\",\n",
    "        \"CRITICAL: Database connection failed\",\n",
    "        \"INFO: User login successful\",\n",
    "        \"ERROR: Null pointer exception in module X\",\n",
    "        \"INFO: Processing completed\",\n",
    "        \"ALERT: Disk space critically low\",\n",
    "        \"INFO: Request processed in 120ms\",\n",
    "        \"ERROR: Authentication failed for user admin\"\n",
    "    ]\n",
    "    \n",
    "    # Test with different models\n",
    "    # Note: cnn_attention model is available in deployment folder\n",
    "    for model_name in ['cnn_attention', 'flnn', 'tabnet']:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Testing with {model_name.upper()} model\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results = demo_dl_prediction(\n",
    "            sample_logs, \n",
    "            content_column='Content',\n",
    "            model_name=model_name,\n",
    "            threshold=0.5,\n",
    "            show_top_n=5\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        output_file = ROOT / \"demo\" / \"results\" / \"dl\" / f\"dl_{model_name}_predictions.csv\"\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        results.to_csv(output_file, index=False)\n",
    "        print(f\"\\n✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1ef1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_Label</th>\n",
       "      <th>Anomaly_Probability</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO: Application started successfully</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.939258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERROR: Connection timeout after 30 seconds</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING: Memory usage at 85%</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.467768</td>\n",
       "      <td>0.532232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITICAL: Database connection failed</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.184581</td>\n",
       "      <td>0.815419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO: User login successful</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.442527</td>\n",
       "      <td>0.557473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERROR: Null pointer exception in module X</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFO: Processing completed</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.450550</td>\n",
       "      <td>0.549450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALERT: Disk space critically low</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFO: Request processed in 120ms</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ERROR: Authentication failed for user admin</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.366791</td>\n",
       "      <td>0.633209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Content  Prediction Prediction_Label  \\\n",
       "0       INFO: Application started successfully           0           normal   \n",
       "1   ERROR: Connection timeout after 30 seconds           0           normal   \n",
       "2                 WARNING: Memory usage at 85%           0           normal   \n",
       "3         CRITICAL: Database connection failed           0           normal   \n",
       "4                  INFO: User login successful           0           normal   \n",
       "5    ERROR: Null pointer exception in module X           0           normal   \n",
       "6                   INFO: Processing completed           0           normal   \n",
       "7             ALERT: Disk space critically low           0           normal   \n",
       "8             INFO: Request processed in 120ms           0           normal   \n",
       "9  ERROR: Authentication failed for user admin           0           normal   \n",
       "\n",
       "   Anomaly_Probability  Confidence  \n",
       "0             0.060742    0.939258  \n",
       "1             0.000000    1.000000  \n",
       "2             0.467768    0.532232  \n",
       "3             0.184581    0.815419  \n",
       "4             0.442527    0.557473  \n",
       "5             0.000000    1.000000  \n",
       "6             0.450550    0.549450  \n",
       "7             0.000000    1.000000  \n",
       "8             0.000000    1.000000  \n",
       "9             0.366791    0.633209  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
