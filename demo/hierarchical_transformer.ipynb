{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed1010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53279d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "MODELS_PATH = ROOT / \"models\" / \"hlogformer\"\n",
    "RESULTS_PATH = ROOT / \"demo\" / \"results\" / \"hierarchical-transformer\"\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}\n",
    "\n",
    "# Model configuration (must match training)\n",
    "MAX_SEQ_LEN = 128\n",
    "D_MODEL = 768\n",
    "N_HEADS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb74510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log(text):\n",
    "    \"\"\"Preprocess log text to normalize patterns\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[0-9a-f]{8,}', '<HEX>', text)\n",
    "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '<IP>', text)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)\n",
    "    text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', '<TIME>', text)\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    text = re.sub(r'[^\\w\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715d9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_templates(texts):\n",
    "    \"\"\"Extract log templates using Drain3\"\"\"\n",
    "    config = TemplateMinerConfig()\n",
    "    config.drain_sim_th = 0.4\n",
    "    config.drain_depth = 4\n",
    "    config.drain_max_children = 100\n",
    "    \n",
    "    miner = TemplateMiner(config=config)\n",
    "    template_ids = []\n",
    "    templates = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        result = miner.add_log_message(str(text))\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        if tid not in templates:\n",
    "            templates[tid] = result[\"template_mined\"]\n",
    "    \n",
    "    return np.array(template_ids), templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5655bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(texts):\n",
    "    \"\"\"Normalize timestamps to [0, 1] range\"\"\"\n",
    "    timestamps = np.arange(len(texts), dtype=np.float32)\n",
    "    if len(timestamps) > 1:\n",
    "        timestamps = (timestamps - timestamps.min()) / (timestamps.max() - timestamps.min() + 1e-8)\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68bdf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateAwareAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.template_alpha = nn.Parameter(torch.tensor(0.1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, template_ids, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.out_proj(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(x + output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TemporalModule(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temporal_embedding = nn.Linear(1, d_model)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, timestamps):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        temporal_emb = self.temporal_embedding(timestamps.unsqueeze(-1)).unsqueeze(1)\n",
    "        x = x + temporal_emb\n",
    "        \n",
    "        sorted_indices = torch.argsort(timestamps)\n",
    "        x_sorted = x[sorted_indices]\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x_sorted)\n",
    "        \n",
    "        unsorted_indices = torch.argsort(sorted_indices)\n",
    "        lstm_out = lstm_out[unsorted_indices]\n",
    "        \n",
    "        output = self.layer_norm(x + lstm_out)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "class SourceAdapter(nn.Module):\n",
    "    def __init__(self, d_model, adapter_dim=192):\n",
    "        super().__init__()\n",
    "        self.down_proj = nn.Linear(d_model, adapter_dim)\n",
    "        self.up_proj = nn.Linear(adapter_dim, d_model)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        adapter_out = self.up_proj(F.relu(self.down_proj(x)))\n",
    "        return self.alpha * x + (1 - self.alpha) * adapter_out\n",
    "\n",
    "class HLogFormer(nn.Module):\n",
    "    \"\"\"Hierarchical Transformer for Log Anomaly Detection\"\"\"\n",
    "    def __init__(self, n_sources, n_templates, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT encoder\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Template embeddings\n",
    "        self.template_embedding = nn.Embedding(n_templates + 1, D_MODEL, padding_idx=n_templates)\n",
    "        \n",
    "        # Template-aware attention\n",
    "        self.template_attention = TemplateAwareAttention(D_MODEL, N_HEADS)\n",
    "        \n",
    "        # Temporal module\n",
    "        self.temporal_module = TemporalModule(D_MODEL)\n",
    "        \n",
    "        # Source adapters\n",
    "        self.source_adapters = nn.ModuleList([\n",
    "            SourceAdapter(D_MODEL) for _ in range(n_sources)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(D_MODEL // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, template_ids, timestamps, source_ids=None):\n",
    "        # BERT encoding\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        \n",
    "        # Template embeddings\n",
    "        template_emb = self.template_embedding(template_ids)\n",
    "        enhanced_output = pooled_output + template_emb\n",
    "        \n",
    "        # Template-aware attention\n",
    "        template_attended = self.template_attention(\n",
    "            sequence_output, template_ids, attention_mask\n",
    "        )\n",
    "        template_pooled = template_attended[:, 0, :]\n",
    "        \n",
    "        combined_output = template_pooled + template_emb\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal_output = self.temporal_module(combined_output, timestamps)\n",
    "        \n",
    "        # Source-specific adaptation (use first adapter if source_ids not provided)\n",
    "        if source_ids is not None and len(self.source_adapters) > 0:\n",
    "            adapted_outputs = []\n",
    "            for i, adapter in enumerate(self.source_adapters):\n",
    "                mask = (source_ids == i)\n",
    "                if mask.any():\n",
    "                    adapted = adapter(temporal_output[mask])\n",
    "                    adapted_outputs.append((mask, adapted))\n",
    "            \n",
    "            final_output = temporal_output.clone()\n",
    "            for mask, adapted in adapted_outputs:\n",
    "                final_output[mask] = adapted\n",
    "        else:\n",
    "            # Use first adapter for all\n",
    "            final_output = self.source_adapters[0](temporal_output) if len(self.source_adapters) > 0 else temporal_output\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(final_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5488b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hlogformer_model():\n",
    "    \"\"\"Load trained HLogFormer model\"\"\"\n",
    "    possible_files = [\n",
    "        MODELS_PATH / \"best_model.pt\",\n",
    "        MODELS_PATH / \"final_production_model.pt\",\n",
    "    ]\n",
    "    \n",
    "    model_file = None\n",
    "    for file in possible_files:\n",
    "        if file.exists():\n",
    "            model_file = file\n",
    "            break\n",
    "    \n",
    "    if model_file is None:\n",
    "        raise FileNotFoundError(f\"HLogFormer model not found. Searched: {possible_files}\")\n",
    "    \n",
    "    print(f\"Loading HLogFormer model from: {model_file}\")\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    \n",
    "    # Get model configuration from checkpoint\n",
    "    n_sources = checkpoint.get('n_sources', 16)\n",
    "    \n",
    "    # Try to infer n_templates from the checkpoint state dict\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint))\n",
    "    if 'template_embedding.weight' in state_dict:\n",
    "        n_templates = state_dict['template_embedding.weight'].shape[0] - 1  # -1 for padding\n",
    "        print(f\"  Inferred {n_templates} templates from checkpoint\")\n",
    "    else:\n",
    "        n_templates = checkpoint.get('n_templates', 10000)\n",
    "    \n",
    "    # Create model with correct template count\n",
    "    model = HLogFormer(n_sources, n_templates, freeze_layers=6).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    try:\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        elif 'model' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model'], strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  Warning: Some weights couldn't be loaded: {e}\")\n",
    "        print(f\"  Continuing with partially loaded model...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Loaded HLogFormer model\")\n",
    "    print(f\"  Sources: {n_sources}, Templates: {n_templates}\")\n",
    "    if 'best_f1' in checkpoint:\n",
    "        print(f\"  Best F1: {checkpoint['best_f1']:.4f}\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  Trained epochs: {checkpoint['epoch'] + 1}\")\n",
    "    \n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a074ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, texts, template_ids, timestamps, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.template_ids = template_ids\n",
    "        self.timestamps = timestamps\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'template_ids': torch.tensor(int(self.template_ids[idx]), dtype=torch.long),\n",
    "            'timestamps': torch.tensor(float(self.timestamps[idx]), dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a082f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_hlogformer(log_data, content_column='Content', batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions using HLogFormer model\n",
    "    \n",
    "    Args:\n",
    "        log_data: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        predictions, probabilities, confidence\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TRANSFORMER (HLogFormer) ANOMALY DETECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    texts = df[content_column].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    # Preprocess texts\n",
    "    print(\"\\n1. Preprocessing texts...\")\n",
    "    processed_texts = [preprocess_log(text) for text in texts]\n",
    "    \n",
    "    # Extract templates\n",
    "    print(\"2. Extracting templates with Drain3...\")\n",
    "    template_ids, templates = extract_templates(processed_texts)\n",
    "    print(f\"   ✓ Found {len(templates)} unique templates\")\n",
    "    \n",
    "    # Normalize timestamps\n",
    "    print(\"3. Normalizing timestamps...\")\n",
    "    timestamps = normalize_timestamps(processed_texts)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\n4. Loading HLogFormer model...\")\n",
    "    model, checkpoint = load_hlogformer_model()\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    print(\"5. Initializing BERT tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = LogDataset(processed_texts, template_ids, timestamps, tokenizer, MAX_SEQ_LEN)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\n6. Making predictions on {len(texts)} logs...\")\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            template_ids_batch = batch['template_ids'].to(device)\n",
    "            timestamps_batch = batch['timestamps'].to(device)\n",
    "            \n",
    "            # Clip template IDs to valid range\n",
    "            max_template_id = model.template_embedding.num_embeddings - 1\n",
    "            template_ids_batch = torch.clamp(template_ids_batch, 0, max_template_id)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, template_ids_batch, timestamps_batch)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    anomaly_probs = all_probs[:, 1]\n",
    "    confidence = np.max(all_probs, axis=1)\n",
    "    \n",
    "    return all_preds, anomaly_probs, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250940c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(log_data, predictions, probabilities, confidence, \n",
    "                   content_column='Content', top_n=10):\n",
    "    \"\"\"Display prediction results\"\"\"\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    df['Prediction'] = predictions\n",
    "    df['Prediction_Label'] = df['Prediction'].map(LABEL_MAP)\n",
    "    df['Anomaly_Probability'] = probabilities\n",
    "    df['Confidence'] = confidence\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total logs analyzed: {len(df)}\")\n",
    "    print(f\"Normal logs: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anomalous logs: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {confidence.mean():.3f}\")\n",
    "    \n",
    "    if (predictions == 1).sum() > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TOP {min(top_n, (predictions == 1).sum())} ANOMALIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        anomalies = df[df['Prediction'] == 1].sort_values('Anomaly_Probability', ascending=False).head(top_n)\n",
    "        \n",
    "        for idx, row in anomalies.iterrows():\n",
    "            print(f\"\\n[{idx}] Probability: {row['Anomaly_Probability']:.3f}, Confidence: {row['Confidence']:.3f}\")\n",
    "            print(f\"Log: {row[content_column][:200]}...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8b5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_hlogformer_prediction(log_data, content_column='Content', batch_size=32, show_top_n=10):\n",
    "    \"\"\"\n",
    "    Main demo function for HLogFormer prediction\n",
    "    \n",
    "    Args:\n",
    "        log_data: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        batch_size: Batch size for inference\n",
    "        show_top_n: Number of top anomalies to display\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions, probabilities, confidence = predict_with_hlogformer(\n",
    "        log_data, content_column, batch_size\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    results_df = display_results(\n",
    "        log_data, predictions, probabilities, confidence, \n",
    "        content_column, show_top_n\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0539d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE: Predicting on custom log messages with HLogFormer\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "HIERARCHICAL TRANSFORMER (HLogFormer) ANOMALY DETECTION\n",
      "================================================================================\n",
      "\n",
      "1. Preprocessing texts...\n",
      "2. Extracting templates with Drain3...\n",
      "   ✓ Found 15 unique templates\n",
      "3. Normalizing timestamps...\n",
      "\n",
      "4. Loading HLogFormer model...\n",
      "Loading HLogFormer model from: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\hlogformer\\best_model.pt\n",
      "  Inferred 1596 templates from checkpoint\n",
      "✓ Loaded HLogFormer model\n",
      "  Sources: 16, Templates: 1596\n",
      "  Best F1: 0.4208\n",
      "  Trained epochs: 1\n",
      "5. Initializing BERT tokenizer...\n",
      "\n",
      "6. Making predictions on 15 logs...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 15\n",
      "Normal logs: 15 (100.0%)\n",
      "Anomalous logs: 0 (0.0%)\n",
      "Average confidence: 0.590\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\hierarchical-transformer\\hlogformer_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: Predicting on custom log messages with HLogFormer\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_logs = [\n",
    "        \"INFO: Application started successfully at port 8080\",\n",
    "        \"ERROR: Connection timeout after 30 seconds to database server\",\n",
    "        \"WARNING: Memory usage at 85% threshold exceeded\",\n",
    "        \"CRITICAL: Database connection failed - max retries reached\",\n",
    "        \"INFO: User authentication successful for user john.doe\",\n",
    "        \"ERROR: Null pointer exception in module UserService.processRequest\",\n",
    "        \"INFO: Data processing completed in 2.5 seconds\",\n",
    "        \"ALERT: Disk space critically low - only 5% remaining\",\n",
    "        \"INFO: HTTP request processed successfully in 120ms\",\n",
    "        \"ERROR: Authentication failed for user admin - invalid credentials\",\n",
    "        \"WARNING: High CPU usage detected - 95% utilization\",\n",
    "        \"INFO: Scheduled backup completed successfully\",\n",
    "        \"CRITICAL: Out of memory error in worker thread\",\n",
    "        \"ERROR: Failed to parse configuration file - invalid JSON\",\n",
    "        \"INFO: Service health check passed\"\n",
    "    ]\n",
    "    \n",
    "    results = demo_hlogformer_prediction(\n",
    "        sample_logs, \n",
    "        content_column='Content',\n",
    "        batch_size=8,\n",
    "        show_top_n=5\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_file = RESULTS_PATH / \"hlogformer_predictions.csv\"\n",
    "    results.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9191e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_Label</th>\n",
       "      <th>Anomaly_Probability</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO: Application started successfully at port...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.412765</td>\n",
       "      <td>0.587235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERROR: Connection timeout after 30 seconds to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.414455</td>\n",
       "      <td>0.585545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING: Memory usage at 85% threshold exceeded</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.411265</td>\n",
       "      <td>0.588735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITICAL: Database connection failed - max ret...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.410806</td>\n",
       "      <td>0.589194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO: User authentication successful for user ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.411307</td>\n",
       "      <td>0.588693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERROR: Null pointer exception in module UserSe...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.406520</td>\n",
       "      <td>0.593480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFO: Data processing completed in 2.5 seconds</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.410724</td>\n",
       "      <td>0.589276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALERT: Disk space critically low - only 5% rem...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.409074</td>\n",
       "      <td>0.590926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFO: HTTP request processed successfully in 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.409207</td>\n",
       "      <td>0.590793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ERROR: Authentication failed for user admin - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.411601</td>\n",
       "      <td>0.588399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WARNING: High CPU usage detected - 95% utiliza...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.409983</td>\n",
       "      <td>0.590017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INFO: Scheduled backup completed successfully</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.409711</td>\n",
       "      <td>0.590289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CRITICAL: Out of memory error in worker thread</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.411852</td>\n",
       "      <td>0.588148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ERROR: Failed to parse configuration file - in...</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.408404</td>\n",
       "      <td>0.591596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INFO: Service health check passed</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.408373</td>\n",
       "      <td>0.591627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Content  Prediction  \\\n",
       "0   INFO: Application started successfully at port...           0   \n",
       "1   ERROR: Connection timeout after 30 seconds to ...           0   \n",
       "2     WARNING: Memory usage at 85% threshold exceeded           0   \n",
       "3   CRITICAL: Database connection failed - max ret...           0   \n",
       "4   INFO: User authentication successful for user ...           0   \n",
       "5   ERROR: Null pointer exception in module UserSe...           0   \n",
       "6      INFO: Data processing completed in 2.5 seconds           0   \n",
       "7   ALERT: Disk space critically low - only 5% rem...           0   \n",
       "8   INFO: HTTP request processed successfully in 1...           0   \n",
       "9   ERROR: Authentication failed for user admin - ...           0   \n",
       "10  WARNING: High CPU usage detected - 95% utiliza...           0   \n",
       "11      INFO: Scheduled backup completed successfully           0   \n",
       "12     CRITICAL: Out of memory error in worker thread           0   \n",
       "13  ERROR: Failed to parse configuration file - in...           0   \n",
       "14                  INFO: Service health check passed           0   \n",
       "\n",
       "   Prediction_Label  Anomaly_Probability  Confidence  \n",
       "0            normal             0.412765    0.587235  \n",
       "1            normal             0.414455    0.585545  \n",
       "2            normal             0.411265    0.588735  \n",
       "3            normal             0.410806    0.589194  \n",
       "4            normal             0.411307    0.588693  \n",
       "5            normal             0.406520    0.593480  \n",
       "6            normal             0.410724    0.589276  \n",
       "7            normal             0.409074    0.590926  \n",
       "8            normal             0.409207    0.590793  \n",
       "9            normal             0.411601    0.588399  \n",
       "10           normal             0.409983    0.590017  \n",
       "11           normal             0.409711    0.590289  \n",
       "12           normal             0.411852    0.588148  \n",
       "13           normal             0.408404    0.591596  \n",
       "14           normal             0.408373    0.591627  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
