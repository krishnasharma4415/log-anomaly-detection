{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e43eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model on cuda...\n",
      "✓ BERT model loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from feature_extractor import extract_features_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f60ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "MODELS_PATH = ROOT / \"models\" / \"meta_learning\"\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}\n",
    "\n",
    "# Meta-learning configuration (must match training)\n",
    "META_CONFIG = {\n",
    "    'input_dim': 200,\n",
    "    'hidden_dims': [256, 128],\n",
    "    'embedding_dim': 64,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': 2,\n",
    "    'inner_lr': 0.01,\n",
    "    'inner_steps': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fe04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"Meta-learning model for few-shot anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim, dropout, num_classes):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, embedding_dim))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        return embeddings\n",
    "    \n",
    "    def predict(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f95f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_model():\n",
    "    \"\"\"Load trained meta-learning model\"\"\"\n",
    "    # Try different possible model files\n",
    "    possible_files = [\n",
    "        MODELS_PATH / \"best_meta_model.pt\",\n",
    "        MODELS_PATH / \"final_meta_model.pt\",\n",
    "    ]\n",
    "    \n",
    "    model_file = None\n",
    "    for file in possible_files:\n",
    "        if file.exists():\n",
    "            model_file = file\n",
    "            break\n",
    "    \n",
    "    if model_file is None:\n",
    "        raise FileNotFoundError(f\"Meta-learning model not found. Searched: {possible_files}\")\n",
    "    \n",
    "    print(f\"Loading meta-learning model from: {model_file}\")\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    \n",
    "    # Create model\n",
    "    model = MetaLearner(\n",
    "        META_CONFIG['input_dim'],\n",
    "        META_CONFIG['hidden_dims'],\n",
    "        META_CONFIG['embedding_dim'],\n",
    "        META_CONFIG['dropout'],\n",
    "        META_CONFIG['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    if 'model' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Loaded meta-learning model\")\n",
    "    if 'iteration' in checkpoint:\n",
    "        print(f\"  Training iteration: {checkpoint['iteration']}\")\n",
    "    if 'meta_loss' in checkpoint:\n",
    "        print(f\"  Meta loss: {checkpoint['meta_loss']:.4f}\")\n",
    "    if 'avg_f1' in checkpoint:\n",
    "        print(f\"  Average F1: {checkpoint['avg_f1']:.4f}\")\n",
    "    \n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93112d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_model_few_shot(model, support_X, support_y, inner_lr=0.01, inner_steps=5):\n",
    "    \"\"\"\n",
    "    Adapt model to new data using few-shot learning\n",
    "    \n",
    "    Args:\n",
    "        model: Meta-learning model\n",
    "        support_X: Support set features (few labeled examples)\n",
    "        support_y: Support set labels\n",
    "        inner_lr: Learning rate for adaptation\n",
    "        inner_steps: Number of adaptation steps\n",
    "    \n",
    "    Returns:\n",
    "        adapted_model: Model adapted to the support set\n",
    "    \"\"\"\n",
    "    print(f\"\\nAdapting model with {len(support_y)} support examples...\")\n",
    "    print(f\"  Support distribution: {dict(zip(*np.unique(support_y, return_counts=True)))}\")\n",
    "    \n",
    "    # Create a copy of the model for adaptation\n",
    "    adapted_model = MetaLearner(\n",
    "        model.input_dim,\n",
    "        model.hidden_dims,\n",
    "        model.embedding_dim,\n",
    "        model.dropout,\n",
    "        model.num_classes\n",
    "    ).to(device)\n",
    "    adapted_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # Adapt using support set\n",
    "    optimizer = SGD(adapted_model.parameters(), lr=inner_lr)\n",
    "    \n",
    "    support_X_tensor = torch.FloatTensor(support_X).to(device)\n",
    "    support_y_tensor = torch.LongTensor(support_y).to(device)\n",
    "    \n",
    "    adapted_model.train()\n",
    "    for step in range(inner_steps):\n",
    "        optimizer.zero_grad()\n",
    "        logits = adapted_model.predict(support_X_tensor)\n",
    "        loss = F.cross_entropy(logits, support_y_tensor)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % 2 == 0:\n",
    "            print(f\"  Adaptation step {step+1}/{inner_steps}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    adapted_model.eval()\n",
    "    print(\"✓ Adaptation complete\")\n",
    "    \n",
    "    return adapted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbec7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_meta_learning(query_X, model, support_X=None, support_y=None, \n",
    "                               adapt=True, inner_lr=0.01, inner_steps=5):\n",
    "    \"\"\"\n",
    "    Make predictions using meta-learning model\n",
    "    \n",
    "    Args:\n",
    "        query_X: Query set features (data to predict)\n",
    "        model: Meta-learning model\n",
    "        support_X: Support set features (optional, for adaptation)\n",
    "        support_y: Support set labels (optional, for adaptation)\n",
    "        adapt: Whether to adapt the model using support set\n",
    "        inner_lr: Learning rate for adaptation\n",
    "        inner_steps: Number of adaptation steps\n",
    "    \n",
    "    Returns:\n",
    "        predictions, probabilities, confidence\n",
    "    \"\"\"\n",
    "    # Adapt model if support set is provided\n",
    "    if adapt and support_X is not None and support_y is not None:\n",
    "        model = adapt_model_few_shot(model, support_X, support_y, inner_lr, inner_steps)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\nMaking predictions on {len(query_X)} query examples...\")\n",
    "    \n",
    "    query_X_tensor = torch.FloatTensor(query_X).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model.predict(query_X_tensor)\n",
    "        probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    confidence = np.max(probabilities, axis=1)\n",
    "    anomaly_probs = probabilities[:, 1]\n",
    "    \n",
    "    return predictions, anomaly_probs, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe3b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(log_data, predictions, probabilities, confidence, \n",
    "                   content_column='Content', top_n=10):\n",
    "    \"\"\"Display prediction results\"\"\"\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    df['Prediction'] = predictions\n",
    "    df['Prediction_Label'] = df['Prediction'].map(LABEL_MAP)\n",
    "    df['Anomaly_Probability'] = probabilities\n",
    "    df['Confidence'] = confidence\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total logs analyzed: {len(df)}\")\n",
    "    print(f\"Normal logs: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anomalous logs: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {confidence.mean():.3f}\")\n",
    "    \n",
    "    if (predictions == 1).sum() > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TOP {min(top_n, (predictions == 1).sum())} ANOMALIES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        anomalies = df[df['Prediction'] == 1].sort_values('Anomaly_Probability', ascending=False).head(top_n)\n",
    "        \n",
    "        for idx, row in anomalies.iterrows():\n",
    "            print(f\"\\n[{idx}] Probability: {row['Anomaly_Probability']:.3f}, Confidence: {row['Confidence']:.3f}\")\n",
    "            print(f\"Log: {row[content_column][:200]}...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11d0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_meta_learning_prediction(query_logs, support_logs=None, support_labels=None,\n",
    "                                 content_column='Content', timestamp_column=None, adapt=True, \n",
    "                                 inner_lr=0.01, inner_steps=5, show_top_n=10):\n",
    "    \"\"\"\n",
    "    Main demo function for meta-learning prediction with FULL feature extraction\n",
    "    \n",
    "    Args:\n",
    "        query_logs: DataFrame or list of log messages to predict\n",
    "        support_logs: DataFrame or list of labeled log messages for adaptation (optional)\n",
    "        support_labels: Labels for support logs (0=normal, 1=anomaly)\n",
    "        content_column: Name of the column containing log messages\n",
    "        timestamp_column: Name of the column containing timestamps (optional)\n",
    "        adapt: Whether to adapt the model using support set\n",
    "        inner_lr: Learning rate for adaptation\n",
    "        inner_steps: Number of adaptation steps\n",
    "        show_top_n: Number of top anomalies to display\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"META-LEARNING ANOMALY DETECTION DEMO (Few-Shot Learning)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    model, checkpoint = load_meta_model()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTRACTING FEATURES USING FULL PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"This includes:\")\n",
    "    print(\"  ✓ BERT embeddings (768-dim)\")\n",
    "    print(\"  ✓ Drain3 template parsing\")\n",
    "    print(\"  ✓ Statistical features (rolling windows, outliers)\")\n",
    "    print(\"  ✓ Error pattern detection (15+ patterns)\")\n",
    "    print(\"  ✓ Temporal features\")\n",
    "    print(\"  ✓ Text complexity features\")\n",
    "    print(\"  ✓ Feature selection (top 200 features)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Process query logs with FULL pipeline\n",
    "    query_X, scaler = extract_features_for_prediction(\n",
    "        query_logs, \n",
    "        content_column, \n",
    "        timestamp_column,\n",
    "        feature_variant='selected_imbalanced'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {query_X.shape[1]} features for query set\")\n",
    "    \n",
    "    # Process support logs if provided\n",
    "    support_X = None\n",
    "    support_y = None\n",
    "    if support_logs is not None and support_labels is not None:\n",
    "        print(\"\\nExtracting features for support set...\")\n",
    "        support_X, _ = extract_features_for_prediction(\n",
    "            support_logs, \n",
    "            content_column, \n",
    "            timestamp_column,\n",
    "            feature_variant='selected_imbalanced'\n",
    "        )\n",
    "        support_y = np.array(support_labels)\n",
    "        \n",
    "        print(f\"✓ Extracted {support_X.shape[1]} features for support set\")\n",
    "        \n",
    "        # Scale features together\n",
    "        scaler = StandardScaler()\n",
    "        support_X = scaler.fit_transform(support_X)\n",
    "        query_X = scaler.transform(query_X)\n",
    "    else:\n",
    "        # Scale query set only\n",
    "        scaler = StandardScaler()\n",
    "        query_X = scaler.fit_transform(query_X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, probabilities, confidence = predict_with_meta_learning(\n",
    "        query_X, model, support_X, support_y, adapt, inner_lr, inner_steps\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    results_df = display_results(\n",
    "        query_logs, predictions, probabilities, confidence, \n",
    "        content_column, show_top_n\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c8860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 1: Zero-shot prediction (no adaptation)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "META-LEARNING ANOMALY DETECTION DEMO (Few-Shot Learning)\n",
      "================================================================================\n",
      "Loading meta-learning model from: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\meta_learning\\best_meta_model.pt\n",
      "✓ Loaded meta-learning model\n",
      "  Training iteration: 124\n",
      "  Meta loss: 0.0417\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 8 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (8, 9)\n",
      "✓ Error features: (8, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (8, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 8 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/8 logs\n",
      "✓ BERT embeddings: (8, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (8, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (8, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features for query set\n",
      "\n",
      "Making predictions on 8 query examples...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 8\n",
      "Normal logs: 1 (12.5%)\n",
      "Anomalous logs: 7 (87.5%)\n",
      "Average confidence: 0.513\n",
      "\n",
      "================================================================================\n",
      "TOP 5 ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "[2] Probability: 0.522, Confidence: 0.522\n",
      "Log: WARNING: Memory usage at 85%...\n",
      "\n",
      "[0] Probability: 0.518, Confidence: 0.518\n",
      "Log: INFO: Application started successfully...\n",
      "\n",
      "[7] Probability: 0.517, Confidence: 0.517\n",
      "Log: ALERT: Disk space critically low...\n",
      "\n",
      "[4] Probability: 0.512, Confidence: 0.512\n",
      "Log: INFO: User login successful...\n",
      "\n",
      "[5] Probability: 0.511, Confidence: 0.511\n",
      "Log: ERROR: Null pointer exception in module X...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 2: Few-shot prediction (with adaptation)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "META-LEARNING ANOMALY DETECTION DEMO (Few-Shot Learning)\n",
      "================================================================================\n",
      "Loading meta-learning model from: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\meta_learning\\best_meta_model.pt\n",
      "✓ Loaded meta-learning model\n",
      "  Training iteration: 124\n",
      "  Meta loss: 0.0417\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 6 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (6, 9)\n",
      "✓ Error features: (6, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (6, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 6 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/6 logs\n",
      "✓ BERT embeddings: (6, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (6, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (6, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features for query set\n",
      "\n",
      "Extracting features for support set...\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 4 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (4, 9)\n",
      "✓ Error features: (4, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (4, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 4 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/4 logs\n",
      "✓ BERT embeddings: (4, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (4, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (4, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "✓ Extracted 200 features for support set\n",
      "\n",
      "Adapting model with 4 support examples...\n",
      "  Support distribution: {np.int64(0): np.int64(2), np.int64(1): np.int64(2)}\n",
      "  Adaptation step 2/5, Loss: 0.6959\n",
      "  Adaptation step 4/5, Loss: 0.6504\n",
      "✓ Adaptation complete\n",
      "\n",
      "Making predictions on 6 query examples...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 6\n",
      "Normal logs: 0 (0.0%)\n",
      "Anomalous logs: 6 (100.0%)\n",
      "Average confidence: 0.512\n",
      "\n",
      "================================================================================\n",
      "TOP 5 ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "[4] Probability: 0.525, Confidence: 0.525\n",
      "Log: WARNING: Response time exceeds threshold...\n",
      "\n",
      "[1] Probability: 0.516, Confidence: 0.516\n",
      "Log: ERROR: Authentication failed - invalid token...\n",
      "\n",
      "[0] Probability: 0.514, Confidence: 0.514\n",
      "Log: INFO: Request processed in 50ms...\n",
      "\n",
      "[2] Probability: 0.507, Confidence: 0.507\n",
      "Log: INFO: Cache cleared successfully...\n",
      "\n",
      "[3] Probability: 0.504, Confidence: 0.504\n",
      "Log: CRITICAL: Service unavailable - max connections reached...\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\meta-learning\\meta_learning_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 1: Zero-shot prediction (no adaptation)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    query_logs = [\n",
    "        \"INFO: Application started successfully\",\n",
    "        \"ERROR: Connection timeout after 30 seconds\",\n",
    "        \"WARNING: Memory usage at 85%\",\n",
    "        \"CRITICAL: Database connection failed\",\n",
    "        \"INFO: User login successful\",\n",
    "        \"ERROR: Null pointer exception in module X\",\n",
    "        \"INFO: Processing completed\",\n",
    "        \"ALERT: Disk space critically low\",\n",
    "    ]\n",
    "    \n",
    "    results = demo_meta_learning_prediction(\n",
    "        query_logs,\n",
    "        content_column='Content',\n",
    "        adapt=False,\n",
    "        show_top_n=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 2: Few-shot prediction (with adaptation)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Provide a few labeled examples for adaptation\n",
    "    support_logs = [\n",
    "        \"INFO: System health check passed\",\n",
    "        \"INFO: Backup completed successfully\",\n",
    "        \"ERROR: Failed to connect to remote server\",\n",
    "        \"CRITICAL: Out of memory error occurred\",\n",
    "    ]\n",
    "    support_labels = [0, 0, 1, 1]  # 0=normal, 1=anomaly\n",
    "    \n",
    "    query_logs_2 = [\n",
    "        \"INFO: Request processed in 50ms\",\n",
    "        \"ERROR: Authentication failed - invalid token\",\n",
    "        \"INFO: Cache cleared successfully\",\n",
    "        \"CRITICAL: Service unavailable - max connections reached\",\n",
    "        \"WARNING: Response time exceeds threshold\",\n",
    "        \"INFO: Configuration reloaded\",\n",
    "    ]\n",
    "    \n",
    "    results_adapted = demo_meta_learning_prediction(\n",
    "        query_logs_2,\n",
    "        support_logs=support_logs,\n",
    "        support_labels=support_labels,\n",
    "        content_column='Content',\n",
    "        adapt=True,\n",
    "        inner_lr=0.01,\n",
    "        inner_steps=5,\n",
    "        show_top_n=5\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_file = ROOT / \"demo\" / \"results\" / \"meta-learning\" / \"meta_learning_predictions.csv\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_adapted.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a63479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_Label</th>\n",
       "      <th>Anomaly_Probability</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO: Application started successfully</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>0.518157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERROR: Connection timeout after 30 seconds</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.508567</td>\n",
       "      <td>0.508567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING: Memory usage at 85%</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.522133</td>\n",
       "      <td>0.522133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITICAL: Database connection failed</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.490340</td>\n",
       "      <td>0.509660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO: User login successful</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.512359</td>\n",
       "      <td>0.512359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERROR: Null pointer exception in module X</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.510805</td>\n",
       "      <td>0.510805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFO: Processing completed</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.509531</td>\n",
       "      <td>0.509531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALERT: Disk space critically low</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.516650</td>\n",
       "      <td>0.516650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Content  Prediction Prediction_Label  \\\n",
       "0      INFO: Application started successfully           1          anomaly   \n",
       "1  ERROR: Connection timeout after 30 seconds           1          anomaly   \n",
       "2                WARNING: Memory usage at 85%           1          anomaly   \n",
       "3        CRITICAL: Database connection failed           0           normal   \n",
       "4                 INFO: User login successful           1          anomaly   \n",
       "5   ERROR: Null pointer exception in module X           1          anomaly   \n",
       "6                  INFO: Processing completed           1          anomaly   \n",
       "7            ALERT: Disk space critically low           1          anomaly   \n",
       "\n",
       "   Anomaly_Probability  Confidence  \n",
       "0             0.518157    0.518157  \n",
       "1             0.508567    0.508567  \n",
       "2             0.522133    0.522133  \n",
       "3             0.490340    0.509660  \n",
       "4             0.512359    0.512359  \n",
       "5             0.510805    0.510805  \n",
       "6             0.509531    0.509531  \n",
       "7             0.516650    0.516650  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
