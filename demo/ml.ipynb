{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac58e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model on cuda...\n",
      "✓ BERT model loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from feature_extractor import extract_features_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569f6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "MODELS_PATH = ROOT / \"models\" / \"ml_models\" / \"deployment\"\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72596948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_model():\n",
    "    \"\"\"Load the trained ML model\"\"\"\n",
    "    model_file = MODELS_PATH / \"best_model_for_deployment.pkl\"\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_file}\")\n",
    "    \n",
    "    with open(model_file, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded ML model: {model_data['model_name'].upper()}\")\n",
    "    print(f\"Training samples: {model_data['training_samples']:,}\")\n",
    "    print(f\"Average F1-Macro: {model_data['metrics']['avg_f1_macro']:.4f}\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "def predict_anomalies(log_data, content_column='Content', timestamp_column=None, \n",
    "                     threshold=None, source_name=None):\n",
    "    \"\"\"\n",
    "    Predict anomalies in custom log data using FULL feature extraction pipeline\n",
    "    \n",
    "    Args:\n",
    "        log_data: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        timestamp_column: Name of the column containing timestamps (optional)\n",
    "        threshold: Custom classification threshold (optional)\n",
    "        source_name: Name of the log source for threshold lookup (optional)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: numpy array of predictions (0=normal, 1=anomaly)\n",
    "        probabilities: numpy array of anomaly probabilities\n",
    "        confidence: numpy array of prediction confidence scores\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model_data = load_ml_model()\n",
    "    model = model_data['model']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTRACTING FEATURES USING FULL PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"This includes:\")\n",
    "    print(\"  ✓ BERT embeddings (768-dim)\")\n",
    "    print(\"  ✓ Drain3 template parsing\")\n",
    "    print(\"  ✓ Statistical features (rolling windows, outliers)\")\n",
    "    print(\"  ✓ Error pattern detection (15+ patterns)\")\n",
    "    print(\"  ✓ Temporal features\")\n",
    "    print(\"  ✓ Text complexity features\")\n",
    "    print(\"  ✓ Feature selection (top 200 features)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Extract features using FULL pipeline\n",
    "    X, scaler = extract_features_for_prediction(\n",
    "        log_data, \n",
    "        content_column, \n",
    "        timestamp_column,\n",
    "        feature_variant='selected_imbalanced'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {X.shape[1]} features (matching training pipeline)\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(X)\n",
    "        confidence = np.max(probabilities, axis=1)\n",
    "        anomaly_probs = probabilities[:, 1]\n",
    "        \n",
    "        # Apply custom threshold if provided\n",
    "        if threshold is not None:\n",
    "            predictions = (anomaly_probs >= threshold).astype(int)\n",
    "            print(f\"Applied custom threshold: {threshold:.3f}\")\n",
    "        elif source_name and 'thresholds_by_source' in model_data:\n",
    "            if source_name in model_data['thresholds_by_source']:\n",
    "                threshold = model_data['thresholds_by_source'][source_name]\n",
    "                predictions = (anomaly_probs >= threshold).astype(int)\n",
    "                print(f\"Applied source-specific threshold for '{source_name}': {threshold:.3f}\")\n",
    "    else:\n",
    "        # Model doesn't support probabilities\n",
    "        probabilities = np.zeros((len(predictions), 2))\n",
    "        probabilities[np.arange(len(predictions)), predictions] = 1.0\n",
    "        confidence = np.ones(len(predictions))\n",
    "        anomaly_probs = predictions.astype(float)\n",
    "    \n",
    "    return predictions, anomaly_probs, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c574d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(log_data, predictions, probabilities, confidence, \n",
    "                   content_column='Content', top_n=10):\n",
    "    \"\"\"Display prediction results\"\"\"\n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(log_data, list):\n",
    "        df = pd.DataFrame({content_column: log_data})\n",
    "    else:\n",
    "        df = log_data.copy()\n",
    "    \n",
    "    # Add predictions\n",
    "    df['Prediction'] = predictions\n",
    "    df['Prediction_Label'] = df['Prediction'].map(LABEL_MAP)\n",
    "    df['Anomaly_Probability'] = probabilities\n",
    "    df['Confidence'] = confidence\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total logs analyzed: {len(df)}\")\n",
    "    print(f\"Normal logs: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anomalous logs: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {confidence.mean():.3f}\")\n",
    "    \n",
    "    # Top anomalies\n",
    "    if (predictions == 1).sum() > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TOP {min(top_n, (predictions == 1).sum())} ANOMALIES (by probability)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        anomalies = df[df['Prediction'] == 1].sort_values('Anomaly_Probability', ascending=False).head(top_n)\n",
    "        \n",
    "        for idx, row in anomalies.iterrows():\n",
    "            print(f\"\\n[{idx}] Probability: {row['Anomaly_Probability']:.3f}, Confidence: {row['Confidence']:.3f}\")\n",
    "            print(f\"Log: {row[content_column][:200]}...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a823e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_ml_prediction(custom_logs, content_column='Content', timestamp_column=None,\n",
    "                      threshold=None, source_name=None, show_top_n=10):\n",
    "    \"\"\"\n",
    "    Main demo function for ML model prediction\n",
    "    \n",
    "    Args:\n",
    "        custom_logs: DataFrame or list of log messages\n",
    "        content_column: Name of the column containing log messages\n",
    "        timestamp_column: Name of the column containing timestamps (optional)\n",
    "        threshold: Custom classification threshold (optional)\n",
    "        source_name: Name of the log source for threshold lookup (optional)\n",
    "        show_top_n: Number of top anomalies to display\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ML MODEL ANOMALY DETECTION DEMO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, probabilities, confidence = predict_anomalies(\n",
    "        custom_logs, content_column, timestamp_column, threshold, source_name\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    results_df = display_results(\n",
    "        custom_logs, predictions, probabilities, confidence, \n",
    "        content_column, show_top_n\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b99f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 1: Predicting on custom log messages\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ML MODEL ANOMALY DETECTION DEMO\n",
      "================================================================================\n",
      "Loaded ML model: NB\n",
      "Training samples: 26,000\n",
      "Average F1-Macro: 0.8337\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 10 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (10, 9)\n",
      "✓ Error features: (10, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (10, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 10 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/10 logs\n",
      "✓ BERT embeddings: (10, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (10, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (10, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features (matching training pipeline)\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 10\n",
      "Normal logs: 9 (90.0%)\n",
      "Anomalous logs: 1 (10.0%)\n",
      "Average confidence: 0.988\n",
      "\n",
      "================================================================================\n",
      "TOP 1 ANOMALIES (by probability)\n",
      "================================================================================\n",
      "\n",
      "[5] Probability: 1.000, Confidence: 1.000\n",
      "Log: ERROR: Null pointer exception in module X...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 2: Predicting on DataFrame with timestamps\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ML MODEL ANOMALY DETECTION DEMO\n",
      "================================================================================\n",
      "Loaded ML model: NB\n",
      "Training samples: 26,000\n",
      "Average F1-Macro: 0.8337\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING FEATURES USING FULL PIPELINE\n",
      "================================================================================\n",
      "This includes:\n",
      "  ✓ BERT embeddings (768-dim)\n",
      "  ✓ Drain3 template parsing\n",
      "  ✓ Statistical features (rolling windows, outliers)\n",
      "  ✓ Error pattern detection (15+ patterns)\n",
      "  ✓ Temporal features\n",
      "  ✓ Text complexity features\n",
      "  ✓ Feature selection (top 200 features)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE EXTRACTION PIPELINE\n",
      "================================================================================\n",
      "Processing 5 log entries...\n",
      "\n",
      "1. Preprocessing texts...\n",
      "\n",
      "2. Extracting text features...\n",
      "✓ Text features: (5, 9)\n",
      "✓ Error features: (5, 15)\n",
      "\n",
      "3. Extracting temporal features...\n",
      "✓ Temporal features: (5, 8)\n",
      "\n",
      "4. Extracting template features...\n",
      "Extracting template features with Drain3...\n",
      "✓ Extracted 10 template features\n",
      "✓ Found 5 unique templates\n",
      "\n",
      "5. Extracting BERT features...\n",
      "Extracting BERT features (batch_size=16)...\n",
      "  Processed 0/5 logs\n",
      "✓ BERT embeddings: (5, 768)\n",
      "Extracting statistical features from embeddings...\n",
      "✓ Statistical features: (5, 28)\n",
      "Extracting sentence-level features...\n",
      "✓ Sentence features: (5, 5)\n",
      "\n",
      "6. Combining features...\n",
      "\n",
      "================================================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "================================================================================\n",
      "Feature variants created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 801 features\n",
      "  - template_enhanced: 10 features\n",
      "  - anomaly_focused: 793 features\n",
      "  - imbalance_aware_full: 838 features\n",
      "  - sentence_focused: 792 features\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Selecting top 200 features for imbalanced classification...\n",
      "✓ Using 200 features (no labels for selection)\n",
      "\n",
      "✓ Extracted 200 features (matching training pipeline)\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "================================================================================\n",
      "PREDICTION SUMMARY\n",
      "================================================================================\n",
      "Total logs analyzed: 5\n",
      "Normal logs: 4 (80.0%)\n",
      "Anomalous logs: 1 (20.0%)\n",
      "Average confidence: 0.955\n",
      "\n",
      "================================================================================\n",
      "TOP 1 ANOMALIES (by probability)\n",
      "================================================================================\n",
      "\n",
      "[1] Probability: 0.989, Confidence: 0.989\n",
      "Log: ERROR: Failed to connect to database...\n",
      "\n",
      "✓ Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\demo\\results\\ml\\ml_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example 1: List of log messages\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 1: Predicting on custom log messages\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_logs = [\n",
    "        \"INFO: Application started successfully\",\n",
    "        \"ERROR: Connection timeout after 30 seconds\",\n",
    "        \"WARNING: Memory usage at 85%\",\n",
    "        \"CRITICAL: Database connection failed\",\n",
    "        \"INFO: User login successful\",\n",
    "        \"ERROR: Null pointer exception in module X\",\n",
    "        \"INFO: Processing completed\",\n",
    "        \"ALERT: Disk space critically low\",\n",
    "        \"INFO: Request processed in 120ms\",\n",
    "        \"ERROR: Authentication failed for user admin\"\n",
    "    ]\n",
    "    \n",
    "    results = demo_ml_prediction(sample_logs, content_column='Content')\n",
    "    \n",
    "    # Example 2: DataFrame with timestamps\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 2: Predicting on DataFrame with timestamps\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_logs = pd.DataFrame({\n",
    "        'Timestamp': pd.date_range('2024-01-01', periods=5, freq='H'),\n",
    "        'Content': [\n",
    "            \"System startup complete\",\n",
    "            \"ERROR: Failed to connect to database\",\n",
    "            \"WARNING: High CPU usage detected\",\n",
    "            \"INFO: Backup completed successfully\",\n",
    "            \"CRITICAL: Out of memory error\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    results_df = demo_ml_prediction(\n",
    "        df_logs, \n",
    "        content_column='Content', \n",
    "        timestamp_column='Timestamp',\n",
    "        show_top_n=3\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_file = ROOT / \"demo\" / \"results\" / \"ml\" / \"ml_predictions.csv\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe0447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_Label</th>\n",
       "      <th>Anomaly_Probability</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>System startup complete</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.174010</td>\n",
       "      <td>0.825990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>ERROR: Failed to connect to database</td>\n",
       "      <td>1</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.989068</td>\n",
       "      <td>0.989068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>WARNING: High CPU usage detected</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.967123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 03:00:00</td>\n",
       "      <td>INFO: Backup completed successfully</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.999850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 04:00:00</td>\n",
       "      <td>CRITICAL: Out of memory error</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.993590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp                               Content  Prediction  \\\n",
       "0 2024-01-01 00:00:00               System startup complete           0   \n",
       "1 2024-01-01 01:00:00  ERROR: Failed to connect to database           1   \n",
       "2 2024-01-01 02:00:00      WARNING: High CPU usage detected           0   \n",
       "3 2024-01-01 03:00:00   INFO: Backup completed successfully           0   \n",
       "4 2024-01-01 04:00:00         CRITICAL: Out of memory error           0   \n",
       "\n",
       "  Prediction_Label  Anomaly_Probability  Confidence  \n",
       "0           normal             0.174010    0.825990  \n",
       "1          anomaly             0.989068    0.989068  \n",
       "2           normal             0.032877    0.967123  \n",
       "3           normal             0.000150    0.999850  \n",
       "4           normal             0.006410    0.993590  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
