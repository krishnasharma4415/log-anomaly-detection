{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640e3e02",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25d4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c13bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 CPU cores for parallel processing\n",
      "Loaded 6 sources, 6 experiments\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "N_JOBS = multiprocessing.cpu_count()  \n",
    "print(f\"Using {N_JOBS} CPU cores for parallel processing\")\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEATURES_PATH = ROOT / \"features\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"cross_source_transfer\" / \"ml_models\"\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(FEATURES_PATH / \"hybrid_features.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)['hybrid_features_data']\n",
    "with open(FEATURES_PATH / \"cross_source_splits.pkl\", 'rb') as f:\n",
    "    splits = pickle.load(f)['splits']\n",
    "\n",
    "print(f\"Loaded {len(data)} sources, {len(splits)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec566326",
   "metadata": {},
   "source": [
    "Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8337c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'nb': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {'var_smoothing': [1e-9, 1e-7]}\n",
    "    },\n",
    "    'lr': {\n",
    "        'model': LogisticRegression(random_state=SEED, max_iter=1000),\n",
    "        'params': {'C': [0.1, 1.0], 'penalty': ['l2'], 'solver': ['lbfgs']}\n",
    "    },\n",
    "    'knn': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'n_neighbors': [3, 5], 'weights': ['uniform', 'distance']}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(random_state=SEED, probability=True),\n",
    "        'params': {'C': [0.1, 1], 'kernel': ['rbf'], 'gamma': ['scale']}\n",
    "    },\n",
    "    'dt': {\n",
    "        'model': DecisionTreeClassifier(random_state=SEED),\n",
    "        'params': {'max_depth': [5, 10], 'min_samples_split': [2, 5]}\n",
    "    },\n",
    "    'rf': {\n",
    "        'model': RandomForestClassifier(random_state=SEED, n_jobs=2),\n",
    "        'params': {'n_estimators': [50, 100], 'max_depth': [10], 'min_samples_split': [2]}\n",
    "    },\n",
    "    'gb': {\n",
    "        'model': GradientBoostingClassifier(random_state=SEED),\n",
    "        'params': {'n_estimators': [50], 'learning_rate': [0.1], 'max_depth': [3, 5]}\n",
    "    },\n",
    "    'xgb': {\n",
    "        'model': XGBClassifier(random_state=SEED, eval_metric='logloss', n_jobs=2, tree_method='hist'),\n",
    "        'params': {'n_estimators': [50], 'learning_rate': [0.1], 'max_depth': [3, 5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "unsupervised_models_config = {\n",
    "    'iso': {\n",
    "        'model': IsolationForest(random_state=SEED, contamination='auto', n_jobs=2),\n",
    "        'params': {}\n",
    "    },\n",
    "    'ocsvm': {\n",
    "        'model': OneClassSVM(kernel='rbf', gamma='scale'),\n",
    "        'params': {}\n",
    "    },\n",
    "    'lof': {\n",
    "        'model': LocalOutlierFactor(contamination='auto', novelty=True, n_jobs=2),\n",
    "        'params': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa10564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred, y_proba=None):\n",
    "    try:\n",
    "        f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.size == 4:\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            tn = fp = fn = tp = 0\n",
    "            \n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        bal_acc = (sens + spec) / 2\n",
    "        \n",
    "        metrics = {'f1': f1, 'mcc': mcc, 'acc': acc, 'bal_acc': bal_acc}\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "            try:\n",
    "                roc = roc_auc_score(y_true, y_proba)\n",
    "                p, r, _ = precision_recall_curve(y_true, y_proba)\n",
    "                pr = auc(r, p)\n",
    "                metrics.update({'roc': roc, 'pr': pr})\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        return {'f1': 0, 'mcc': 0, 'acc': 0, 'bal_acc': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2279f",
   "metadata": {},
   "source": [
    "Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636f9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_models(X_train, y_train, X_test, y_test, models_config):\n",
    "    \"\"\"Train all supervised models with hyperparameter tuning.\"\"\"\n",
    "    if len(X_train) == 0 or len(y_train) == 0:\n",
    "        raise ValueError(\"Empty training data\")\n",
    "    \n",
    "    if X_train.shape[1] != X_test.shape[1]:\n",
    "        raise ValueError(f\"Feature dimension mismatch: train={X_train.shape[1]}, test={X_test.shape[1]}\")\n",
    "    \n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        raise ValueError(\"Training data must have at least 2 classes\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in models_config.items():\n",
    "        try:\n",
    "            if len(config['params']) > 0:\n",
    "                cv = StratifiedKFold(n_splits=min(3, len(y_train) // 10), shuffle=True, random_state=SEED)\n",
    "                \n",
    "                grid = GridSearchCV(\n",
    "                    config['model'], config['params'], \n",
    "                    cv=cv, scoring='f1', n_jobs=1, verbose=0\n",
    "                )\n",
    "                grid.fit(X_train_scaled, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                best_params = grid.best_params_\n",
    "                best_score = grid.best_score_\n",
    "            else:\n",
    "                # No hyperparameters to tune\n",
    "                best_model = config['model']\n",
    "                best_model.fit(X_train_scaled, y_train)\n",
    "                best_params = {}\n",
    "                best_score = 0\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            \n",
    "            # Get probabilities or decision function\n",
    "            y_proba = None\n",
    "            try:\n",
    "                y_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    y_proba = best_model.decision_function(X_test_scaled)\n",
    "                    y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min() + 1e-10)\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            \n",
    "            metrics = calc_metrics(y_test, y_pred, y_proba)\n",
    "            \n",
    "            results[name] = {\n",
    "                'metrics': metrics,\n",
    "                'best_params': best_params,\n",
    "                'best_score': best_score\n",
    "            }\n",
    "            \n",
    "            # Free memory\n",
    "            del best_model\n",
    "            if 'grid' in locals():\n",
    "                del grid\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    return results, scaler\n",
    "\n",
    "def train_unsupervised_models(X_train, X_test, y_test, unsupervised_models_config, scaler):\n",
    "    \"\"\"Train all unsupervised models for anomaly detection.\"\"\"\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in unsupervised_models_config.items():\n",
    "        try:\n",
    "            model = config['model']\n",
    "            model.fit(X_train_scaled)\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Get anomaly scores\n",
    "            if hasattr(model, 'score_samples'):\n",
    "                scores = -model.score_samples(X_test_scaled)\n",
    "            elif hasattr(model, 'decision_function'):\n",
    "                scores = -model.decision_function(X_test_scaled)\n",
    "            else:\n",
    "                scores = None\n",
    "            \n",
    "            y_pred_binary = (y_pred == -1).astype(int)\n",
    "            \n",
    "            metrics = calc_metrics(y_test, y_pred_binary, scores)\n",
    "            results[name] = {'metrics': metrics}\n",
    "            \n",
    "            # Free memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbc710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(split, feature_type):\n",
    "    \"\"\"Run a single cross-source transfer experiment.\"\"\"\n",
    "    try:\n",
    "        test_src = split['test_source']\n",
    "        train_srcs = split['train_sources']\n",
    "        \n",
    "        # Get test data\n",
    "        test_data = data[test_src]\n",
    "        if test_data['labels'] is None:\n",
    "            return None, None\n",
    "        \n",
    "        # Extract features based on type\n",
    "        if feature_type == 'bert':\n",
    "            if 'bert_only' not in test_data or test_data['bert_only'] is None:\n",
    "                return None, None\n",
    "            X_test = test_data['bert_only']\n",
    "        elif feature_type == 'hybrid':\n",
    "            if 'hybrid_variants' not in test_data or 'bert_embedding_concat' not in test_data['hybrid_variants']:\n",
    "                return None, None\n",
    "            X_test = test_data['hybrid_variants']['bert_embedding_concat']\n",
    "        else:\n",
    "            return None, None\n",
    "        \n",
    "        y_test = test_data['labels']\n",
    "        \n",
    "        # Collect training data from multiple sources\n",
    "        X_train_list, y_train_list = [], []\n",
    "        for source in train_srcs:\n",
    "            if data[source]['labels'] is None:\n",
    "                continue\n",
    "                \n",
    "            if feature_type == 'bert':\n",
    "                if 'bert_only' in data[source] and data[source]['bert_only'] is not None:\n",
    "                    X_train_list.append(data[source]['bert_only'])\n",
    "                    y_train_list.append(data[source]['labels'])\n",
    "            elif feature_type == 'hybrid':\n",
    "                if 'hybrid_variants' in data[source] and 'bert_embedding_concat' in data[source]['hybrid_variants']:\n",
    "                    X_train_list.append(data[source]['hybrid_variants']['bert_embedding_concat'])\n",
    "                    y_train_list.append(data[source]['labels'])\n",
    "        \n",
    "        if not X_train_list:\n",
    "            return None, None\n",
    "        \n",
    "        # Combine training data\n",
    "        X_train = np.vstack(X_train_list)\n",
    "        y_train = np.concatenate(y_train_list)\n",
    "        \n",
    "        # Validate data\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            return None, None\n",
    "        \n",
    "        # Train models\n",
    "        sup_results, scaler = train_supervised_models(X_train, y_train, X_test, y_test, models_config)\n",
    "        unsup_results = train_unsupervised_models(X_train, X_test, y_test, unsupervised_models_config, scaler)\n",
    "        \n",
    "        exp_name = f\"{test_src}_{feature_type}\"\n",
    "        result = {\n",
    "            'supervised': sup_results,\n",
    "            'unsupervised': unsup_results,\n",
    "            'test_samples': len(y_test),\n",
    "            'train_samples': len(y_train),\n",
    "            'anomaly_rate': float(np.mean(y_test)),\n",
    "            'train_sources': train_srcs\n",
    "        }\n",
    "        \n",
    "        # Free memory\n",
    "        del X_train, y_train, X_test, y_test, scaler\n",
    "        gc.collect()\n",
    "        \n",
    "        return exp_name, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405a4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 12 experiment configurations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiments: 100%|██████████| 12/12 [2:40:32<00:00, 802.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 12 experiments successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_configs = []\n",
    "for split in splits:\n",
    "    experiment_configs.append((split, 'bert'))\n",
    "    experiment_configs.append((split, 'hybrid'))\n",
    "\n",
    "print(f\"\\nPrepared {len(experiment_configs)} experiment configurations\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for split, feature_type in tqdm(experiment_configs, desc=\"Experiments\"):\n",
    "    exp_name, result = run_single_experiment(split, feature_type)\n",
    "    if exp_name is not None:\n",
    "        all_results[exp_name] = result\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} experiments successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7f86f",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db97faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for exp_name, exp_data in all_results.items():\n",
    "    parts = exp_name.split('_')\n",
    "    test_source = parts[0]\n",
    "    feature_type = '_'.join(parts[1:])\n",
    "    \n",
    "    # Supervised models\n",
    "    for model_name, model_data in exp_data['supervised'].items():\n",
    "        if 'error' not in model_data:\n",
    "            metrics = model_data['metrics']\n",
    "            results_list.append({\n",
    "                'experiment': exp_name,\n",
    "                'test_source': test_source,\n",
    "                'feature_type': feature_type,\n",
    "                'model_type': 'supervised',\n",
    "                'model': model_name,\n",
    "                'f1': metrics.get('f1', 0),\n",
    "                'mcc': metrics.get('mcc', 0),\n",
    "                'roc': metrics.get('roc', 0),\n",
    "                'pr': metrics.get('pr', 0),\n",
    "                'bal_acc': metrics.get('bal_acc', 0),\n",
    "                'acc': metrics.get('acc', 0),\n",
    "                'best_params': str(model_data.get('best_params', {})),\n",
    "                'train_samples': exp_data['train_samples'],\n",
    "                'test_samples': exp_data['test_samples'],\n",
    "                'anomaly_rate': exp_data['anomaly_rate']\n",
    "            })\n",
    "    \n",
    "    # Unsupervised models\n",
    "    for model_name, model_data in exp_data['unsupervised'].items():\n",
    "        if 'error' not in model_data:\n",
    "            metrics = model_data['metrics']\n",
    "            results_list.append({\n",
    "                'experiment': exp_name,\n",
    "                'test_source': test_source,\n",
    "                'feature_type': feature_type,\n",
    "                'model_type': 'unsupervised',\n",
    "                'model': model_name,\n",
    "                'f1': metrics.get('f1', 0),\n",
    "                'mcc': metrics.get('mcc', 0),\n",
    "                'roc': metrics.get('roc', 0),\n",
    "                'pr': metrics.get('pr', 0),\n",
    "                'bal_acc': metrics.get('bal_acc', 0),\n",
    "                'acc': metrics.get('acc', 0),\n",
    "                'best_params': 'N/A',\n",
    "                'train_samples': exp_data['train_samples'],\n",
    "                'test_samples': exp_data['test_samples'],\n",
    "                'anomaly_rate': exp_data['anomaly_rate']\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73066b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Models by F1 Score:\n",
      "model test_source feature_type    f1    pr    mcc  bal_acc\n",
      "   lr      Apache         bert 0.990 0.990  0.986    0.990\n",
      "  svm     OpenSSH         bert 0.989 0.997  0.949    0.980\n",
      "  xgb     OpenSSH         bert 0.968 0.999  0.873    0.968\n",
      "   lr     OpenSSH         bert 0.958 0.987  0.826    0.940\n",
      "   gb     OpenSSH         bert 0.916 0.970  0.726    0.917\n",
      "   rf     OpenSSH         bert 0.915 0.984  0.724    0.917\n",
      "  lof         BGL       hybrid 0.862 0.757  0.185    0.531\n",
      "  lof     OpenSSH       hybrid 0.861 0.671 -0.077    0.483\n",
      "  lof         BGL         bert 0.857 0.789  0.026    0.501\n",
      "ocsvm         BGL         bert 0.853 0.819  0.089    0.518\n",
      "Average Performance by Model:\n",
      "          f1     pr    mcc  bal_acc    roc\n",
      "model                                     \n",
      "lr     0.510  0.645  0.232    0.626  0.691\n",
      "lof    0.496  0.441 -0.074    0.473  0.451\n",
      "svm    0.468  0.665  0.166    0.595  0.692\n",
      "ocsvm  0.435  0.389 -0.204    0.406  0.341\n",
      "xgb    0.399  0.611  0.220    0.629  0.757\n",
      "gb     0.398  0.621  0.132    0.585  0.689\n",
      "nb     0.374  0.668  0.142    0.581  0.646\n",
      "rf     0.348  0.537  0.082    0.552  0.617\n",
      "knn    0.333  0.548 -0.058    0.475  0.508\n",
      "dt     0.331  0.497  0.056    0.547  0.550\n",
      "iso    0.184  0.351 -0.160    0.401  0.289\n",
      "Average Performance by Feature Type:\n",
      "                 f1     pr    mcc  bal_acc\n",
      "feature_type                              \n",
      "bert          0.426  0.584  0.112    0.569\n",
      "hybrid        0.351  0.503 -0.015    0.499\n",
      "Average Performance by Model Type:\n",
      "                 f1     pr    mcc  bal_acc\n",
      "model_type                                \n",
      "supervised    0.395  0.599  0.121    0.574\n",
      "unsupervised  0.372  0.394 -0.146    0.427\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Models by F1 Score:\")\n",
    "top_models = df.nlargest(10, 'f1')[['model', 'test_source', 'feature_type', 'f1', 'pr', 'mcc', 'bal_acc']]\n",
    "print(top_models.round(3).to_string(index=False))\n",
    "\n",
    "print(\"Average Performance by Model:\")\n",
    "avg_perf = df.groupby('model')[['f1', 'pr', 'mcc', 'bal_acc', 'roc']].mean().sort_values('f1', ascending=False).round(3)\n",
    "print(avg_perf)\n",
    "\n",
    "print(\"Average Performance by Feature Type:\")\n",
    "feat_perf = df.groupby('feature_type')[['f1', 'pr', 'mcc', 'bal_acc']].mean().round(3)\n",
    "print(feat_perf)\n",
    "\n",
    "print(\"Average Performance by Model Type:\")\n",
    "type_perf = df.groupby('model_type')[['f1', 'pr', 'mcc', 'bal_acc']].mean().round(3)\n",
    "print(type_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805a1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\cross_source_transfer\\ml_models\\ml_results.csv\n",
      "Saved pickle to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\cross_source_transfer\\ml_models\\ml_results.pkl\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(RESULTS_PATH / \"ml_results.csv\", index=False)\n",
    "print(f\"Saved CSV to: {RESULTS_PATH / 'ml_results.csv'}\")\n",
    "\n",
    "with open(RESULTS_PATH / \"ml_results.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results': all_results,\n",
    "        'summary': df,\n",
    "        'top_models': top_models,\n",
    "        'avg_performance': avg_perf,\n",
    "        'feature_performance': feat_perf,\n",
    "        'type_performance': type_perf\n",
    "    }, f)\n",
    "print(f\"Saved pickle to: {RESULTS_PATH / 'ml_results.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4912a79",
   "metadata": {},
   "source": [
    "Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723f96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Configuration:\n",
      "  Model: lr\n",
      "  Feature Type: bert\n",
      "  Test Source: Apache\n",
      "  F1 Score: 0.990\n",
      "  MCC: 0.986\n",
      "  Balanced Accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "best_row = df.loc[df['f1'].idxmax()]\n",
    "best_model_name = best_row['model']\n",
    "best_feature_type = best_row['feature_type']\n",
    "best_test_source = best_row['test_source']\n",
    "\n",
    "print(f\"Best Model Configuration:\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Feature Type: {best_feature_type}\")\n",
    "print(f\"  Test Source: {best_test_source}\")\n",
    "print(f\"  F1 Score: {best_row['f1']:.3f}\")\n",
    "print(f\"  MCC: {best_row['mcc']:.3f}\")\n",
    "print(f\"  Balanced Accuracy: {best_row['bal_acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sources: Apache, BGL, HPC, OpenSSH, Proxifier, Zookeeper\n",
      "Total training samples: 12,000\n",
      "Feature dimensions: 768\n",
      "Anomaly rate: 40.73%\n"
     ]
    }
   ],
   "source": [
    "all_train_sources = [s for s in data.keys() if data[s]['labels'] is not None]\n",
    "print(f\"Training sources: {', '.join(all_train_sources)}\")\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "for source in all_train_sources:\n",
    "    if best_feature_type == 'bert':\n",
    "        if 'bert_only' in data[source] and data[source]['bert_only'] is not None:\n",
    "            X_train_list.append(data[source]['bert_only'])\n",
    "            y_train_list.append(data[source]['labels'])\n",
    "    else:  # hybrid\n",
    "        if 'hybrid_variants' in data[source] and 'bert_embedding_concat' in data[source]['hybrid_variants']:\n",
    "            X_train_list.append(data[source]['hybrid_variants']['bert_embedding_concat'])\n",
    "            y_train_list.append(data[source]['labels'])\n",
    "\n",
    "X_train_all = np.vstack(X_train_list)\n",
    "y_train_all = np.concatenate(y_train_list)\n",
    "\n",
    "print(f\"Total training samples: {len(y_train_all):,}\")\n",
    "print(f\"Feature dimensions: {X_train_all.shape[1]}\")\n",
    "print(f\"Anomaly rate: {np.mean(y_train_all)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e8da667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final lr model\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_all)\n",
    "\n",
    "print(f\"Training final {best_model_name} model\")\n",
    "if best_model_name in models_config:\n",
    "    model_config = models_config[best_model_name]\n",
    "    is_supervised = True\n",
    "else:\n",
    "    model_config = unsupervised_models_config[best_model_name]\n",
    "    is_supervised = False\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "if is_supervised:\n",
    "    if len(model_config['params']) > 0:\n",
    "        from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        grid = GridSearchCV(\n",
    "            model_config['model'], \n",
    "            model_config['params'],\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid.fit(X_train_scaled, y_train_all)\n",
    "        final_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    else:\n",
    "        final_model = clone(model_config['model'])\n",
    "        final_model.fit(X_train_scaled, y_train_all)\n",
    "        best_params = {}\n",
    "else:\n",
    "    final_model = clone(model_config['model'])\n",
    "    final_model.fit(X_train_scaled)\n",
    "    best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5489e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on Apache:\n",
      "  F1: 1.000\n",
      "  MCC: 1.000\n",
      "  Balanced Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "validation_source = best_test_source\n",
    "if validation_source in data and data[validation_source]['labels'] is not None:\n",
    "    if best_feature_type == 'bert':\n",
    "        X_val = data[validation_source]['bert_only']\n",
    "    else:\n",
    "        X_val = data[validation_source]['hybrid_variants']['bert_embedding_concat']\n",
    "    \n",
    "    y_val = data[validation_source]['labels']\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    if is_supervised:\n",
    "        y_pred = final_model.predict(X_val_scaled)\n",
    "    else:\n",
    "        y_pred = final_model.predict(X_val_scaled)\n",
    "        y_pred = (y_pred == -1).astype(int)\n",
    "    \n",
    "    val_metrics = calc_metrics(y_val, y_pred)\n",
    "    print(f\"Validation on {validation_source}:\")\n",
    "    print(f\"  F1: {val_metrics['f1']:.3f}\")\n",
    "    print(f\"  MCC: {val_metrics['mcc']:.3f}\")\n",
    "    print(f\"  Balanced Accuracy: {val_metrics['bal_acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f952b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment model saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\cross_source_transfer\\ml_models\\deployment\\best_classifier.pkl\n",
      "File size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "deployment_path = RESULTS_PATH / \"deployment\"\n",
    "deployment_path.mkdir(exist_ok=True)\n",
    "\n",
    "deployment_data = {\n",
    "    'model': final_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_type': best_feature_type,\n",
    "    'model_name': best_model_name,\n",
    "    'is_supervised': is_supervised,\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'f1': float(best_row['f1']),\n",
    "        'mcc': float(best_row['mcc']),\n",
    "        'bal_acc': float(best_row['bal_acc']),\n",
    "        'acc': float(best_row['acc']),\n",
    "        'roc': float(best_row.get('roc', 0)),\n",
    "        'pr': float(best_row.get('pr', 0))\n",
    "    },\n",
    "    'training_info': {\n",
    "        'n_samples': len(y_train_all),\n",
    "        'n_features': X_train_all.shape[1],\n",
    "        'n_sources': len(all_train_sources),\n",
    "        'sources': all_train_sources,\n",
    "        'anomaly_rate': float(np.mean(y_train_all))\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(deployment_path / \"best_classifier.pkl\", 'wb') as f:\n",
    "    pickle.dump(deployment_data, f)\n",
    "\n",
    "print(f\"Deployment model saved to: {deployment_path / 'best_classifier.pkl'}\")\n",
    "print(f\"File size: {(deployment_path / 'best_classifier.pkl').stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09fd027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\cross_source_transfer\\ml_models\\deployment\\model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'feature_type': best_feature_type,\n",
    "    'is_supervised': is_supervised,\n",
    "    'best_params': str(best_params),\n",
    "    'metrics': deployment_data['metrics'],\n",
    "    'training_info': deployment_data['training_info'],\n",
    "    'timestamp': deployment_data['timestamp']\n",
    "}\n",
    "\n",
    "with open(deployment_path / \"model_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {deployment_path / 'model_metadata.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
