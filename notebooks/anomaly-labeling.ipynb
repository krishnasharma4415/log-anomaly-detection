{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdd59e4",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e585ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de2e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATA_PATH = PROJECT_ROOT / \"dataset\" / \"structured_data\"\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"dataset\" / \"labeled_data\"\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "281caa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SOURCES = [\n",
    "    'Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', \n",
    "    'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k',\n",
    "    'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k',\n",
    "    'Windows_2k', 'Zookeeper_2k'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57f9d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    0: \"normal\",\n",
    "    1: \"security_anomaly\", \n",
    "    2: \"system_failure\",\n",
    "    3: \"performance_issue\",\n",
    "    4: \"network_anomaly\", \n",
    "    5: \"config_error\",\n",
    "    6: \"hardware_issue\",\n",
    "    7: \"unknown_anomaly\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c74fceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_PATTERNS = {\n",
    "    'security': ['authentication failure', 'invalid user', 'break-in attempt', \n",
    "                'failed password', 'unauthorized', 'access denied'],\n",
    "    'system': ['error', 'critical', 'fatal', 'exception', 'crash', 'abort'],\n",
    "    'network': ['timeout', 'connection refused', 'host unreachable'],\n",
    "    'performance': ['slow', 'overload', 'resource exhausted', 'quota exceeded'],\n",
    "    'hardware': ['hardware error', 'disk error', 'i/o error', 'device error']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9eaf7",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3c8dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    datasets = {}\n",
    "    for source in LOG_SOURCES:\n",
    "        try:\n",
    "            file_path = DATA_PATH / f\"{source}.log_structured.csv\"\n",
    "            df = pd.read_csv(file_path)\n",
    "            datasets[source] = df\n",
    "            print(f\"✓ Loaded {source}: {len(df):,} logs\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed {source}: {e}\")\n",
    "    \n",
    "    total = sum(len(df) for df in datasets.values())\n",
    "    print(f\"\\nLoaded {len(datasets)} sources, {total:,} total logs\")\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0637aaf",
   "metadata": {},
   "source": [
    "Analyze Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31287bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_templates(datasets):\n",
    "    stats = {}\n",
    "    for source, df in datasets.items():\n",
    "        if 'EventTemplate' not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        templates = df['EventTemplate'].value_counts()\n",
    "        stats[source] = {\n",
    "            'total_logs': len(df),\n",
    "            'unique_templates': len(templates),\n",
    "            'efficiency': len(df) / len(templates),\n",
    "            'top_templates': templates.head(5)\n",
    "        }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d69c37",
   "metadata": {},
   "source": [
    "Find Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "118d6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anomalies(datasets):\n",
    "    results = {}\n",
    "    \n",
    "    for source, df in datasets.items():\n",
    "        if 'Content' not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        content_lower = df['Content'].str.lower()\n",
    "        anomaly_counts = {}\n",
    "        \n",
    "        for category, keywords in ANOMALY_PATTERNS.items():\n",
    "            pattern = '|'.join(re.escape(kw) for kw in keywords)\n",
    "            matches = content_lower.str.contains(pattern, na=False, regex=True)\n",
    "            anomaly_counts[category] = matches.sum()\n",
    "        \n",
    "        total_anomalies = sum(anomaly_counts.values())\n",
    "        results[source] = {\n",
    "            'total_logs': len(df),\n",
    "            'anomalies': total_anomalies,\n",
    "            'anomaly_rate': (total_anomalies / len(df)) * 100,\n",
    "            'categories': anomaly_counts\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cf302",
   "metadata": {},
   "source": [
    "Rank Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22387a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sources(template_stats, anomaly_stats):\n",
    "    rankings = []\n",
    "    \n",
    "    for source in template_stats.keys():\n",
    "        if source not in anomaly_stats:\n",
    "            continue\n",
    "            \n",
    "        t_stats = template_stats[source] \n",
    "        a_stats = anomaly_stats[source]\n",
    "        \n",
    "        anomaly_score = a_stats['anomaly_rate']\n",
    "        template_score = 100 - (t_stats['unique_templates'] / 10)  # Fewer templates = better\n",
    "        efficiency_score = t_stats['efficiency'] / 10\n",
    "        \n",
    "        priority = (anomaly_score * 0.4 + template_score * 0.3 + efficiency_score * 0.3)\n",
    "        \n",
    "        rankings.append({\n",
    "            'source': source,\n",
    "            'priority_score': priority,\n",
    "            'anomaly_rate': a_stats['anomaly_rate'],\n",
    "            'templates': t_stats['unique_templates'],\n",
    "            'efficiency': t_stats['efficiency']\n",
    "        })\n",
    "    \n",
    "    return sorted(rankings, key=lambda x: x['priority_score'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c8113",
   "metadata": {},
   "source": [
    "Prepare Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0434fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_templates(df, source_name):\n",
    "    if 'EventTemplate' not in df.columns:\n",
    "        return []\n",
    "    \n",
    "    templates = df['EventTemplate'].value_counts()\n",
    "    labeling_data = []\n",
    "    \n",
    "    for template, count in templates.items():\n",
    "        samples = df[df['EventTemplate'] == template]['Content'].head(3).tolist()\n",
    "        \n",
    "        suggested_label = 0  \n",
    "        confidence = \"low\"\n",
    "        \n",
    "        content_text = ' '.join(samples).lower()\n",
    "        \n",
    "        for category, keywords in ANOMALY_PATTERNS.items():\n",
    "            if any(kw in content_text for kw in keywords):\n",
    "                if category == 'security':\n",
    "                    suggested_label = 1\n",
    "                elif category == 'system': \n",
    "                    suggested_label = 2\n",
    "                elif category == 'performance':\n",
    "                    suggested_label = 3\n",
    "                elif category == 'network':\n",
    "                    suggested_label = 4\n",
    "                elif category == 'hardware':\n",
    "                    suggested_label = 6\n",
    "                confidence = \"high\"\n",
    "                break\n",
    "        \n",
    "        labeling_data.append({\n",
    "            'template': template,\n",
    "            'count': count,\n",
    "            'percentage': (count / len(df)) * 100,\n",
    "            'samples': samples,\n",
    "            'suggested': suggested_label,\n",
    "            'confidence': confidence,\n",
    "            'label': None,  \n",
    "            'notes': ''\n",
    "        })\n",
    "    \n",
    "    return sorted(labeling_data, key=lambda x: x['count'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b80f3f",
   "metadata": {},
   "source": [
    "Auto-label High Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8716737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label(labeling_data):\n",
    "    auto_count = 0\n",
    "    for item in labeling_data:\n",
    "        if item['confidence'] == 'high' and item['label'] is None:\n",
    "            item['label'] = item['suggested']\n",
    "            item['notes'] = 'Auto-labeled (high confidence)'\n",
    "            auto_count += 1\n",
    "    \n",
    "    print(f\"Auto-labeled {auto_count} templates\")\n",
    "    return auto_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafcb8a",
   "metadata": {},
   "source": [
    "Interactive Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac30db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_batch(labeling_data, start=0, count=5):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEMPLATE LABELING\")\n",
    "    print(\"Labels:\", \", \".join(f\"{k}:{v}\" for k, v in LABELS.items()))\n",
    "    print(\"Commands: 0-7 (label), 'skip', 'quit', 'save'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    end = min(start + count, len(labeling_data))\n",
    "    labeled = 0\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        item = labeling_data[i]\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{len(labeling_data)}] Template\")\n",
    "        print(f\"Frequency: {item['count']:,} logs ({item['percentage']:.1f}%)\")\n",
    "        print(f\"Template: {item['template']}\")\n",
    "        print(\"Samples:\")\n",
    "        for j, sample in enumerate(item['samples'][:2], 1):\n",
    "            print(f\"  {j}. {sample}\")\n",
    "        print(f\"Suggested: {item['suggested']} ({LABELS[item['suggested']]}) - {item['confidence']}\")\n",
    "        \n",
    "        while True:\n",
    "            response = input(f\"\\nLabel (suggested {item['suggested']}): \").strip().lower()\n",
    "            \n",
    "            if response == 'quit':\n",
    "                return i, labeled\n",
    "            elif response == 'skip':\n",
    "                break\n",
    "            elif response == 'save':\n",
    "                save_progress(labeling_data, source_name)\n",
    "                continue\n",
    "            elif response.isdigit() and 0 <= int(response) <= 7:\n",
    "                item['label'] = int(response) \n",
    "                notes = input(\"Notes (optional): \").strip()\n",
    "                if notes:\n",
    "                    item['notes'] = notes\n",
    "                labeled += 1\n",
    "                break\n",
    "            else:\n",
    "                print(\"Enter 0-7, 'skip', 'save', or 'quit'\")\n",
    "    \n",
    "    print(f\"\\nLabeled {labeled} templates in this batch\")\n",
    "    return end, labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9ff8d",
   "metadata": {},
   "source": [
    "Quick Label Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c216b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_label(labeling_data, indices, labels):\n",
    "    for idx, label in zip(indices, labels):\n",
    "        if 0 <= idx < len(labeling_data) and 0 <= label <= 7:\n",
    "            labeling_data[idx]['label'] = label\n",
    "            labeling_data[idx]['notes'] = 'Quick label'\n",
    "            print(f\"Labeled template {idx}: {LABELS[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531b3ca",
   "metadata": {},
   "source": [
    "Save Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e2d29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(labeling_data, source_name):\n",
    "    df = pd.DataFrame(labeling_data)\n",
    "    df.to_csv(OUTPUT_PATH / f\"{source_name}_progress.csv\", index=False)\n",
    "    \n",
    "    labeled = sum(1 for item in labeling_data if item['label'] is not None)\n",
    "    print(f\"Saved progress: {labeled}/{len(labeling_data)} templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2db4f",
   "metadata": {},
   "source": [
    "Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17cdb760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(source_name):\n",
    "    file_path = OUTPUT_PATH / f\"{source_name}_progress.csv\"\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = df.to_dict('records')\n",
    "        labeled = sum(1 for item in data if pd.notna(item.get('label')))\n",
    "        print(f\"Loaded progress: {labeled}/{len(data)} templates\")\n",
    "        return data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3b125",
   "metadata": {},
   "source": [
    "Show Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ebb4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(labeling_data):\n",
    "    total = len(labeling_data)\n",
    "    labeled = sum(1 for item in labeling_data if item['label'] is not None)\n",
    "    \n",
    "    print(f\"\\nProgress: {labeled}/{total} templates ({labeled/total*100:.1f}%)\")\n",
    "    \n",
    "    if labeled > 0:\n",
    "        dist = defaultdict(int)\n",
    "        for item in labeling_data:\n",
    "            if item['label'] is not None and not pd.isna(item['label']):\n",
    "                dist[int(item['label'])] += item['count']\n",
    "        \n",
    "        print(\"Label distribution:\")\n",
    "        for label in sorted(dist.keys()):\n",
    "            count = dist[label]\n",
    "            if label in LABELS:\n",
    "                print(f\"  {label} ({LABELS[label]}): {count:,} logs\")\n",
    "            else:\n",
    "                print(f\"  {label} (unknown): {count:,} logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44c4c9",
   "metadata": {},
   "source": [
    "Export Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "984bdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_final(df, labeling_data, source_name):\n",
    "    template_labels = {item['template']: item['label'] \n",
    "                      for item in labeling_data if item['label'] is not None}\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    result_df['AnomalyLabel'] = result_df['EventTemplate'].map(template_labels).fillna(-1).astype(int)\n",
    "    result_df['AnomalyLabelName'] = result_df['AnomalyLabel'].map(lambda x: LABELS.get(x, 'unlabeled'))\n",
    "    \n",
    "    output_file = OUTPUT_PATH / f\"{source_name}_labeled.csv\"\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    total = len(result_df)\n",
    "    labeled_count = (result_df['AnomalyLabel'] >= 0).sum()\n",
    "    anomaly_count = (result_df['AnomalyLabel'] > 0).sum()\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {output_file}\")\n",
    "    print(f\"Total logs: {total:,}\")\n",
    "    print(f\"Labeled: {labeled_count:,} ({labeled_count/total*100:.1f}%)\")\n",
    "    print(f\"Anomalies: {anomaly_count:,} ({anomaly_count/labeled_count*100:.1f}% of labeled)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be67da",
   "metadata": {},
   "source": [
    "Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68ed758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow():\n",
    "    print(\"LOG ANOMALY LABELING WORKFLOW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"1. Loading datasets...\")\n",
    "    datasets = load_data()\n",
    "    \n",
    "    print(\"\\n2. Analyzing templates...\")\n",
    "    template_stats = analyze_templates(datasets)\n",
    "    \n",
    "    print(\"\\n3. Finding anomalies...\")\n",
    "    anomaly_stats = find_anomalies(datasets)\n",
    "    \n",
    "    print(\"\\n4. Ranking sources...\")\n",
    "    rankings = rank_sources(template_stats, anomaly_stats)\n",
    "    \n",
    "    print(\"\\nTop 3 sources for labeling:\")\n",
    "    for i, rank in enumerate(rankings[:3], 1):\n",
    "        print(f\"{i}. {rank['source']} (score: {rank['priority_score']:.1f})\")\n",
    "        print(f\"   Anomaly rate: {rank['anomaly_rate']:.1f}%\")\n",
    "        print(f\"   Templates: {rank['templates']}\")\n",
    "    \n",
    "    best_source = rankings[0]['source']\n",
    "    print(f\"\\n5. Preparing {best_source} for labeling...\")\n",
    "    \n",
    "    labeling_data = load_progress(best_source)\n",
    "    if labeling_data is None:\n",
    "        labeling_data = prep_templates(datasets[best_source], best_source)\n",
    "        auto_label(labeling_data)\n",
    "    \n",
    "    show_progress(labeling_data)\n",
    "    \n",
    "    print(f\"\\n6. Ready for labeling!\")\n",
    "    print(\"Next steps:\")\n",
    "    print(f\"- label_batch(labeling_data) - Interactive labeling\")\n",
    "    print(f\"- quick_label(labeling_data, [0,1,2], [0,1,2]) - Quick labeling\")\n",
    "    print(f\"- show_progress(labeling_data) - Check progress\")\n",
    "    print(f\"- export_final(datasets['{best_source}'], labeling_data, '{best_source}') - Export final dataset\")\n",
    "    \n",
    "    return datasets, labeling_data, best_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34089edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = None\n",
    "labeling_data = None \n",
    "best_source = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db8fe7",
   "metadata": {},
   "source": [
    "Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abe87b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG ANOMALY LABELING WORKFLOW\n",
      "==================================================\n",
      "1. Loading datasets...\n",
      "✓ Loaded Android_2k: 2,000 logs\n",
      "✓ Loaded Apache_2k: 2,000 logs\n",
      "✓ Loaded BGL_2k: 2,000 logs\n",
      "✓ Loaded Hadoop_2k: 2,000 logs\n",
      "✓ Loaded HDFS_2k: 2,000 logs\n",
      "✓ Loaded HealthApp_2k: 2,000 logs\n",
      "✓ Loaded HPC_2k: 2,000 logs\n",
      "✓ Loaded Linux_2k: 2,000 logs\n",
      "✓ Loaded Mac_2k: 2,000 logs\n",
      "✓ Loaded OpenSSH_2k: 2,000 logs\n",
      "✓ Loaded OpenStack_2k: 2,000 logs\n",
      "✓ Loaded Proxifier_2k: 2,000 logs\n",
      "✓ Loaded Spark_2k: 2,000 logs\n",
      "✓ Loaded Thunderbird_2k: 2,000 logs\n",
      "✓ Loaded Windows_2k: 2,000 logs\n",
      "✓ Loaded Zookeeper_2k: 2,000 logs\n",
      "\n",
      "Loaded 16 sources, 32,000 total logs\n",
      "\n",
      "2. Analyzing templates...\n",
      "\n",
      "3. Finding anomalies...\n",
      "\n",
      "4. Ranking sources...\n",
      "\n",
      "Top 3 sources for labeling:\n",
      "1. OpenSSH_2k (score: 59.2)\n",
      "   Anomaly rate: 69.5%\n",
      "   Templates: 27\n",
      "2. Apache_2k (score: 50.6)\n",
      "   Anomaly rate: 27.0%\n",
      "   Templates: 6\n",
      "3. HPC_2k (score: 39.7)\n",
      "   Anomaly rate: 24.5%\n",
      "   Templates: 46\n",
      "\n",
      "5. Preparing OpenSSH_2k for labeling...\n",
      "Loaded progress: 19/27 templates\n",
      "\n",
      "Progress: 27/27 templates (100.0%)\n",
      "Label distribution:\n",
      "  1 (security_anomaly): 1,890 logs\n",
      "  2 (system_failure): 48 logs\n",
      "\n",
      "6. Ready for labeling!\n",
      "Next steps:\n",
      "- label_batch(labeling_data) - Interactive labeling\n",
      "- quick_label(labeling_data, [0,1,2], [0,1,2]) - Quick labeling\n",
      "- show_progress(labeling_data) - Check progress\n",
      "- export_final(datasets['OpenSSH_2k'], labeling_data, 'OpenSSH_2k') - Export final dataset\n"
     ]
    }
   ],
   "source": [
    "datasets, labeling_data, best_source = run_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6e4cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEMPLATE LABELING\n",
      "Labels: 0:normal, 1:security_anomaly, 2:system_failure, 3:performance_issue, 4:network_anomaly, 5:config_error, 6:hardware_issue, 7:unknown_anomaly\n",
      "Commands: 0-7 (label), 'skip', 'quit', 'save'\n",
      "============================================================\n",
      "\n",
      "[1/27] Template\n",
      "Frequency: 413 logs (20.6%)\n",
      "Template: Received disconnect from <*>: <*>: Bye Bye [preauth]\n",
      "Samples:\n",
      "  1. Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]\n",
      "  2. Received disconnect from 202.100.179.208: 11: Bye Bye [preauth]\n",
      "Suggested: 0 (normal) - low\n",
      "\n",
      "[2/27] Template\n",
      "Frequency: 384 logs (19.2%)\n",
      "Template: pam_unix(sshd:auth): authentication failure; logname= uid=<*> euid=<*> tty=ssh ruser= rhost=<*> user=<*>\n",
      "Samples:\n",
      "  1. pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.36.59.76.dynamic-dsl-ip.omantel.net.om  user=root\n",
      "  2. pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[3/27] Template\n",
      "Frequency: 383 logs (19.1%)\n",
      "Template: Failed password for <*> from <*> port <*> ssh2\n",
      "Samples:\n",
      "  1. Failed password for root from 5.36.59.76 port 42393 ssh2\n",
      "  2. Failed password for root from 112.95.230.3 port 45378 ssh2\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[4/27] Template\n",
      "Frequency: 135 logs (6.8%)\n",
      "Template: Failed password for invalid user <*> from <*> port <*> ssh2\n",
      "Samples:\n",
      "  1. Failed password for invalid user webmaster from 173.234.31.186 port 38926 ssh2\n",
      "  2. Failed password for invalid user test9 from 52.80.34.196 port 36060 ssh2\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[5/27] Template\n",
      "Frequency: 135 logs (6.8%)\n",
      "Template: pam_unix(sshd:auth): check pass; user unknown\n",
      "Samples:\n",
      "  1. pam_unix(sshd:auth): check pass; user unknown\n",
      "  2. pam_unix(sshd:auth): check pass; user unknown\n",
      "Suggested: 0 (normal) - low\n",
      "\n",
      "Labeled 5 templates in this batch\n"
     ]
    }
   ],
   "source": [
    "current_pos, labeled_count = label_batch(labeling_data, start=0, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5e85949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEMPLATE LABELING\n",
      "Labels: 0:normal, 1:security_anomaly, 2:system_failure, 3:performance_issue, 4:network_anomaly, 5:config_error, 6:hardware_issue, 7:unknown_anomaly\n",
      "Commands: 0-7 (label), 'skip', 'quit', 'save'\n",
      "============================================================\n",
      "\n",
      "[6/27] Template\n",
      "Frequency: 113 logs (5.7%)\n",
      "Template: Invalid user <*> from <*>\n",
      "Samples:\n",
      "  1. Invalid user webmaster from 173.234.31.186\n",
      "  2. Invalid user test9 from 52.80.34.196\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[7/27] Template\n",
      "Frequency: 113 logs (5.7%)\n",
      "Template: input_userauth_request: invalid user <*> [preauth]\n",
      "Samples:\n",
      "  1. input_userauth_request: invalid user webmaster [preauth]\n",
      "  2. input_userauth_request: invalid user test9 [preauth]\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[8/27] Template\n",
      "Frequency: 110 logs (5.5%)\n",
      "Template: pam_unix(sshd:auth): authentication failure; logname= uid=<*> euid=<*> tty=ssh ruser= rhost=<*>\n",
      "Samples:\n",
      "  1. pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=173.234.31.186\n",
      "  2. pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[9/27] Template\n",
      "Frequency: 85 logs (4.2%)\n",
      "Template: reverse mapping checking getaddrinfo for <*> [<*>] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "Samples:\n",
      "  1. reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "  2. reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "Suggested: 1 (security_anomaly) - high\n",
      "\n",
      "[10/27] Template\n",
      "Frequency: 45 logs (2.2%)\n",
      "Template: error: Received disconnect from <*>: <*>: No more user authentication methods available. [preauth]\n",
      "Samples:\n",
      "  1. error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]\n",
      "  2. error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]\n",
      "Suggested: 2 (system_failure) - high\n",
      "\n",
      "Labeled 5 templates in this batch\n"
     ]
    }
   ],
   "source": [
    "current_pos, labeled_count = label_batch(labeling_data, start=current_pos, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43e20b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 19/27 templates (70.4%)\n",
      "Label distribution:\n",
      "  1 (security_anomaly): 1,890 logs\n",
      "  2 (system_failure): 48 logs\n"
     ]
    }
   ],
   "source": [
    "show_progress(labeling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b72f8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress: 19/27 templates\n"
     ]
    }
   ],
   "source": [
    "save_progress(labeling_data, best_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "372dda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\dataset\\labeled_data\\OpenSSH_2k_labeled.csv\n",
      "Total logs: 2,000\n",
      "Labeled: 1,938 (96.9%)\n",
      "Anomalies: 1,938 (100.0% of labeled)\n"
     ]
    }
   ],
   "source": [
    "final_dataset = export_final(datasets[best_source], labeling_data, best_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ae6f8",
   "metadata": {},
   "source": [
    "Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a577359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quality(labeling_data):\n",
    "    labeled = [item for item in labeling_data if item['label'] is not None]\n",
    "    \n",
    "    if not labeled:\n",
    "        print(\"No labeled data to check\")\n",
    "        return {}\n",
    "    \n",
    "    total_templates = len(labeling_data)\n",
    "    labeled_count = len(labeled)\n",
    "    total_logs = sum(item['count'] for item in labeling_data)\n",
    "    labeled_logs = sum(item['count'] for item in labeled)\n",
    "    \n",
    "    template_coverage = (labeled_count / total_templates) * 100\n",
    "    log_coverage = (labeled_logs / total_logs) * 100\n",
    "    \n",
    "    print(f\"Coverage Report:\")\n",
    "    print(f\"Templates: {labeled_count}/{total_templates} ({template_coverage:.1f}%)\")\n",
    "    print(f\"Logs: {labeled_logs:,}/{total_logs:,} ({log_coverage:.1f}%)\")\n",
    "    \n",
    "    label_counts = defaultdict(int)\n",
    "    log_counts = defaultdict(int)\n",
    "    \n",
    "    for item in labeled:\n",
    "        label = item['label']\n",
    "        label_counts[label] += 1\n",
    "        log_counts[label] += item['count']\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    for label in sorted(label_counts.keys()):\n",
    "        templates = label_counts[label]\n",
    "        logs = log_counts[label]\n",
    "        print(f\"{label} ({LABELS[label]}): {templates} templates, {logs:,} logs\")\n",
    "    \n",
    "    return {\n",
    "        'template_coverage': template_coverage,\n",
    "        'log_coverage': log_coverage,\n",
    "        'label_counts': dict(label_counts),\n",
    "        'log_counts': dict(log_counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566a295",
   "metadata": {},
   "source": [
    "Find Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_issues(labeling_data):\n",
    "    issues = []\n",
    "    \n",
    "    for i, item in enumerate(labeling_data):\n",
    "        if item['label'] is None:\n",
    "            continue\n",
    "        \n",
    "        template = item['template'].lower()\n",
    "        content = ' '.join(item['samples']).lower()\n",
    "        label = item['label']\n",
    "        \n",
    "        if label == 1:\n",
    "            security_words = ['auth', 'login', 'password', 'user', 'invalid', 'fail']\n",
    "            if not any(word in content for word in security_words):\n",
    "                issues.append(f\"Template {i}: Security label without security keywords\")\n",
    "        \n",
    "        elif label == 0:\n",
    "            error_words = ['error', 'fail', 'critical', 'exception']\n",
    "            if any(word in content for word in error_words):\n",
    "                issues.append(f\"Template {i}: Normal label with error keywords\")\n",
    "        \n",
    "        elif label > 0 and item['percentage'] > 10:\n",
    "            issues.append(f\"Template {i}: High-frequency anomaly ({item['percentage']:.1f}%)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"Found {len(issues)} potential issues:\")\n",
    "        for issue in issues[:10]:\n",
    "            print(f\"  {issue}\")\n",
    "    else:\n",
    "        print(\"No issues found - labeling looks good!\")\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e11dd",
   "metadata": {},
   "source": [
    "Group Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb61045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_similar(labeling_data):\n",
    "    unlabeled = [item for item in labeling_data if item['label'] is None]\n",
    "    \n",
    "    groups = {\n",
    "        'connection': [],\n",
    "        'auth': [],\n",
    "        'error': [],\n",
    "        'timeout': [],\n",
    "        'success': []\n",
    "    }\n",
    "    \n",
    "    for item in unlabeled:\n",
    "        text = (item['template'] + ' ' + ' '.join(item['samples'])).lower()\n",
    "        \n",
    "        if 'connect' in text or 'connection' in text:\n",
    "            groups['connection'].append(item)\n",
    "        elif 'auth' in text or 'login' in text or 'user' in text:\n",
    "            groups['auth'].append(item)\n",
    "        elif 'error' in text or 'fail' in text or 'critical' in text:\n",
    "            groups['error'].append(item)\n",
    "        elif 'timeout' in text:\n",
    "            groups['timeout'].append(item)\n",
    "        elif 'success' in text or 'ok' in text or 'complete' in text:\n",
    "            groups['success'].append(item)\n",
    "    \n",
    "    print(f\"Similar template groups:\")\n",
    "    for name, items in groups.items():\n",
    "        if items:\n",
    "            total_logs = sum(item['count'] for item in items)\n",
    "            print(f\"{name}: {len(items)} templates, {total_logs:,} logs\")\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be4a7d",
   "metadata": {},
   "source": [
    "Smart Suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_suggest(labeling_data):\n",
    "    labeled = [item for item in labeling_data if item['label'] is not None]\n",
    "    \n",
    "    label_words = defaultdict(set)\n",
    "    for item in labeled:\n",
    "        text = item['template'].lower() + ' ' + ' '.join(item['samples']).lower()\n",
    "        words = set(text.split())\n",
    "        label_words[item['label']].update(words)\n",
    "    \n",
    "    updated = 0\n",
    "    for item in labeling_data:\n",
    "        if item['label'] is not None:\n",
    "            continue\n",
    "        \n",
    "        text = item['template'].lower() + ' ' + ' '.join(item['samples']).lower()\n",
    "        words = set(text.split())\n",
    "        \n",
    "        best_label = 0\n",
    "        best_score = 0\n",
    "        \n",
    "        for label_id, pattern_words in label_words.items():\n",
    "            overlap = len(words.intersection(pattern_words))\n",
    "            score = overlap / len(pattern_words) if pattern_words else 0\n",
    "            \n",
    "            if score > best_score and score > 0.2:\n",
    "                best_score = score\n",
    "                best_label = label_id\n",
    "        \n",
    "        if best_label != item['suggested']:\n",
    "            item['suggested'] = best_label\n",
    "            item['confidence'] = \"medium\" if best_score > 0.4 else \"low\"\n",
    "            updated += 1\n",
    "    \n",
    "    print(f\"Updated {updated} suggestions based on learned patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a5822",
   "metadata": {},
   "source": [
    "Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(labeling_data, source_name):\n",
    "    report_file = OUTPUT_PATH / f\"{source_name}_report.txt\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(f\"LABELING REPORT: {source_name}\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        total = len(labeling_data)\n",
    "        labeled = sum(1 for item in labeling_data if item['label'] is not None)\n",
    "        total_logs = sum(item['count'] for item in labeling_data)\n",
    "        labeled_logs = sum(item['count'] for item in labeling_data if item['label'] is not None)\n",
    "        \n",
    "        f.write(f\"Templates: {labeled}/{total} ({labeled/total*100:.1f}%)\\n\")\n",
    "        f.write(f\"Logs: {labeled_logs:,}/{total_logs:,} ({labeled_logs/total_logs*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        label_dist = defaultdict(int)\n",
    "        for item in labeling_data:\n",
    "            if item['label'] is not None:\n",
    "                label_dist[item['label']] += item['count']\n",
    "        \n",
    "        f.write(\"Label Distribution:\\n\")\n",
    "        for label in sorted(label_dist.keys()):\n",
    "            count = label_dist[label]\n",
    "            f.write(f\"{label} ({LABELS[label]}): {count:,} logs\\n\")\n",
    "    \n",
    "    print(f\"Report saved: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d6ef8",
   "metadata": {},
   "source": [
    "Find Next Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_source(datasets, labeling_data, current_source):\n",
    "    patterns = defaultdict(set)\n",
    "    for item in labeling_data:\n",
    "        if item['label'] is not None:\n",
    "            words = item['template'].lower().split()\n",
    "            patterns[item['label']].update(words)\n",
    "    \n",
    "    remaining = [src for src in LOG_SOURCES if src != current_source]\n",
    "    scores = []\n",
    "    \n",
    "    for source in remaining:\n",
    "        try:\n",
    "            df = datasets[source]\n",
    "            if 'EventTemplate' not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            templates = df['EventTemplate'].head(100)\n",
    "            matches = 0\n",
    "            \n",
    "            for template in templates:\n",
    "                template_words = set(template.lower().split())\n",
    "                for pattern_words in patterns.values():\n",
    "                    if len(template_words.intersection(pattern_words)) > 0:\n",
    "                        matches += 1\n",
    "                        break\n",
    "            \n",
    "            score = matches / len(templates) if templates is not None and len(templates) > 0 else 0\n",
    "            scores.append((source, score, len(df)))\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Next source recommendations:\")\n",
    "    for i, (source, score, logs) in enumerate(scores[:3], 1):\n",
    "        print(f\"{i}. {source} (similarity: {score:.1f}, logs: {logs:,})\")\n",
    "    \n",
    "    return scores[0][0] if scores else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e06504",
   "metadata": {},
   "source": [
    "Export for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ml_data(final_dataset, source_name, test_size=0.2):\n",
    "    labeled = final_dataset[final_dataset['AnomalyLabel'] >= 0].copy()\n",
    "    \n",
    "    if len(labeled) == 0:\n",
    "        print(\"No labeled data to export\")\n",
    "        return\n",
    "    \n",
    "    labeled['ContentLength'] = labeled['Content'].str.len()\n",
    "    labeled['HasError'] = labeled['Content'].str.lower().str.contains('error|fail|critical')\n",
    "    labeled['HasAuth'] = labeled['Content'].str.lower().str.contains('auth|login|user')\n",
    "    \n",
    "    X = labeled[['Content', 'EventTemplate', 'ContentLength', 'HasError', 'HasAuth']]\n",
    "    y = labeled['AnomalyLabel']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_file = OUTPUT_PATH / f\"{source_name}_train.csv\"\n",
    "    test_file = OUTPUT_PATH / f\"{source_name}_test.csv\"\n",
    "    \n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(train_file, index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(test_file, index=False)\n",
    "    \n",
    "    print(f\"ML data exported:\")\n",
    "    print(f\"Train: {len(X_train):,} samples -> {train_file}\")\n",
    "    print(f\"Test: {len(X_test):,} samples -> {test_file}\")\n",
    "    \n",
    "    print(f\"Training labels:\")\n",
    "    for label, count in y_train.value_counts().sort_index().items():\n",
    "        pct = count/len(y_train)*100\n",
    "        print(f\"  {label} ({LABELS[label]}): {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d7758",
   "metadata": {},
   "source": [
    "Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_eval(labeling_data, source_name):\n",
    "    print(f\"QUICK EVALUATION: {source_name}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    quality = check_quality(labeling_data)\n",
    "    \n",
    "    issues = find_issues(labeling_data)\n",
    "    \n",
    "    groups = group_similar(labeling_data)\n",
    "    \n",
    "    smart_suggest(labeling_data)\n",
    "    \n",
    "    save_report(labeling_data, source_name)\n",
    "    \n",
    "    return quality, issues, groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9528c",
   "metadata": {},
   "source": [
    "Setup Next Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f780818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_next(datasets, current_labeling_data, current_source):\n",
    "    next_source = find_next_source(datasets, current_labeling_data, current_source)\n",
    "    \n",
    "    if not next_source:\n",
    "        print(\"No suitable next source found\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Setting up {next_source} for labeling...\")\n",
    "    \n",
    "    next_data = prep_templates(datasets[next_source], next_source)\n",
    "    \n",
    "    labeled_items = [item for item in current_labeling_data if item['label'] is not None]\n",
    "    \n",
    "    if labeled_items:\n",
    "        keyword_labels = {}\n",
    "        for item in labeled_items:\n",
    "            words = item['template'].lower().split()\n",
    "            for word in words:\n",
    "                if word not in keyword_labels:\n",
    "                    keyword_labels[word] = []\n",
    "                keyword_labels[word].append(item['label'])\n",
    "        \n",
    "        for item in next_data:\n",
    "            words = item['template'].lower().split()\n",
    "            label_votes = []\n",
    "            for word in words:\n",
    "                if word in keyword_labels:\n",
    "                    label_votes.extend(keyword_labels[word])\n",
    "            \n",
    "            if label_votes:\n",
    "                from collections import Counter\n",
    "                most_common = Counter(label_votes).most_common(1)[0][0]\n",
    "                item['suggested'] = most_common\n",
    "                item['confidence'] = \"medium\"\n",
    "    \n",
    "    auto_label(next_data)\n",
    "    \n",
    "    print(f\"Next source ready: {next_source}\")\n",
    "    print(f\"Templates to label: {len([item for item in next_data if item['label'] is None])}\")\n",
    "    \n",
    "    return next_source, next_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115eb05",
   "metadata": {},
   "source": [
    "Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ec35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_workflow(datasets, labeling_data, source_name, final_dataset):\n",
    "    print(\"COMPLETE WORKFLOW EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"1. Evaluating current labeling...\")\n",
    "    quality, issues, groups = quick_eval(labeling_data, source_name)\n",
    "    \n",
    "    print(\"\\n2. Exporting ML-ready data...\")\n",
    "    export_ml_data(final_dataset, source_name)\n",
    "    \n",
    "    print(\"\\n3. Setting up next source...\")\n",
    "    next_source, next_data = setup_next(datasets, labeling_data, source_name)\n",
    "    \n",
    "    print(\"\\n4. RECOMMENDATIONS:\")\n",
    "    \n",
    "    if quality['template_coverage'] < 80:\n",
    "        print(f\"- Complete current source (only {quality['template_coverage']:.1f}% done)\")\n",
    "    elif next_source:\n",
    "        print(f\"- Start labeling {next_source} using learned patterns\")\n",
    "        print(f\"- Focus on templates that don't match existing patterns\")\n",
    "    \n",
    "    if quality['log_coverage'] > 90:\n",
    "        print(f\"- Ready for ML model training\")\n",
    "        print(f\"- Consider ensemble methods for better accuracy\")\n",
    "    \n",
    "    if len(issues) > 5:\n",
    "        print(f\"- Review and fix {len(issues)} potential labeling issues\")\n",
    "    \n",
    "    print(f\"\\n5. NEXT ACTIONS:\")\n",
    "    print(f\"- Review issues found in quality check\")\n",
    "    print(f\"- Continue labeling current source or start next source\")\n",
    "    print(f\"- Begin ML model experiments with exported data\")\n",
    "    \n",
    "    return quality, issues, groups, next_source, next_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
