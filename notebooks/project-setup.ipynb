{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b194d9f",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dceff0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "import drain3\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2b090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8478c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "NORMALIZED_DATA_PATH = LABELED_DATA_PATH / \"normalized\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\" / \"cross_source_transfer\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\"\n",
    "\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d109febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n",
      "Project root: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\n",
      "Results path: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\cross_source_transfer\n",
      "Models path: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results path: {RESULTS_PATH}\")\n",
    "print(f\"Models path: {MODELS_PATH}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6a795",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18638697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.4.1\n",
      "Available cores: 16\n",
      "Driver memory: 18g\n",
      "Executor memory: 16g\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\"\n",
    "\n",
    "app_name = \"CrossSourceAnomalyDetection\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 20) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark {spark.version}\")\n",
    "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Executor memory: {spark.conf.get('spark.executor.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ad1105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized for CrossSourceAnomalyDetection\n",
      "Project configured for 16 log sources\n",
      "Leave-one-out cross-validation will run 16 experiments\n"
     ]
    }
   ],
   "source": [
    "PROJECT_CONFIG = {\n",
    "    'experiment_name': 'cross_source_transfer_anomaly_detection',\n",
    "    'log_sources': [\n",
    "        'Android', 'Apache', 'BGL', 'Hadoop', 'HDFS', 'HealthApp',\n",
    "        'HPC', 'Linux', 'Mac', 'OpenSSH', 'OpenStack', 'Proxifier',\n",
    "        'Spark', 'Thunderbird', 'Windows', 'Zookeeper'\n",
    "    ],\n",
    "    'few_shot_sizes': [10, 50],\n",
    "    'random_seeds': [42, 123, 456],\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'bert_model_name': 'bert-base-uncased',\n",
    "    'template_extraction_method': 'drain'\n",
    "}\n",
    "\n",
    "print(f\"Spark session initialized for {app_name}\")\n",
    "print(f\"Project configured for {len(PROJECT_CONFIG['log_sources'])} log sources\")\n",
    "print(f\"Leave-one-out cross-validation will run {len(PROJECT_CONFIG['log_sources'])} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05979b4c",
   "metadata": {},
   "source": [
    "Data Discovery and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3b7e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled datasets found: 6\n",
      "Normalized datasets found: 6\n"
     ]
    }
   ],
   "source": [
    "LOG_SOURCES = [\n",
    "    'Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', \n",
    "    'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k',\n",
    "    'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k',\n",
    "    'Windows_2k', 'Zookeeper_2k'\n",
    "]\n",
    "\n",
    "LABELS = {\n",
    "    0: \"normal\",\n",
    "    1: \"security_anomaly\", \n",
    "    2: \"system_failure\",\n",
    "    3: \"performance_issue\",\n",
    "    4: \"network_anomaly\", \n",
    "    5: \"config_error\",\n",
    "    6: \"hardware_issue\",\n",
    "    7: \"unknown_anomaly\"\n",
    "}\n",
    "\n",
    "labeled_files = list(LABELED_DATA_PATH.glob(\"*_labeled.csv\"))\n",
    "normalized_files = list(NORMALIZED_DATA_PATH.glob(\"*_normalized.csv\")) if NORMALIZED_DATA_PATH.exists() else []\n",
    "\n",
    "print(f\"Labeled datasets found: {len(labeled_files)}\")\n",
    "print(f\"Normalized datasets found: {len(normalized_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b48997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normalized file: Apache_2k_normalized.csv\n",
      "Apache       |    2,000 records | 10 cols | Labels   |  29.8% anomalies\n",
      "Loading normalized file: BGL_2k_normalized.csv\n",
      "BGL          |    2,000 records | 17 cols | Labels   |  75.0% anomalies\n",
      "Loading normalized file: HPC_2k_normalized.csv\n",
      "HPC          |    2,000 records | 14 cols | Labels   |   9.8% anomalies\n",
      "Loading normalized file: OpenSSH_2k_normalized.csv\n",
      "OpenSSH      |    2,000 records | 13 cols | Labels   |  78.8% anomalies\n",
      "Loading normalized file: Proxifier_2k_normalized.csv\n",
      "Proxifier    |    2,000 records | 10 cols | Labels   |   4.9% anomalies\n",
      "Loading normalized file: Zookeeper_2k_normalized.csv\n",
      "Zookeeper    |    2,000 records | 14 cols | Labels   |  46.2% anomalies\n"
     ]
    }
   ],
   "source": [
    "dataset_registry = {}\n",
    "total_records = 0\n",
    "total_anomalies = 0\n",
    "\n",
    "for file_path in sorted(labeled_files):\n",
    "    filename = file_path.name\n",
    "    log_source = None\n",
    "    for source in PROJECT_CONFIG['log_sources']:\n",
    "        if source.lower() in filename.lower():\n",
    "            log_source = source\n",
    "            break\n",
    "    \n",
    "    if log_source is None:\n",
    "        print(f\"Could not identify log source for {filename}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        normalized_filename = filename.replace('_labeled.csv', '_normalized.csv')\n",
    "        normalized_file_path = NORMALIZED_DATA_PATH / normalized_filename\n",
    "\n",
    "        if normalized_file_path.exists():\n",
    "            print(f\"Loading normalized file: {normalized_filename}\")\n",
    "            df = pd.read_csv(normalized_file_path)\n",
    "            file_to_register_path = normalized_file_path\n",
    "        else:\n",
    "            print(f\"Normalized file not found. Loading labeled file: {filename}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            file_to_register_path = file_path\n",
    "\n",
    "        n_records = len(df)\n",
    "        n_columns = len(df.columns)\n",
    "        \n",
    "        has_labels = 'AnomalyLabel' in df.columns\n",
    "        \n",
    "        label_stats = {}\n",
    "        if has_labels:\n",
    "            label_counts = df['AnomalyLabelName'].value_counts()\n",
    "            \n",
    "            normal_count = int(label_counts.get(\"normal\", 0))\n",
    "            \n",
    "            anomaly_count = n_records - normal_count\n",
    "            anomaly_rate = (anomaly_count / n_records) * 100 if n_records > 0 else 0\n",
    "            \n",
    "            label_stats = {\n",
    "                'normal_count': normal_count,\n",
    "                'anomaly_count': anomaly_count,\n",
    "                'anomaly_rate': anomaly_rate,\n",
    "                'unique_labels': {str(k): int(v) for k, v in label_counts.to_dict().items()}\n",
    "            }\n",
    "            \n",
    "            total_records += n_records\n",
    "            total_anomalies += anomaly_count\n",
    "        \n",
    "        has_normalized_timestamp = 'timestamp_normalized' in df.columns\n",
    "        \n",
    "        dataset_registry[log_source] = {\n",
    "            'file_path': file_path,\n",
    "            'n_records': n_records,\n",
    "            'n_columns': n_columns,\n",
    "            'columns': list(df.columns),\n",
    "            'has_labels': has_labels,\n",
    "            'has_normalized_timestamp': has_normalized_timestamp,\n",
    "            'label_stats': label_stats,\n",
    "            'size_mb': file_path.stat().st_size / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "        print(f\"{log_source:<12} | {n_records:>8,} records | {n_columns:>2} cols | \"\n",
    "              f\"{'Labels' if has_labels else 'No Labels':<8} | \"\n",
    "              f\"{anomaly_rate:>5.1f}% anomalies\" if has_labels else \"\")\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d7b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully registered: 6/16 log sources\n",
      "Total records: 12,000\n",
      "Total anomalies: 4,888\n",
      "Overall anomaly rate: 40.73%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSuccessfully registered: {len(dataset_registry)}/{len(PROJECT_CONFIG['log_sources'])} log sources\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Total anomalies: {total_anomalies:,}\")\n",
    "if total_records > 0:\n",
    "    print(f\"Overall anomaly rate: {(total_anomalies/total_records*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1f27b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Missing sources: {'HDFS', 'Android', 'Thunderbird', 'Linux', 'Hadoop', 'HealthApp', 'OpenStack', 'Windows', 'Mac', 'Spark'}\n",
      "Updating project config to only include available sources...\n"
     ]
    }
   ],
   "source": [
    "missing_sources = set(PROJECT_CONFIG['log_sources']) - set(dataset_registry.keys())\n",
    "if missing_sources:\n",
    "    print(f\"⚠️  Missing sources: {missing_sources}\")\n",
    "    print(\"Updating project config to only include available sources...\")\n",
    "    PROJECT_CONFIG['log_sources'] = list(dataset_registry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e7e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-one-source-out experiments: 6\n",
      "Few-shot adaptation sizes: [10, 50]\n",
      "Random seeds for reproducibility: [42, 123, 456]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Leave-one-source-out experiments: {len(PROJECT_CONFIG['log_sources'])}\")\n",
    "print(f\"Few-shot adaptation sizes: {PROJECT_CONFIG['few_shot_sizes']}\")\n",
    "print(f\"Random seeds for reproducibility: {PROJECT_CONFIG['random_seeds']}\")\n",
    "\n",
    "registry_path = RESULTS_PATH / \"dataset_registry.json\"\n",
    "with open(registry_path, 'w') as f:\n",
    "    registry_for_export = {}\n",
    "    for source, info in dataset_registry.items():\n",
    "        registry_for_export[source] = {\n",
    "            'file_path': str(info['file_path']),\n",
    "            'n_records': info['n_records'],\n",
    "            'n_columns': info['n_columns'],\n",
    "            'columns': info['columns'],\n",
    "            'has_labels': info['has_labels'],\n",
    "            'has_normalized_timestamp': info['has_normalized_timestamp'],\n",
    "            'label_stats': info['label_stats'],\n",
    "            'size_mb': info['size_mb']\n",
    "        }\n",
    "    \n",
    "    json.dump({\n",
    "        'dataset_registry': registry_for_export,\n",
    "        'project_config': PROJECT_CONFIG,\n",
    "        'total_records': total_records,\n",
    "        'total_anomalies': total_anomalies,\n",
    "        'analysis_timestamp': datetime.now().isoformat()\n",
    "    }, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
