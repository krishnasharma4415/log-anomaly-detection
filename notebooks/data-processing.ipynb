{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfd5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels:\n",
      "  0: normal\n",
      "  1: security_anomaly\n",
      "  2: system_failure\n",
      "  3: performance_issue\n",
      "  4: network_anomaly\n",
      "  5: config_error\n",
      "  6: hardware_issue\n",
      "Dataset path: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\dataset\\labeled_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: 'normal',\n",
    "    1: 'security_anomaly',\n",
    "    2: 'system_failure',\n",
    "    3: 'performance_issue',\n",
    "    4: 'network_anomaly',\n",
    "    5: 'config_error',\n",
    "    6: 'hardware_issue'\n",
    "}\n",
    "\n",
    "print(\"Class Labels:\")\n",
    "for label_id, label_name in LABEL_MAP.items():\n",
    "    print(f\"  {label_id}: {label_name}\")\n",
    "print(f\"Dataset path: {LABELED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ba07c",
   "metadata": {},
   "source": [
    "Timestamp parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da424d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_android_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}-{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_apache_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(time_str, \"%a %b %d %H:%M:%S %Y\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_bgl_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "        return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hadoop_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hdfs_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        year = \"20\" + date_str[:2]\n",
    "        month = date_str[2:4]\n",
    "        day = date_str[4:6]\n",
    "        hour = time_str[:2]\n",
    "        minute = time_str[2:4]\n",
    "        second = time_str[4:6]\n",
    "        dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_healthapp_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) >= 4:\n",
    "            time_str = ':'.join(parts[:-1]) + '.' + parts[-1]\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d-%H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hpc_timestamp(row):\n",
    "    try:\n",
    "        timestamp = int(str(row['Time']).strip())\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_linux_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_mac_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openssh_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Date']).strip()\n",
    "        day_str = str(row['Day']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openstack_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_proxifier_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}.{time_str}\", \"%Y.%m.%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_spark_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"20{date_str} {time_str}\", \"%Y/%m/%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_thunderbird_timestamp(row):\n",
    "    try:\n",
    "        if 'Month' in row and 'Day' in row and 'Time' in row:\n",
    "            month_str = str(row['Month']).strip()\n",
    "            day_str = str(row['Day']).strip()\n",
    "            time_str = str(row['Time']).strip()\n",
    "            current_year = datetime.now().year\n",
    "            dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "        elif 'Date' in row:\n",
    "            date_str = str(row['Date']).strip()\n",
    "            dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "            return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_windows_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_zookeeper_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b03e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_log_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'android' in filename:\n",
    "        return 'android'\n",
    "    elif 'apache' in filename:\n",
    "        return 'apache'\n",
    "    elif 'bgl' in filename:\n",
    "        return 'bgl'\n",
    "    elif 'hadoop' in filename:\n",
    "        return 'hadoop'\n",
    "    elif 'hdfs' in filename:\n",
    "        return 'hdfs'\n",
    "    elif 'health' in filename:\n",
    "        return 'healthapp'\n",
    "    elif 'hpc' in filename:\n",
    "        return 'hpc'\n",
    "    elif 'linux' in filename:\n",
    "        return 'linux'\n",
    "    elif 'mac' in filename:\n",
    "        return 'mac'\n",
    "    elif 'openssh' in filename:\n",
    "        return 'openssh'\n",
    "    elif 'openstack' in filename:\n",
    "        return 'openstack'\n",
    "    elif 'proxifier' in filename:\n",
    "        return 'proxifier'\n",
    "    elif 'spark' in filename:\n",
    "        return 'spark'\n",
    "    elif 'thunderbird' in filename:\n",
    "        return 'thunderbird'\n",
    "    elif 'windows' in filename:\n",
    "        return 'windows'\n",
    "    elif 'zookeeper' in filename or 'zookeper' in filename:\n",
    "        return 'zookeeper'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "timestamp_parsers = {\n",
    "    'android': parse_android_timestamp,\n",
    "    'apache': parse_apache_timestamp,\n",
    "    'bgl': parse_bgl_timestamp,\n",
    "    'hadoop': parse_hadoop_timestamp,\n",
    "    'hdfs': parse_hdfs_timestamp,\n",
    "    'healthapp': parse_healthapp_timestamp,\n",
    "    'hpc': parse_hpc_timestamp,\n",
    "    'linux': parse_linux_timestamp,\n",
    "    'mac': parse_mac_timestamp,\n",
    "    'openssh': parse_openssh_timestamp,\n",
    "    'openstack': parse_openstack_timestamp,\n",
    "    'proxifier': parse_proxifier_timestamp,\n",
    "    'spark': parse_spark_timestamp,\n",
    "    'thunderbird': parse_thunderbird_timestamp,\n",
    "    'windows': parse_windows_timestamp,\n",
    "    'zookeeper': parse_zookeeper_timestamp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1330be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 labeled CSV files\n"
     ]
    }
   ],
   "source": [
    "csv_files = list(LABELED_DATA_PATH.glob(\"*_labeled.csv\"))\n",
    "print(f\"Found {len(csv_files)} labeled CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c590c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: Apache_2k_labeled.csv\n",
      "Size: 0.29 MB\n",
      "Detected log type: apache\n",
      "Loaded dataframe: (2000, 9)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 3/7\n",
      "  Present: ['normal', 'security_anomaly', 'system_failure']\n",
      "  ⚠️  Missing: ['performance_issue', 'network_anomaly', 'config_error', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,405 (70.25%)\n",
      "    1 (security_anomaly): 32 (1.60%)\n",
      "    2 (system_failure): 563 (28.15%)\n",
      "\n",
      "  Imbalance ratio: 43.91:1 ⚠️ HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: BGL_2k_labeled.csv\n",
      "Size: 0.45 MB\n",
      "Detected log type: bgl\n",
      "Loaded dataframe: (2000, 16)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 5/7\n",
      "  Present: ['normal', 'security_anomaly', 'system_failure', 'config_error', 'hardware_issue']\n",
      "  ⚠️  Missing: ['performance_issue', 'network_anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 501 (25.05%)\n",
      "    1 (security_anomaly): 150 (7.50%)\n",
      "    2 (system_failure): 913 (45.65%)\n",
      "    5 (config_error): 73 (3.65%)\n",
      "    6 (hardware_issue): 363 (18.15%)\n",
      "\n",
      "  Imbalance ratio: 12.51:1 ⚠️ HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Hadoop_2k_labeled.csv\n",
      "Size: 0.56 MB\n",
      "Detected log type: hadoop\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 5/7\n",
      "  Present: ['normal', 'system_failure', 'performance_issue', 'network_anomaly', 'config_error']\n",
      "  ⚠️  Missing: ['security_anomaly', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,217 (60.85%)\n",
      "    2 (system_failure): 156 (7.80%)\n",
      "    3 (performance_issue): 1 (0.05%)\n",
      "    4 (network_anomaly): 625 (31.25%)\n",
      "    5 (config_error): 1 (0.05%)\n",
      "\n",
      "  Imbalance ratio: 1217.00:1 ⚠️ EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: HDFS_2k_labeled.csv\n",
      "Size: 0.43 MB\n",
      "Detected log type: hdfs\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 237/2000 (11.8%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/7\n",
      "  Present: ['normal', 'system_failure']\n",
      "  ⚠️  Missing: ['security_anomaly', 'performance_issue', 'network_anomaly', 'config_error', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,920 (96.00%)\n",
      "    2 (system_failure): 80 (4.00%)\n",
      "\n",
      "  Imbalance ratio: 24.00:1 ⚠️ HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: HPC_2k_labeled.csv\n",
      "Size: 0.24 MB\n",
      "Detected log type: hpc\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 6/7\n",
      "  Present: ['normal', 'system_failure', 'performance_issue', 'network_anomaly', 'config_error', 'hardware_issue']\n",
      "  ⚠️  Missing: ['security_anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,804 (90.20%)\n",
      "    2 (system_failure): 60 (3.00%)\n",
      "    3 (performance_issue): 29 (1.45%)\n",
      "    4 (network_anomaly): 85 (4.25%)\n",
      "    5 (config_error): 4 (0.20%)\n",
      "    6 (hardware_issue): 18 (0.90%)\n",
      "\n",
      "  Imbalance ratio: 451.00:1 ⚠️ EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: Linux_2k_labeled.csv\n",
      "Size: 0.36 MB\n",
      "Detected log type: linux\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 7/7\n",
      "  Present: ['normal', 'security_anomaly', 'system_failure', 'performance_issue', 'network_anomaly', 'config_error', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,359 (67.95%)\n",
      "    1 (security_anomaly): 527 (26.35%)\n",
      "    2 (system_failure): 46 (2.30%)\n",
      "    3 (performance_issue): 15 (0.75%)\n",
      "    4 (network_anomaly): 34 (1.70%)\n",
      "    5 (config_error): 5 (0.25%)\n",
      "    6 (hardware_issue): 14 (0.70%)\n",
      "\n",
      "  Imbalance ratio: 271.80:1 ⚠️ EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: OpenSSH_2k_labeled.csv\n",
      "Size: 0.39 MB\n",
      "Detected log type: openssh\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 3/7\n",
      "  Present: ['normal', 'security_anomaly', 'config_error']\n",
      "  ⚠️  Missing: ['system_failure', 'performance_issue', 'network_anomaly', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 424 (21.20%)\n",
      "    1 (security_anomaly): 1,569 (78.45%)\n",
      "    5 (config_error): 7 (0.35%)\n",
      "\n",
      "  Imbalance ratio: 224.14:1 ⚠️ EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: Proxifier_2k_labeled.csv\n",
      "Size: 0.38 MB\n",
      "Detected log type: proxifier\n",
      "Loaded dataframe: (2000, 9)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/7\n",
      "  Present: ['normal', 'network_anomaly']\n",
      "  ⚠️  Missing: ['security_anomaly', 'system_failure', 'performance_issue', 'config_error', 'hardware_issue']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,903 (95.15%)\n",
      "    4 (network_anomaly): 97 (4.85%)\n",
      "\n",
      "  Imbalance ratio: 19.62:1 ⚠️ HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Zookeeper_2k_labeled.csv\n",
      "Size: 0.41 MB\n",
      "Detected log type: zookeeper\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "📊 CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 6/7\n",
      "  Present: ['normal', 'system_failure', 'performance_issue', 'network_anomaly', 'config_error', 'hardware_issue']\n",
      "  ⚠️  Missing: ['security_anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,075 (53.75%)\n",
      "    2 (system_failure): 4 (0.20%)\n",
      "    3 (performance_issue): 37 (1.85%)\n",
      "    4 (network_anomaly): 873 (43.65%)\n",
      "    5 (config_error): 6 (0.30%)\n",
      "    6 (hardware_issue): 5 (0.25%)\n",
      "\n",
      "  Imbalance ratio: 268.75:1 ⚠️ EXTREME IMBALANCE!\n"
     ]
    }
   ],
   "source": [
    "processed_files = {}\n",
    "source_class_analysis = {}\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {file_path.name}\")\n",
    "    print(f\"Size: {file_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    log_type = detect_log_type(file_path.name)\n",
    "    print(f\"Detected log type: {log_type}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded dataframe: {df.shape}\")\n",
    "        \n",
    "        if log_type != 'unknown' and log_type in timestamp_parsers:\n",
    "            parser_func = timestamp_parsers[log_type]\n",
    "            df['timestamp_normalized'] = df.apply(parser_func, axis=1)\n",
    "            \n",
    "            successful = df['timestamp_normalized'].notna().sum()\n",
    "            total = len(df)\n",
    "            print(f\"Normalized timestamps: {successful}/{total} ({successful/total*100:.1f}%)\")\n",
    "            \n",
    "            df['timestamp_dt'] = pd.to_datetime(df['timestamp_normalized'], errors='coerce')\n",
    "            \n",
    "            df['hour'] = df['timestamp_dt'].dt.hour\n",
    "            df['day_of_week'] = df['timestamp_dt'].dt.dayofweek\n",
    "            df['day_of_month'] = df['timestamp_dt'].dt.day\n",
    "            df['month'] = df['timestamp_dt'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "            df['is_business_hours'] = df['hour'].between(9, 17).astype(int)\n",
    "            df['is_night'] = df['hour'].between(0, 6).astype(int)\n",
    "            \n",
    "            df = df.sort_values('timestamp_dt').reset_index(drop=True)\n",
    "            df['time_diff_seconds'] = df['timestamp_dt'].diff().dt.total_seconds().fillna(0)\n",
    "            \n",
    "            df['log_index'] = range(len(df))\n",
    "            df['logs_last_10'] = df.groupby(pd.Grouper(key='timestamp_dt', freq='1min'))['log_index'].transform('count')\n",
    "            \n",
    "            if 'AnomalyLabel' in df.columns:\n",
    "                df['AnomalyLabel'] = df['AnomalyLabel'].fillna(0).astype(int).clip(0, 6)\n",
    "                \n",
    "                unique_labels = df['AnomalyLabel'].unique()\n",
    "                present_classes = sorted([int(x) for x in unique_labels])\n",
    "                missing_classes = [i for i in range(7) if i not in present_classes]\n",
    "                \n",
    "                label_counts = df['AnomalyLabel'].value_counts().sort_index()\n",
    "                \n",
    "                print(f\"\\n📊 CLASS DISTRIBUTION ANALYSIS:\")\n",
    "                print(f\"  Classes present: {len(present_classes)}/7\")\n",
    "                print(f\"  Present: {[LABEL_MAP[i] for i in present_classes]}\")\n",
    "                if missing_classes:\n",
    "                    print(f\"  ⚠️  Missing: {[LABEL_MAP[i] for i in missing_classes]}\")\n",
    "                \n",
    "                print(f\"\\n  Distribution:\")\n",
    "                for label in present_classes:\n",
    "                    count = label_counts.get(label, 0)\n",
    "                    label_name = LABEL_MAP[label]\n",
    "                    percentage = (count / len(df) * 100)\n",
    "                    print(f\"    {label} ({label_name}): {count:,} ({percentage:.2f}%)\")\n",
    "                \n",
    "                class_counts = [label_counts.get(i, 0) for i in present_classes]\n",
    "                if len(class_counts) > 1:\n",
    "                    imbalance_ratio = max(class_counts) / min([c for c in class_counts if c > 0])\n",
    "                    print(f\"\\n  Imbalance ratio: {imbalance_ratio:.2f}:1\", end=\"\")\n",
    "                    if imbalance_ratio > 100:\n",
    "                        print(\" ⚠️ EXTREME IMBALANCE!\")\n",
    "                    elif imbalance_ratio > 10:\n",
    "                        print(\" ⚠️ HIGH IMBALANCE\")\n",
    "                    elif imbalance_ratio > 5:\n",
    "                        print(\" ⚠️ MODERATE IMBALANCE\")\n",
    "                    else:\n",
    "                        print(\" ✓\")\n",
    "                \n",
    "                source_class_analysis[file_path.stem] = {\n",
    "                    'present_classes': present_classes,\n",
    "                    'missing_classes': missing_classes,\n",
    "                    'class_counts': {int(k): int(v) for k, v in label_counts.items()},\n",
    "                    'total_samples': len(df),\n",
    "                    'imbalance_ratio': imbalance_ratio if len(class_counts) > 1 else 0\n",
    "                }\n",
    "            \n",
    "            processed_files[file_path.name] = {\n",
    "                'dataframe': df,\n",
    "                'log_type': log_type,\n",
    "                'file_path': file_path\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(f\"Skipping - unknown type\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa98565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Apache_2k_enhanced.csv\n",
      "Saved: BGL_2k_enhanced.csv\n",
      "Saved: Hadoop_2k_enhanced.csv\n",
      "Saved: HDFS_2k_enhanced.csv\n",
      "Saved: HPC_2k_enhanced.csv\n",
      "Saved: Linux_2k_enhanced.csv\n",
      "Saved: OpenSSH_2k_enhanced.csv\n",
      "Saved: Proxifier_2k_enhanced.csv\n",
      "Saved: Zookeeper_2k_enhanced.csv\n"
     ]
    }
   ],
   "source": [
    "normalized_output_path = LABELED_DATA_PATH / \"normalized\"\n",
    "normalized_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "for filename, data in processed_files.items():\n",
    "    df = data['dataframe']\n",
    "    output_filename = filename.replace('_labeled.csv', '_enhanced.csv')\n",
    "    output_path = normalized_output_path / output_filename\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2952ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-SOURCE CLASS AVAILABILITY ANALYSIS\n",
      "\n",
      "Class availability across sources:\n",
      "\n",
      "0 (normal):\n",
      "  Available in: 9/9 sources (100.0%)\n",
      "  Sources: Apache_2k_labeled, BGL_2k_labeled, Hadoop_2k_labeled, HDFS_2k_labeled, HPC_2k_labeled ...\n",
      "\n",
      "1 (security_anomaly):\n",
      "  Available in: 4/9 sources (44.4%)\n",
      "  ⚠️ LOW AVAILABILITY - Limited training data\n",
      "  Sources: Apache_2k_labeled, BGL_2k_labeled, Linux_2k_labeled, OpenSSH_2k_labeled\n",
      "\n",
      "2 (system_failure):\n",
      "  Available in: 7/9 sources (77.8%)\n",
      "  Sources: Apache_2k_labeled, BGL_2k_labeled, Hadoop_2k_labeled, HDFS_2k_labeled, HPC_2k_labeled ...\n",
      "\n",
      "3 (performance_issue):\n",
      "  Available in: 4/9 sources (44.4%)\n",
      "  ⚠️ LOW AVAILABILITY - Limited training data\n",
      "  Sources: Hadoop_2k_labeled, HPC_2k_labeled, Linux_2k_labeled, Zookeeper_2k_labeled\n",
      "\n",
      "4 (network_anomaly):\n",
      "  Available in: 5/9 sources (55.6%)\n",
      "  Sources: Hadoop_2k_labeled, HPC_2k_labeled, Linux_2k_labeled, Proxifier_2k_labeled, Zookeeper_2k_labeled\n",
      "\n",
      "5 (config_error):\n",
      "  Available in: 6/9 sources (66.7%)\n",
      "  Sources: BGL_2k_labeled, Hadoop_2k_labeled, HPC_2k_labeled, Linux_2k_labeled, OpenSSH_2k_labeled ...\n",
      "\n",
      "6 (hardware_issue):\n",
      "  Available in: 4/9 sources (44.4%)\n",
      "  ⚠️ LOW AVAILABILITY - Limited training data\n",
      "  Sources: BGL_2k_labeled, HPC_2k_labeled, Linux_2k_labeled, Zookeeper_2k_labeled\n"
     ]
    }
   ],
   "source": [
    "print(\"CROSS-SOURCE CLASS AVAILABILITY ANALYSIS\")\n",
    "all_sources = list(source_class_analysis.keys())\n",
    "class_availability = {i: [] for i in range(7)}\n",
    "\n",
    "for source, analysis in source_class_analysis.items():\n",
    "    for cls in range(7):\n",
    "        if cls in analysis['present_classes']:\n",
    "            class_availability[cls].append(source)\n",
    "\n",
    "print(\"\\nClass availability across sources:\")\n",
    "for cls in range(7):\n",
    "    sources_with_class = class_availability[cls]\n",
    "    coverage = len(sources_with_class) / len(all_sources) * 100 if all_sources else 0\n",
    "    print(f\"\\n{cls} ({LABEL_MAP[cls]}):\")\n",
    "    print(f\"  Available in: {len(sources_with_class)}/{len(all_sources)} sources ({coverage:.1f}%)\")\n",
    "    if coverage < 50:\n",
    "        print(f\"  ⚠️ LOW AVAILABILITY - Limited training data\")\n",
    "    if sources_with_class:\n",
    "        print(f\"  Sources: {', '.join(sources_with_class[:5])}{' ...' if len(sources_with_class) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9e6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔴 CRITICAL: 5 sources have extreme imbalance (>100:1)\n",
      "   → Use SMOTE with careful k-neighbors selection\n",
      "   → Apply class weights in model training\n",
      "   → Consider focal loss for deep learning\n"
     ]
    }
   ],
   "source": [
    "recommendations = []\n",
    "\n",
    "extreme_imbalance_sources = [s for s, a in source_class_analysis.items() if a['imbalance_ratio'] > 100]\n",
    "if extreme_imbalance_sources:\n",
    "    recommendations.append(f\"🔴 CRITICAL: {len(extreme_imbalance_sources)} sources have extreme imbalance (>100:1)\")\n",
    "    recommendations.append(\"   → Use SMOTE with careful k-neighbors selection\")\n",
    "    recommendations.append(\"   → Apply class weights in model training\")\n",
    "    recommendations.append(\"   → Consider focal loss for deep learning\")\n",
    "\n",
    "rare_classes = [cls for cls, sources in class_availability.items() if len(sources) < len(all_sources) * 0.3]\n",
    "if rare_classes:\n",
    "    rare_names = [LABEL_MAP[c] for c in rare_classes]\n",
    "    recommendations.append(f\"\\n🟡 WARNING: {len(rare_classes)} classes are rare across sources\")\n",
    "    recommendations.append(f\"   Classes: {', '.join(rare_names)}\")\n",
    "    recommendations.append(\"   → Use stratified cross-validation\")\n",
    "    recommendations.append(\"   → Consider hierarchical classification (binary first, then multi-class)\")\n",
    "    recommendations.append(\"   → Use transfer learning from sources with these classes\")\n",
    "\n",
    "missing_in_all = [cls for cls, sources in class_availability.items() if len(sources) == 0]\n",
    "if missing_in_all:\n",
    "    recommendations.append(f\"\\n🔴 CRITICAL: {len(missing_in_all)} classes missing from ALL sources!\")\n",
    "    recommendations.append(\"   → Cannot train on these classes\")\n",
    "    recommendations.append(\"   → Consider reducing to fewer classes or synthetic data generation\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"✓ Data appears reasonably balanced for multi-class training\")\n",
    "    recommendations.append(\"  → Still recommend using class weights and stratified sampling\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28220205",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_metadata = {\n",
    "    'num_classes': 7,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'source_analysis': {k: {**v, 'present_classes': [int(x) for x in v['present_classes']], \n",
    "                             'missing_classes': [int(x) for x in v['missing_classes']]} \n",
    "                        for k, v in source_class_analysis.items()},\n",
    "    'class_availability': {int(k): v for k, v in class_availability.items()},\n",
    "    'recommendations': recommendations,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = normalized_output_path / \"imbalance_analysis.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(imbalance_metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
