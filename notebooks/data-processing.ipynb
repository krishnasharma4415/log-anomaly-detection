{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cfd5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da424d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_android_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}-{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_apache_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(time_str, \"%a %b %d %H:%M:%S %Y\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_bgl_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "        return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hadoop_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hdfs_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        \n",
    "        year = \"20\" + date_str[:2]\n",
    "        month = date_str[2:4]\n",
    "        day = date_str[4:6]\n",
    "        \n",
    "        hour = time_str[:2]\n",
    "        minute = time_str[2:4]\n",
    "        second = time_str[4:6]\n",
    "        \n",
    "        dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_healthapp_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) >= 4:\n",
    "            time_str = ':'.join(parts[:-1]) + '.' + parts[-1]\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d-%H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hpc_timestamp(row):\n",
    "    try:\n",
    "        timestamp = int(str(row['Time']).strip())\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_linux_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_mac_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openssh_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Date']).strip() \n",
    "        day_str = str(row['Day']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openstack_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_proxifier_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}.{time_str}\", \"%Y.%m.%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_spark_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"20{date_str} {time_str}\", \"%Y/%m/%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_thunderbird_timestamp(row):\n",
    "    try:\n",
    "        if 'Month' in row and 'Day' in row and 'Time' in row:\n",
    "            month_str = str(row['Month']).strip()\n",
    "            day_str = str(row['Day']).strip()\n",
    "            time_str = str(row['Time']).strip()\n",
    "            current_year = datetime.now().year\n",
    "            dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "        elif 'Date' in row:\n",
    "            date_str = str(row['Date']).strip()\n",
    "            dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "            return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_windows_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_zookeeper_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b03e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_log_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'android' in filename:\n",
    "        return 'android'\n",
    "    elif 'apache' in filename:\n",
    "        return 'apache'\n",
    "    elif 'bgl' in filename:\n",
    "        return 'bgl'\n",
    "    elif 'hadoop' in filename:\n",
    "        return 'hadoop'\n",
    "    elif 'hdfs' in filename:\n",
    "        return 'hdfs'\n",
    "    elif 'health' in filename:\n",
    "        return 'healthapp'\n",
    "    elif 'hpc' in filename:\n",
    "        return 'hpc'\n",
    "    elif 'linux' in filename:\n",
    "        return 'linux'\n",
    "    elif 'mac' in filename:\n",
    "        return 'mac'\n",
    "    elif 'openssh' in filename:\n",
    "        return 'openssh'\n",
    "    elif 'openstack' in filename:\n",
    "        return 'openstack'\n",
    "    elif 'proxifier' in filename:\n",
    "        return 'proxifier'\n",
    "    elif 'spark' in filename:\n",
    "        return 'spark'\n",
    "    elif 'thunderbird' in filename:\n",
    "        return 'thunderbird'\n",
    "    elif 'windows' in filename:\n",
    "        return 'windows'\n",
    "    elif 'zookeeper' in filename or 'zookeper' in filename:\n",
    "        return 'zookeeper'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "timestamp_parsers = {\n",
    "    'android': parse_android_timestamp,\n",
    "    'apache': parse_apache_timestamp,\n",
    "    'bgl': parse_bgl_timestamp,\n",
    "    'hadoop': parse_hadoop_timestamp,\n",
    "    'hdfs': parse_hdfs_timestamp,\n",
    "    'healthapp': parse_healthapp_timestamp,\n",
    "    'hpc': parse_hpc_timestamp,\n",
    "    'linux': parse_linux_timestamp,\n",
    "    'mac': parse_mac_timestamp,\n",
    "    'openssh': parse_openssh_timestamp,\n",
    "    'openstack': parse_openstack_timestamp,\n",
    "    'proxifier': parse_proxifier_timestamp,\n",
    "    'spark': parse_spark_timestamp,\n",
    "    'thunderbird': parse_thunderbird_timestamp,\n",
    "    'windows': parse_windows_timestamp,\n",
    "    'zookeeper': parse_zookeeper_timestamp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5618762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps_in_df(df, log_type):\n",
    "    \"\"\"\n",
    "    Normalize timestamps in a dataframe based on log type\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame\n",
    "    log_type: string indicating the log type\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with normalized timestamps\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"Normalizing timestamps for {log_type} logs...\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    if log_type in timestamp_parsers:\n",
    "        parser_func = timestamp_parsers[log_type]\n",
    "        \n",
    "        # Apply the parser function row-wise\n",
    "        df['timestamp_normalized'] = df.apply(parser_func, axis=1)\n",
    "        \n",
    "        # Count successful conversions\n",
    "        successful = df['timestamp_normalized'].notna().sum()\n",
    "        total = len(df)\n",
    "        print(f\"Successfully normalized {successful}/{total} timestamps ({successful/total*100:.1f}%)\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(\"\\nExample conversions:\")\n",
    "        examples = df[df['timestamp_normalized'].notna()].head(3)\n",
    "        \n",
    "        # Show original timestamp components and normalized result\n",
    "        for idx, row in examples.iterrows():\n",
    "            original_parts = []\n",
    "            \n",
    "            # Collect relevant timestamp columns based on log type\n",
    "            if log_type == 'android':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'hadoop':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'hdfs':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'linux':\n",
    "                original_parts.append(f\"Month: {row.get('Month', 'N/A')}, Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'mac':\n",
    "                original_parts.append(f\"Month: {row.get('Month', 'N/A')}, Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'openssh':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Day: {row.get('Day', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'openstack':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'spark':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'thunderbird':\n",
    "                original_parts.append(f\"Month: {row.get('Month', 'N/A')}, Day: {row.get('Day', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'windows':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            elif log_type == 'zookeeper':\n",
    "                original_parts.append(f\"Date: {row.get('Date', 'N/A')}, Time: {row.get('Time', 'N/A')}\")\n",
    "            else:\n",
    "                # For single column timestamps\n",
    "                time_cols = [col for col in row.index if col.lower() in ['time', 'date', 'timestamp']]\n",
    "                if time_cols:\n",
    "                    original_parts.append(f\"{time_cols[0]}: {row.get(time_cols[0], 'N/A')}\")\n",
    "            \n",
    "            original_str = original_parts[0] if original_parts else \"N/A\"\n",
    "            print(f\"  Original: {original_str}\")\n",
    "            print(f\"  Normalized: {row['timestamp_normalized']}\")\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(f\"Warning: Unknown log type '{log_type}'. Supported types: {list(timestamp_parsers.keys())}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63f485cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 labeled CSV files\n"
     ]
    }
   ],
   "source": [
    "csv_files = list(LABELED_DATA_PATH.glob(\"*_labeled.csv\"))\n",
    "print(f\"Found {len(csv_files)} labeled CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71699af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: Apache_2k_labeled.csv\n",
      "Size: 0.29 MB\n",
      "Detected log type: apache\n",
      "Loaded dataframe with shape: (2000, 9)\n",
      "Columns: ['LineId', 'Time', 'Level', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for apache logs...\n",
      "Available columns: ['LineId', 'Time', 'Level', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Time: Sun Dec 04 04:47:44 2005\n",
      "  Normalized: 2005-12-04 04:47:44.000\n",
      "\n",
      "  Original: Time: Sun Dec 04 04:47:44 2005\n",
      "  Normalized: 2005-12-04 04:47:44.000\n",
      "\n",
      "  Original: Time: Sun Dec 04 04:51:08 2005\n",
      "  Normalized: 2005-12-04 04:51:08.000\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: BGL_2k_labeled.csv\n",
      "Size: 0.45 MB\n",
      "Detected log type: bgl\n",
      "Loaded dataframe with shape: (2000, 16)\n",
      "Columns: ['LineId', 'Label', 'Timestamp', 'Date', 'Node', 'Time', 'NodeRepeat', 'Type', 'Component', 'Level', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for bgl logs...\n",
      "Available columns: ['LineId', 'Label', 'Timestamp', 'Date', 'Node', 'Time', 'NodeRepeat', 'Type', 'Component', 'Level', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Timestamp: 1117838570\n",
      "  Normalized: 2005-06-03 00:00:00.000\n",
      "\n",
      "  Original: Timestamp: 1117838573\n",
      "  Normalized: 2005-06-03 00:00:00.000\n",
      "\n",
      "  Original: Timestamp: 1117838976\n",
      "  Normalized: 2005-06-03 00:00:00.000\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: HPC_2k_labeled.csv\n",
      "Size: 0.24 MB\n",
      "Detected log type: hpc\n",
      "Loaded dataframe with shape: (2000, 13)\n",
      "Columns: ['LineId', 'LogId', 'Node', 'Component', 'State', 'Time', 'Flag', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for hpc logs...\n",
      "Available columns: ['LineId', 'LogId', 'Node', 'Component', 'State', 'Time', 'Flag', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Time: 1077804742\n",
      "  Normalized: 2004-02-26 19:42:22.000\n",
      "\n",
      "  Original: Time: 1084680778\n",
      "  Normalized: 2004-05-16 09:42:58.000\n",
      "\n",
      "  Original: Time: 1084270955\n",
      "  Normalized: 2004-05-11 15:52:35.000\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: OpenSSH_2k_labeled.csv\n",
      "Size: 0.39 MB\n",
      "Detected log type: openssh\n",
      "Loaded dataframe with shape: (2000, 12)\n",
      "Columns: ['LineId', 'Date', 'Day', 'Time', 'Component', 'Pid', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for openssh logs...\n",
      "Available columns: ['LineId', 'Date', 'Day', 'Time', 'Component', 'Pid', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Date: Dec, Day: 10, Time: 06:55:46\n",
      "  Normalized: 2025-12-10 06:55:46.000\n",
      "\n",
      "  Original: Date: Dec, Day: 10, Time: 06:55:46\n",
      "  Normalized: 2025-12-10 06:55:46.000\n",
      "\n",
      "  Original: Date: Dec, Day: 10, Time: 06:55:46\n",
      "  Normalized: 2025-12-10 06:55:46.000\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Proxifier_2k_labeled.csv\n",
      "Size: 0.38 MB\n",
      "Detected log type: proxifier\n",
      "Loaded dataframe with shape: (2000, 9)\n",
      "Columns: ['LineId', 'Time', 'Program', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for proxifier logs...\n",
      "Available columns: ['LineId', 'Time', 'Program', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Time: 10.30 16:49:06\n",
      "  Normalized: 2025-10-30 16:49:06.000\n",
      "\n",
      "  Original: Time: 10.30 16:49:06\n",
      "  Normalized: 2025-10-30 16:49:06.000\n",
      "\n",
      "  Original: Time: 10.30 16:49:06\n",
      "  Normalized: 2025-10-30 16:49:06.000\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Zookeeper_2k_labeled.csv\n",
      "Size: 0.41 MB\n",
      "Detected log type: zookeeper\n",
      "Loaded dataframe with shape: (2000, 13)\n",
      "Columns: ['LineId', 'Date', 'Time', 'Level', 'Node', 'Component', 'Id', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Normalizing timestamps for zookeeper logs...\n",
      "Available columns: ['LineId', 'Date', 'Time', 'Level', 'Node', 'Component', 'Id', 'Content', 'EventId', 'EventTemplate', 'AnomalyLabel', 'AnomalyLabelName', 'Source']\n",
      "Successfully normalized 2000/2000 timestamps (100.0%)\n",
      "\n",
      "Example conversions:\n",
      "  Original: Date: 2015-07-29, Time: 17:41:44,747\n",
      "  Normalized: 2015-07-29 17:41:44.747\n",
      "\n",
      "  Original: Date: 2015-07-29, Time: 19:04:12,394\n",
      "  Normalized: 2015-07-29 19:04:12.394\n",
      "\n",
      "  Original: Date: 2015-07-29, Time: 19:04:29,071\n",
      "  Normalized: 2015-07-29 19:04:29.071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_files = {}\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {file_path.name}\")\n",
    "    print(f\"Size: {file_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    log_type = detect_log_type(file_path.name)\n",
    "    print(f\"Detected log type: {log_type}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded dataframe with shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        if log_type != 'unknown':\n",
    "            df_normalized = normalize_timestamps_in_df(df, log_type)\n",
    "            processed_files[file_path.name] = {\n",
    "                'original_df': df,\n",
    "                'normalized_df': df_normalized,\n",
    "                'log_type': log_type,\n",
    "                'file_path': file_path\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Skipping {file_path.name} - unknown log type\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e67b62a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving normalized files to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\dataset\\labeled_data\\normalized\n",
      "✓ Saved: Apache_2k_normalized.csv (0.33 MB)\n",
      "✓ Saved: BGL_2k_normalized.csv (0.49 MB)\n",
      "✓ Saved: HPC_2k_normalized.csv (0.29 MB)\n",
      "✓ Saved: OpenSSH_2k_normalized.csv (0.44 MB)\n",
      "✓ Saved: Proxifier_2k_normalized.csv (0.43 MB)\n",
      "✓ Saved: Zookeeper_2k_normalized.csv (0.45 MB)\n"
     ]
    }
   ],
   "source": [
    "normalized_output_path = LABELED_DATA_PATH / \"normalized\"\n",
    "normalized_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving normalized files to: {normalized_output_path}\")\n",
    "\n",
    "for filename, data in processed_files.items():\n",
    "    try:\n",
    "        normalized_df = data['normalized_df']\n",
    "        output_filename = filename.replace('_labeled.csv', '_normalized.csv')\n",
    "        output_path = normalized_output_path / output_filename\n",
    "        \n",
    "        normalized_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"Saved: {output_filename} ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2211ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apache_2k_labeled.csv\n",
      "  Log type: apache\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2005-12-04 04:47:44.000', '2005-12-04 04:47:44.000']\n",
      "\n",
      "BGL_2k_labeled.csv\n",
      "  Log type: bgl\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2005-06-03 00:00:00.000', '2005-06-03 00:00:00.000']\n",
      "\n",
      "HPC_2k_labeled.csv\n",
      "  Log type: hpc\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2004-02-26 19:42:22.000', '2004-05-16 09:42:58.000']\n",
      "\n",
      "OpenSSH_2k_labeled.csv\n",
      "  Log type: openssh\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2025-12-10 06:55:46.000', '2025-12-10 06:55:46.000']\n",
      "\n",
      "Proxifier_2k_labeled.csv\n",
      "  Log type: proxifier\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2025-10-30 16:49:06.000', '2025-10-30 16:49:06.000']\n",
      "\n",
      "Zookeeper_2k_labeled.csv\n",
      "  Log type: zookeeper\n",
      "  Total records: 2,000\n",
      "  Normalized: 2,000\n",
      "  Success rate: 100.0%\n",
      "  Sample normalized: ['2015-07-29 17:41:44.747', '2015-07-29 19:04:12.394']\n",
      "\n",
      "OVERALL SUMMARY:\n",
      "Total records processed: 12,000\n",
      "Total normalized: 12,000\n",
      "Overall success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "total_records = 0\n",
    "total_normalized = 0\n",
    "\n",
    "for filename, data in processed_files.items():\n",
    "    df = data['normalized_df']\n",
    "    log_type = data['log_type']\n",
    "    \n",
    "    normalized_col = None\n",
    "    for col in df.columns:\n",
    "        if 'normalized' in col.lower():\n",
    "            normalized_col = col\n",
    "            break\n",
    "    \n",
    "    if normalized_col:\n",
    "        records = len(df)\n",
    "        normalized_count = df[normalized_col].notna().sum()\n",
    "        success_rate = (normalized_count / records * 100) if records > 0 else 0\n",
    "        \n",
    "        total_records += records\n",
    "        total_normalized += normalized_count\n",
    "        \n",
    "        print(f\"\\n{filename}\")\n",
    "        print(f\"  Log type: {log_type}\")\n",
    "        print(f\"  Total records: {records:,}\")\n",
    "        print(f\"  Normalized: {normalized_count:,}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        if normalized_count > 0:\n",
    "            sample_timestamps = df[normalized_col].dropna().head(2).tolist()\n",
    "            print(f\"  Sample normalized: {sample_timestamps}\")\n",
    "\n",
    "overall_success_rate = (total_normalized / total_records * 100) if total_records > 0 else 0\n",
    "print(f\"\\nOVERALL SUMMARY:\")\n",
    "print(f\"Total records processed: {total_records:,}\")\n",
    "print(f\"Total normalized: {total_normalized:,}\")\n",
    "print(f\"Overall success rate: {overall_success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330be88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
