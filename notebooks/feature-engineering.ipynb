{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203d2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c287874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, CountVectorizer, IDF, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler,\n",
    "    MinMaxScaler, RegexTokenizer\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    NaiveBayes, LinearSVC, LogisticRegression\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554f3e2",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705df321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\n",
      "Processed Data: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\dataset\\processed\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "PROCESSED_DATA_PATH = DATASET_PATH / \"processed\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\"\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Processed Data: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2219849",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4064307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Spark session initialized\n",
      "Driver memory: 18g\n",
      "Shuffle partitions: 8\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.maxRecords\", \"1000000\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", \"20\") \\\n",
    "    .appName(\"LogAnomalyFeatureEngineering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Enhanced Spark session initialized\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed45f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading EDA results and processed datasets...\n",
      "Found 16 processed datasets\n",
      "Loaded Android_2k.log_structured: 2,000 rows × 10 columns\n",
      "Loaded Apache_2k.log_structured: 2,000 rows × 6 columns\n",
      "Loaded BGL_2k.log_structured: 2,000 rows × 13 columns\n",
      "Loaded Hadoop_2k.log_structured: 2,000 rows × 9 columns\n",
      "Loaded HDFS_2k.log_structured: 2,000 rows × 9 columns\n",
      "Loaded HealthApp_2k.log_structured: 2,000 rows × 7 columns\n",
      "Loaded HPC_2k.log_structured: 2,000 rows × 10 columns\n",
      "Loaded Linux_2k.log_structured: 2,000 rows × 10 columns\n",
      "Loaded Mac_2k.log_structured: 2,000 rows × 11 columns\n",
      "Loaded OpenSSH_2k.log_structured: 2,000 rows × 9 columns\n",
      "Loaded OpenStack_2k.log_structured: 2,000 rows × 11 columns\n",
      "Loaded Proxifier_2k.log_structured: 2,000 rows × 6 columns\n",
      "Loaded Spark_2k.log_structured: 2,000 rows × 8 columns\n",
      "Loaded Thunderbird_2k.log_structured: 2,000 rows × 14 columns\n",
      "Loaded Windows_2k.log_structured: 2,000 rows × 8 columns\n",
      "Loaded Zookeeper_2k.log_structured: 2,000 rows × 10 columns\n"
     ]
    }
   ],
   "source": [
    "def load_eda_metadata():\n",
    "    metadata_file = RESULTS_PATH / \"eda_metadata.json\"\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(\"EDA metadata not found.\")\n",
    "        return None\n",
    "\n",
    "def load_processed_datasets():\n",
    "    processed_files = list(PROCESSED_DATA_PATH.glob(\"*_processed.parquet\"))\n",
    "    print(f\"Found {len(processed_files)} processed datasets\")\n",
    "    \n",
    "    datasets = {}\n",
    "    for file_path in processed_files:\n",
    "        dataset_name = file_path.stem.replace(\"_processed\", \"\")\n",
    "        try:\n",
    "            df = spark.read.parquet(str(file_path))\n",
    "            datasets[dataset_name] = df\n",
    "            print(f\"Loaded {dataset_name}: {df.count():,} rows × {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "print(\"\\nLoading EDA results and processed datasets...\")\n",
    "eda_metadata = load_eda_metadata()\n",
    "datasets = load_processed_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0eb04cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     result = harmonized.select(*available_columns)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCreating unified dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdatasets\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sources...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m unified_dfs = []\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, df \u001b[38;5;129;01min\u001b[39;00m datasets.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "def create_unified_schema():\n",
    "    universal_columns = ['LineId', 'Time', 'Content', 'EventId', 'EventTemplate']\n",
    "    \n",
    "    unified_schema = StructType([\n",
    "        StructField(\"source_system\", StringType(), True),      # Log source identifier\n",
    "        StructField(\"line_id\", IntegerType(), True),           # Original LineId\n",
    "        StructField(\"timestamp\", TimestampType(), True),        # Standardized timestamp\n",
    "        StructField(\"content\", StringType(), False),           # Log content (primary feature)\n",
    "        StructField(\"event_id\", StringType(), True),           # Event identifier\n",
    "        StructField(\"event_template\", StringType(), True),     # Event template\n",
    "        StructField(\"level\", StringType(), True),              # Log level (when available)\n",
    "        StructField(\"component\", StringType(), True),          # Component (when available)\n",
    "        StructField(\"content_length\", IntegerType(), True),    # Content character count\n",
    "        StructField(\"word_count\", IntegerType(), True)         # Word count\n",
    "    ])\n",
    "    \n",
    "    return unified_schema\n",
    "\n",
    "def harmonize_dataset(df, source_name):\n",
    "\n",
    "    print(f\"Harmonizing {source_name}...\")\n",
    "    \n",
    "    harmonized = df.withColumn(\"source_system\", F.lit(source_name))\n",
    "    \n",
    "    column_mapping = {\n",
    "        'LineId': 'line_id',\n",
    "        'Time': 'timestamp',\n",
    "        'Content': 'content',\n",
    "        'EventId': 'event_id',\n",
    "        'EventTemplate': 'event_template',\n",
    "        'Level': 'level',\n",
    "        'Component': 'component'\n",
    "    }\n",
    "    \n",
    "    for old_col, new_col in column_mapping.items():\n",
    "        if old_col in harmonized.columns:\n",
    "            harmonized = harmonized.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    for col_name, col_type in [('level', StringType()), ('component', StringType())]:\n",
    "        if col_name not in harmonized.columns:\n",
    "            harmonized = harmonized.withColumn(col_name, F.lit(None).cast(col_type))\n",
    "    \n",
    "    harmonized = harmonized \\\n",
    "        .withColumn(\"content_length\", F.length(\"content\")) \\\n",
    "        .withColumn(\"word_count\", F.size(F.split(F.col(\"content\"), \" \")))\n",
    "    \n",
    "    unified_columns = ['source_system', 'line_id', 'timestamp', 'content', \n",
    "                      'event_id', 'event_template', 'level', 'component', \n",
    "                      'content_length', 'word_count']\n",
    "    \n",
    "    available_columns = [col for col in unified_columns if col in harmonized.columns]\n",
    "    result = harmonized.select(*available_columns)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(f\"\\nCreating unified dataset from {len(datasets)} sources...\")\n",
    "unified_dfs = []\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    harmonized_df = harmonize_dataset(df, dataset_name)\n",
    "    unified_dfs.append(harmonized_df)\n",
    "\n",
    "unified_df = unified_dfs[0]\n",
    "for df in unified_dfs[1:]:\n",
    "    unified_df = unified_df.union(df)\n",
    "\n",
    "print(f\"Unified dataset created: {unified_df.count():,} total rows\")\n",
    "print(f\"Source distribution:\")\n",
    "unified_df.groupBy(\"source_system\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4b8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
