{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21da223",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import drain3\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.masking import MaskingInstruction\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac1a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "NORMALIZED_DATA_PATH = LABELED_DATA_PATH / \"normalized\"\n",
    "FEATURES_PATH = PROJECT_ROOT / \"features\"\n",
    "FEATURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402d6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .appName(\"MultiClassFeatureEngineering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa3e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 log sources\n",
      "Label mapping: {0: 'normal', 1: 'security_anomaly', 2: 'system_failure', 3: 'performance_issue', 4: 'network_anomaly', 5: 'config_error', 6: 'hardware_issue'}\n"
     ]
    }
   ],
   "source": [
    "PROJECT_CONFIG = {\n",
    "    'bert_model_name': 'bert-base-uncased',\n",
    "    'max_sequence_length': 512,\n",
    "    'num_classes': 7,\n",
    "    'label_map': {\n",
    "        0: 'normal',\n",
    "        1: 'security_anomaly',\n",
    "        2: 'system_failure',\n",
    "        3: 'performance_issue',\n",
    "        4: 'network_anomaly',\n",
    "        5: 'config_error',\n",
    "        6: 'hardware_issue'\n",
    "    },\n",
    "    'log_sources': []\n",
    "}\n",
    "\n",
    "dataset_registry = {}\n",
    "\n",
    "enhanced_files = list(NORMALIZED_DATA_PATH.glob(\"*_enhanced.csv\"))\n",
    "for file_path in enhanced_files:\n",
    "    source_name = file_path.stem.replace('_enhanced', '')\n",
    "    PROJECT_CONFIG['log_sources'].append(source_name)\n",
    "    dataset_registry[source_name] = {\n",
    "        'file_path': str(file_path),\n",
    "        'log_type': source_name\n",
    "    }\n",
    "\n",
    "print(f\"Loaded {len(dataset_registry)} log sources\")\n",
    "print(f\"Label mapping: {PROJECT_CONFIG['label_map']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5824bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model = AutoModel.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "drain_configs = {\n",
    "    'hdfs': {'sim_th': 0.5, 'depth': 4},\n",
    "    'bgl': {'sim_th': 0.3, 'depth': 5},\n",
    "    'hadoop': {'sim_th': 0.4, 'depth': 4},\n",
    "    'apache': {'sim_th': 0.4, 'depth': 4},\n",
    "    'default': {'sim_th': 0.4, 'depth': 4}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e69c19",
   "metadata": {},
   "source": [
    "Temporal and Statistical Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43bae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shannon_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy - higher for more random/error messages\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    text = str(text)\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    prob = [text.count(c) / len(text) for c in set(text)]\n",
    "    return -sum(p * np.log2(p) for p in prob if p > 0)\n",
    "\n",
    "def count_repeated_words(text):\n",
    "    \"\"\"Count repeated words in text (common in system failures)\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    words = str(text).lower().split()\n",
    "    if len(words) <= 1:\n",
    "        return 0\n",
    "    word_counts = Counter(words)\n",
    "    return sum(1 for count in word_counts.values() if count > 1)\n",
    "\n",
    "def count_repeated_chars(text):\n",
    "    \"\"\"Count repeated character sequences (common in error messages)\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    text = str(text)\n",
    "    repeated_count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == text[i + 1]:\n",
    "            repeated_count += 1\n",
    "    return repeated_count\n",
    "\n",
    "def get_error_patterns_by_source(source_type):\n",
    "    \"\"\"Source-specific error patterns for minority class detection\"\"\"\n",
    "    patterns = {\n",
    "        'apache': {\n",
    "            'error_level': r'\\b(error|critical|alert|emergency)\\b',\n",
    "            'http_error': r'\\b(40[0-9]|50[0-9])\\b',\n",
    "            'security_threat': r'\\b(attack|intrusion|unauthorized|forbidden|hack)\\b',\n",
    "            'resource_issue': r'\\b(timeout|memory|disk|space|limit)\\b'\n",
    "        },\n",
    "        'linux': {\n",
    "            'kernel_panic': r'\\b(kernel|panic|oops|segfault|core dump)\\b',\n",
    "            'auth_failure': r'\\b(authentication failed|login failed|access denied)\\b',\n",
    "            'resource_exhaustion': r'\\b(out of memory|disk full|no space|quota exceeded)\\b',\n",
    "            'hardware_error': r'\\b(hardware|disk error|i/o error|bad sector)\\b'\n",
    "        },\n",
    "        'hadoop': {\n",
    "            'job_failure': r'\\b(job failed|task failed|exception|error)\\b',\n",
    "            'performance_issue': r'\\b(slow|timeout|latency|performance)\\b',\n",
    "            'network_problem': r'\\b(connection|unreachable|network|socket)\\b',\n",
    "            'config_error': r'\\b(configuration|config|property|setting)\\b'\n",
    "        },\n",
    "        'openssh': {\n",
    "            'security_breach': r'\\b(failed password|invalid user|break-in|attack)\\b',\n",
    "            'connection_issue': r'\\b(connection closed|timeout|refused)\\b'\n",
    "        },\n",
    "        'bgl': {\n",
    "            'system_failure': r'\\b(failure|failed|error|exception)\\b',\n",
    "            'hardware_issue': r'\\b(hardware|disk|memory|cpu|node)\\b',\n",
    "            'config_error': r'\\b(config|configuration|parameter)\\b'\n",
    "        },\n",
    "        'hdfs': {\n",
    "            'system_failure': r'\\b(block|replica|datanode|namenode|error)\\b',\n",
    "            'network_problem': r'\\b(connection|network|timeout)\\b'\n",
    "        },\n",
    "        'hpc': {\n",
    "            'system_failure': r'\\b(node|job|task|error|failure)\\b',\n",
    "            'performance_issue': r'\\b(slow|performance|latency|timeout)\\b',\n",
    "            'network_problem': r'\\b(network|connection|communication)\\b',\n",
    "            'hardware_issue': r'\\b(hardware|memory|disk|cpu)\\b'\n",
    "        },\n",
    "        'proxifier': {\n",
    "            'network_anomaly': r'\\b(connection|proxy|tunnel|network)\\b'\n",
    "        },\n",
    "        'zookeeper': {\n",
    "            'system_failure': r'\\b(error|exception|failure)\\b',\n",
    "            'performance_issue': r'\\b(timeout|slow|latency)\\b',\n",
    "            'network_problem': r'\\b(connection|network|socket)\\b',\n",
    "            'config_error': r'\\b(config|configuration|property)\\b'\n",
    "        }\n",
    "    }\n",
    "    return patterns.get(source_type.lower(), {})\n",
    "\n",
    "def detect_holiday_patterns(timestamps):\n",
    "    \"\"\"Detect holiday/special time periods (simplified implementation)\"\"\"\n",
    "    # This is a simplified version - in practice, you'd use a holiday calendar\n",
    "    if timestamps is None or len(timestamps) == 0:\n",
    "        return [0] * len(timestamps) if hasattr(timestamps, '__len__') else 0\n",
    "    \n",
    "    # For now, just mark weekends as \"holiday periods\"\n",
    "    try:\n",
    "        if hasattr(timestamps, 'dt'):\n",
    "            return (timestamps.dt.dayofweek >= 5).astype(int)\n",
    "        else:\n",
    "            return [0] * len(timestamps)\n",
    "    except:\n",
    "        return [0] * len(timestamps) if hasattr(timestamps, '__len__') else 0\n",
    "\n",
    "def add_imbalance_aware_features(df_pandas, source_type, content_col):\n",
    "    \"\"\"Add features specifically designed for imbalanced anomaly detection\"\"\"\n",
    "    \n",
    "    print(f\"  Adding imbalance-aware features for {source_type}\")\n",
    "    \n",
    "    # Message complexity features (anomalies often have different complexity)\n",
    "    if content_col in df_pandas.columns:\n",
    "        df_pandas['msg_length'] = df_pandas[content_col].str.len().fillna(0)\n",
    "        df_pandas['msg_word_count'] = df_pandas[content_col].str.split().str.len().fillna(0)\n",
    "        df_pandas['msg_unique_chars'] = df_pandas[content_col].apply(\n",
    "            lambda x: len(set(str(x))) if pd.notna(x) else 0\n",
    "        )\n",
    "        df_pandas['msg_entropy'] = df_pandas[content_col].apply(calculate_shannon_entropy)\n",
    "        \n",
    "        # Anomaly-specific patterns (source-dependent)\n",
    "        error_patterns = get_error_patterns_by_source(source_type)\n",
    "        for pattern_name, pattern in error_patterns.items():\n",
    "            df_pandas[f'has_{pattern_name}'] = df_pandas[content_col].str.contains(\n",
    "                pattern, case=False, na=False\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Character distribution features\n",
    "        df_pandas['special_char_ratio'] = (\n",
    "            df_pandas[content_col].str.count(r'[^a-zA-Z0-9\\s]') / \n",
    "            (df_pandas['msg_length'] + 1)\n",
    "        ).fillna(0)\n",
    "        df_pandas['number_ratio'] = (\n",
    "            df_pandas[content_col].str.count(r'\\d') / \n",
    "            (df_pandas['msg_length'] + 1)\n",
    "        ).fillna(0)\n",
    "        df_pandas['uppercase_ratio'] = (\n",
    "            df_pandas[content_col].str.count(r'[A-Z]') / \n",
    "            (df_pandas['msg_length'] + 1)\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Repetition patterns (common in system failures)\n",
    "        df_pandas['repeated_words'] = df_pandas[content_col].apply(count_repeated_words)\n",
    "        df_pandas['repeated_chars'] = df_pandas[content_col].apply(count_repeated_chars)\n",
    "    \n",
    "    return df_pandas\n",
    "\n",
    "def add_temporal_anomaly_features(df_pandas):\n",
    "    \"\"\"Add temporal features that capture anomaly patterns\"\"\"\n",
    "    \n",
    "    print(\"  Adding enhanced temporal anomaly features\")\n",
    "    \n",
    "    # Enhanced anomaly-specific temporal features\n",
    "    if 'hour' in df_pandas.columns:\n",
    "        df_pandas['is_off_hours'] = ((df_pandas['hour'] < 6) | (df_pandas['hour'] > 22)).astype(int)\n",
    "    \n",
    "    if 'is_weekend' in df_pandas.columns and 'is_night' in df_pandas.columns:\n",
    "        df_pandas['is_weekend_night'] = (df_pandas['is_weekend'] & df_pandas['is_night']).astype(int)\n",
    "    \n",
    "    # Holiday patterns (simplified)\n",
    "    if 'timestamp_dt' in df_pandas.columns:\n",
    "        df_pandas['is_holiday_period'] = detect_holiday_patterns(df_pandas['timestamp_dt'])\n",
    "    \n",
    "    # Time gap analysis (anomalies often have unusual timing)\n",
    "    if 'timestamp_dt' in df_pandas.columns:\n",
    "        df_pandas = df_pandas.sort_values('timestamp_dt').reset_index(drop=True)\n",
    "        df_pandas['time_gap_seconds'] = df_pandas['timestamp_dt'].diff().dt.total_seconds().fillna(0)\n",
    "        df_pandas['is_burst'] = (df_pandas['time_gap_seconds'] < 1).astype(int)  # Rapid succession\n",
    "        df_pandas['is_isolated'] = (df_pandas['time_gap_seconds'] > 300).astype(int)  # Isolated events\n",
    "        \n",
    "        # Rolling window features for different time scales\n",
    "        windows = ['1min', '5min', '15min', '1H', '6H']\n",
    "        df_pandas_indexed = df_pandas.set_index('timestamp_dt')\n",
    "        \n",
    "        for window in windows:\n",
    "            try:\n",
    "                # Log frequency in window\n",
    "                df_pandas[f'log_count_{window}'] = df_pandas_indexed.rolling(window).size().values\n",
    "                \n",
    "                # Message diversity in window\n",
    "                if any(col.startswith('has_') for col in df_pandas.columns):\n",
    "                    error_cols = [col for col in df_pandas.columns if col.startswith('has_')]\n",
    "                    df_pandas[f'error_density_{window}'] = (\n",
    "                        df_pandas_indexed[error_cols].rolling(window).sum().sum(axis=1).values\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: Could not create {window} features: {e}\")\n",
    "                df_pandas[f'log_count_{window}'] = 0\n",
    "                df_pandas[f'error_density_{window}'] = 0\n",
    "    \n",
    "    return df_pandas\n",
    "\n",
    "def add_statistical_anomaly_features(df_pandas):\n",
    "    \"\"\"Add statistical features that capture anomaly distributions\"\"\"\n",
    "    \n",
    "    print(\"  Adding statistical anomaly features\")\n",
    "    \n",
    "    # Message length distribution features\n",
    "    if 'msg_length' in df_pandas.columns:\n",
    "        windows = [10, 50, 100]\n",
    "        for w in windows:\n",
    "            df_pandas[f'msg_len_mean_{w}'] = df_pandas['msg_length'].rolling(w, min_periods=1).mean()\n",
    "            df_pandas[f'msg_len_std_{w}'] = df_pandas['msg_length'].rolling(w, min_periods=1).std()\n",
    "            df_pandas[f'msg_len_zscore_{w}'] = (\n",
    "                (df_pandas['msg_length'] - df_pandas[f'msg_len_mean_{w}']) / \n",
    "                (df_pandas[f'msg_len_std_{w}'] + 1e-6)\n",
    "            )\n",
    "            df_pandas[f'is_length_outlier_{w}'] = (\n",
    "                np.abs(df_pandas[f'msg_len_zscore_{w}']) > 2\n",
    "            ).astype(int)\n",
    "    \n",
    "    # Temporal distribution features\n",
    "    for col in ['time_diff_seconds', 'logs_last_minute']:\n",
    "        if col in df_pandas.columns:\n",
    "            windows = [10, 50, 100]\n",
    "            for w in windows:\n",
    "                df_pandas[f'{col}_mean_{w}'] = df_pandas[col].rolling(w, min_periods=1).mean()\n",
    "                df_pandas[f'{col}_std_{w}'] = df_pandas[col].rolling(w, min_periods=1).std()\n",
    "                df_pandas[f'{col}_zscore_{w}'] = (\n",
    "                    (df_pandas[col] - df_pandas[f'{col}_mean_{w}']) / \n",
    "                    (df_pandas[f'{col}_std_{w}'] + 1e-6)\n",
    "                )\n",
    "                df_pandas[f'is_{col}_outlier_{w}'] = (\n",
    "                    np.abs(df_pandas[f'{col}_zscore_{w}']) > 2\n",
    "                ).astype(int)\n",
    "    \n",
    "    return df_pandas\n",
    "\n",
    "def create_imbalance_aware_template_features(template_data, labels):\n",
    "    \"\"\"Create template features that emphasize minority class patterns\"\"\"\n",
    "    \n",
    "    enhanced_features = []\n",
    "    \n",
    "    for log_source, data_dict in template_data.items():\n",
    "        templates = data_dict['templates']\n",
    "        template_ids = data_dict['template_ids']\n",
    "        \n",
    "        # Enhanced features for each log entry\n",
    "        template_counts = Counter(template_ids)\n",
    "        total = len(template_ids)\n",
    "        \n",
    "        for i, template_id in enumerate(template_ids):\n",
    "            if template_id == -1:\n",
    "                enhanced_features.append([0] * 15)  # Increased feature count\n",
    "                continue\n",
    "            \n",
    "            # Basic features (existing)\n",
    "            frequency = template_counts[template_id] / total\n",
    "            rarity = 1.0 / (frequency + 1e-6)\n",
    "            template_text = templates[template_id]['template']\n",
    "            length = len(template_text.split())\n",
    "            n_wildcards = sum([template_text.count(w) for w in ['<NUM>', '<IP>', '<PATH>', '<UUID>']])\n",
    "            \n",
    "            # NEW: Class-specific template features\n",
    "            class_probs = np.array(templates[template_id]['class_dist']) / (templates[template_id]['count'] + 1e-6)\n",
    "            \n",
    "            # Minority class indicators\n",
    "            minority_score = sum(class_probs[c] for c in [1, 3, 6])  # Classes with low coverage\n",
    "            anomaly_score = sum(class_probs[c] for c in [1, 2, 3, 4, 5, 6])  # All non-normal\n",
    "            \n",
    "            # Template complexity features\n",
    "            complexity_score = length * n_wildcards / (frequency + 1e-6)\n",
    "            uniqueness_score = rarity * (1 - max(class_probs))\n",
    "            \n",
    "            # Combine features\n",
    "            features = [\n",
    "                rarity, length, n_wildcards, frequency,  # Original 4\n",
    "                minority_score, anomaly_score,           # Class-specific 2\n",
    "                complexity_score, uniqueness_score,      # Complexity 2\n",
    "                *class_probs.tolist()                    # Class probabilities 7\n",
    "            ]  # Total: 15 features\n",
    "            \n",
    "            enhanced_features.append(features)\n",
    "    \n",
    "    return np.array(enhanced_features)\n",
    "\n",
    "def analyze_class_imbalance(df_pandas):\n",
    "    \"\"\"Comprehensive class imbalance analysis\"\"\"\n",
    "    \n",
    "    if 'AnomalyLabel' not in df_pandas.columns:\n",
    "        return None\n",
    "        \n",
    "    analysis = {}\n",
    "    \n",
    "    # Basic distribution\n",
    "    label_counts = df_pandas['AnomalyLabel'].value_counts().sort_index()\n",
    "    total_samples = len(df_pandas)\n",
    "    \n",
    "    analysis['class_distribution'] = {}\n",
    "    analysis['class_percentages'] = {}\n",
    "    \n",
    "    for label in range(7):\n",
    "        count = label_counts.get(label, 0)\n",
    "        analysis['class_distribution'][label] = count\n",
    "        analysis['class_percentages'][label] = (count / total_samples) * 100\n",
    "    \n",
    "    # Imbalance metrics\n",
    "    present_classes = [label for label in range(7) if label_counts.get(label, 0) > 0]\n",
    "    if len(present_classes) > 1:\n",
    "        counts = [label_counts[label] for label in present_classes]\n",
    "        analysis['imbalance_ratio'] = max(counts) / min(counts)\n",
    "        analysis['minority_classes'] = [\n",
    "            label for label in present_classes \n",
    "            if label_counts[label] < total_samples * 0.05  # Less than 5%\n",
    "        ]\n",
    "        analysis['extreme_minority'] = [\n",
    "            label for label in present_classes \n",
    "            if label_counts[label] < total_samples * 0.01  # Less than 1%\n",
    "        ]\n",
    "    else:\n",
    "        analysis['imbalance_ratio'] = 1.0\n",
    "        analysis['minority_classes'] = []\n",
    "        analysis['extreme_minority'] = []\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be303a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: Apache_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 21 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,405 (70.25%)\n",
      "  1 (security_anomaly): 32 (1.60%)\n",
      "  2 (system_failure): 563 (28.15%)\n",
      "Converted to Pandas: (2000, 31)\n",
      "  Adding imbalance-aware features for Apache_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 3/7\n",
      "Imbalance ratio: 43.91:1\n",
      "Minority classes: [1]\n",
      "Enhanced features shape: (2000, 92)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: BGL_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 28 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 501 (25.05%)\n",
      "  1 (security_anomaly): 150 (7.50%)\n",
      "  2 (system_failure): 913 (45.65%)\n",
      "  5 (config_error): 73 (3.65%)\n",
      "  6 (hardware_issue): 363 (18.15%)\n",
      "Converted to Pandas: (2000, 37)\n",
      "  Adding imbalance-aware features for BGL_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: window must be an integer 0 or greater\n",
      "    Warning: Could not create 5min features: window must be an integer 0 or greater\n",
      "    Warning: Could not create 15min features: window must be an integer 0 or greater\n",
      "    Warning: Could not create 1H features: window must be an integer 0 or greater\n",
      "    Warning: Could not create 6H features: window must be an integer 0 or greater\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 5/7\n",
      "Imbalance ratio: 12.51:1\n",
      "Minority classes: [5]\n",
      "Enhanced features shape: (2000, 98)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Hadoop_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,217 (60.85%)\n",
      "  2 (system_failure): 156 (7.80%)\n",
      "  3 (performance_issue): 1 (0.05%)\n",
      "  4 (network_anomaly): 625 (31.25%)\n",
      "  5 (config_error): 1 (0.05%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "  Adding imbalance-aware features for Hadoop_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 5/7\n",
      "Imbalance ratio: 1217.00:1\n",
      "Minority classes: [3, 5]\n",
      "Extreme minority: [3, 5]\n",
      "Enhanced features shape: (2000, 95)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: HDFS_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,920 (96.00%)\n",
      "  2 (system_failure): 80 (4.00%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "  Adding imbalance-aware features for HDFS_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: index values must not have NaT\n",
      "    Warning: Could not create 5min features: index values must not have NaT\n",
      "    Warning: Could not create 15min features: index values must not have NaT\n",
      "    Warning: Could not create 1H features: index values must not have NaT\n",
      "    Warning: Could not create 6H features: index values must not have NaT\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/7\n",
      "Imbalance ratio: 24.00:1\n",
      "Minority classes: [2]\n",
      "Enhanced features shape: (2000, 95)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: HPC_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,804 (90.20%)\n",
      "  2 (system_failure): 60 (3.00%)\n",
      "  3 (performance_issue): 29 (1.45%)\n",
      "  4 (network_anomaly): 85 (4.25%)\n",
      "  5 (config_error): 4 (0.20%)\n",
      "  6 (hardware_issue): 18 (0.90%)\n",
      "Converted to Pandas: (2000, 35)\n",
      "  Adding imbalance-aware features for HPC_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 6/7\n",
      "Imbalance ratio: 451.00:1\n",
      "Minority classes: [2, 3, 4, 5, 6]\n",
      "Extreme minority: [5, 6]\n",
      "Enhanced features shape: (2000, 96)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Linux_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,359 (67.95%)\n",
      "  1 (security_anomaly): 527 (26.35%)\n",
      "  2 (system_failure): 46 (2.30%)\n",
      "  3 (performance_issue): 15 (0.75%)\n",
      "  4 (network_anomaly): 34 (1.70%)\n",
      "  5 (config_error): 5 (0.25%)\n",
      "  6 (hardware_issue): 14 (0.70%)\n",
      "Converted to Pandas: (2000, 36)\n",
      "  Adding imbalance-aware features for Linux_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 7/7\n",
      "Imbalance ratio: 271.80:1\n",
      "Minority classes: [2, 3, 4, 5, 6]\n",
      "Extreme minority: [3, 5, 6]\n",
      "Enhanced features shape: (2000, 97)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: OpenSSH_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 424 (21.20%)\n",
      "  1 (security_anomaly): 1,569 (78.45%)\n",
      "  5 (config_error): 7 (0.35%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "  Adding imbalance-aware features for OpenSSH_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 3/7\n",
      "Imbalance ratio: 224.14:1\n",
      "Minority classes: [5]\n",
      "Extreme minority: [5]\n",
      "Enhanced features shape: (2000, 95)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Proxifier_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 21 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,903 (95.15%)\n",
      "  4 (network_anomaly): 97 (4.85%)\n",
      "Converted to Pandas: (2000, 31)\n",
      "  Adding imbalance-aware features for Proxifier_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/7\n",
      "Imbalance ratio: 19.62:1\n",
      "Minority classes: [4]\n",
      "Enhanced features shape: (2000, 92)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing: Zookeeper_2k\n",
      "============================================================\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting basic temporal features\n",
      "Calculating basic statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,075 (53.75%)\n",
      "  2 (system_failure): 4 (0.20%)\n",
      "  3 (performance_issue): 37 (1.85%)\n",
      "  4 (network_anomaly): 873 (43.65%)\n",
      "  5 (config_error): 6 (0.30%)\n",
      "  6 (hardware_issue): 5 (0.25%)\n",
      "Converted to Pandas: (2000, 35)\n",
      "  Adding imbalance-aware features for Zookeeper_2k\n",
      "  Adding enhanced temporal anomaly features\n",
      "    Warning: Could not create 1min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 5min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 15min features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 1H features: 'Rolling' object has no attribute 'size'\n",
      "    Warning: Could not create 6H features: 'Rolling' object has no attribute 'size'\n",
      "  Adding statistical anomaly features\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 6/7\n",
      "Imbalance ratio: 268.75:1\n",
      "Minority classes: [2, 3, 5, 6]\n",
      "Extreme minority: [2, 5, 6]\n",
      "Enhanced features shape: (2000, 96)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_features_data = {}\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in dataset_registry:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {log_source}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    file_path = dataset_registry[log_source]['file_path']\n",
    "    \n",
    "    df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(f\"Loaded {df_spark.count()} rows with {len(df_spark.columns)} columns\")\n",
    "    \n",
    "    content_col = None\n",
    "    for col in ['Content', 'content', 'Message', 'message', 'Text', 'text']:\n",
    "        if col in df_spark.columns:\n",
    "            content_col = col\n",
    "            break\n",
    "    \n",
    "    if content_col is None:\n",
    "        print(f\"No content column found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    if 'timestamp_dt' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.col('timestamp_dt').cast(TimestampType()))\n",
    "    elif 'timestamp_normalized' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.to_timestamp('timestamp_normalized'))\n",
    "    else:\n",
    "        print(\"No timestamp column found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Extracting basic temporal features\")\n",
    "    \n",
    "    df_spark = df_spark.withColumn('hour', F.hour('timestamp')) \\\n",
    "                       .withColumn('day_of_week', F.dayofweek('timestamp')) \\\n",
    "                       .withColumn('day_of_month', F.dayofmonth('timestamp')) \\\n",
    "                       .withColumn('month', F.month('timestamp')) \\\n",
    "                       .withColumn('is_weekend', F.when(F.dayofweek('timestamp').isin([1, 7]), 1).otherwise(0)) \\\n",
    "                       .withColumn('is_business_hours', F.when(F.hour('timestamp').between(9, 17), 1).otherwise(0)) \\\n",
    "                       .withColumn('is_night', F.when(F.hour('timestamp').between(0, 6), 1).otherwise(0))\n",
    "    \n",
    "    window_spec = Window.orderBy('timestamp')\n",
    "    window_spec_1min = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-60, 0)\n",
    "    \n",
    "    df_spark = df_spark.withColumn('log_index', F.monotonically_increasing_id())\n",
    "    \n",
    "    df_spark = df_spark.withColumn('prev_timestamp', F.lag('timestamp', 1).over(window_spec))\n",
    "    df_spark = df_spark.withColumn('time_diff_seconds', \n",
    "                                    F.when(F.col('prev_timestamp').isNotNull(), \n",
    "                                           F.unix_timestamp('timestamp') - F.unix_timestamp('prev_timestamp'))\n",
    "                                    .otherwise(0))\n",
    "    \n",
    "    df_spark = df_spark.withColumn('logs_last_minute', \n",
    "                                    F.count('*').over(window_spec_1min))\n",
    "    \n",
    "    print(\"Calculating basic statistical features\")\n",
    "    \n",
    "    df_spark = df_spark.withColumn('content_length', F.length(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('word_count', F.size(F.split(F.col(content_col), ' ')))\n",
    "    \n",
    "    window_10 = Window.orderBy('timestamp').rowsBetween(-9, 0)\n",
    "    \n",
    "    df_spark = df_spark.withColumn('content_length_mean_10', F.avg('content_length').over(window_10))\n",
    "    df_spark = df_spark.withColumn('content_length_std_10', F.stddev('content_length').over(window_10))\n",
    "    df_spark = df_spark.withColumn('time_diff_mean_10', F.avg('time_diff_seconds').over(window_10))\n",
    "    df_spark = df_spark.withColumn('time_diff_std_10', F.stddev('time_diff_seconds').over(window_10))\n",
    "    \n",
    "    hour_counts = df_spark.groupBy('hour').count().withColumnRenamed('count', 'hour_frequency')\n",
    "    df_spark = df_spark.join(hour_counts, on='hour', how='left')\n",
    "    \n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', F.col('AnomalyLabel').cast(IntegerType()))\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', \n",
    "                                        F.when(F.col('AnomalyLabel').isNull(), 0)\n",
    "                                        .when(F.col('AnomalyLabel') < 0, 0)\n",
    "                                        .when(F.col('AnomalyLabel') > 6, 0)\n",
    "                                        .otherwise(F.col('AnomalyLabel')))\n",
    "    \n",
    "    df_spark.cache()\n",
    "    \n",
    "    total_count = df_spark.count()\n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        label_dist = df_spark.groupBy('AnomalyLabel').count().orderBy('AnomalyLabel').collect()\n",
    "        print(f\"Total: {total_count:,}\")\n",
    "        print(\"Label distribution:\")\n",
    "        for row in label_dist:\n",
    "            lbl = row['AnomalyLabel']\n",
    "            cnt = row['count']\n",
    "            lbl_name = PROJECT_CONFIG['label_map'].get(lbl, 'unknown')\n",
    "            print(f\"  {lbl} ({lbl_name}): {cnt:,} ({cnt/total_count*100:.2f}%)\")\n",
    "    \n",
    "    max_samples = 5000\n",
    "    if total_count > max_samples:\n",
    "        sample_fraction = max_samples / total_count\n",
    "        df_spark_sampled = df_spark.sample(withReplacement=False, fraction=sample_fraction, seed=RANDOM_SEED)\n",
    "        print(f\"Sampled {df_spark_sampled.count()} rows\")\n",
    "    else:\n",
    "        df_spark_sampled = df_spark\n",
    "    \n",
    "    df_pandas = df_spark_sampled.toPandas()\n",
    "    \n",
    "    print(f\"Converted to Pandas: {df_pandas.shape}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # APPLY ENHANCED FEATURE ENGINEERING FOR IMBALANCED DATA\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Phase 1: Add imbalance-aware features\n",
    "    df_pandas = add_imbalance_aware_features(df_pandas, log_source, content_col)\n",
    "    \n",
    "    # Phase 2: Enhanced temporal features\n",
    "    df_pandas = add_temporal_anomaly_features(df_pandas)\n",
    "    \n",
    "    # Phase 3: Statistical anomaly features\n",
    "    df_pandas = add_statistical_anomaly_features(df_pandas)\n",
    "    \n",
    "    # Analyze class imbalance\n",
    "    imbalance_analysis = analyze_class_imbalance(df_pandas)\n",
    "    \n",
    "    if imbalance_analysis:\n",
    "        print(f\"IMBALANCE ANALYSIS:\")\n",
    "        print(f\"Classes present: {len([c for c in range(7) if imbalance_analysis['class_distribution'][c] > 0])}/7\")\n",
    "        print(f\"Imbalance ratio: {imbalance_analysis['imbalance_ratio']:.2f}:1\")\n",
    "        \n",
    "        if imbalance_analysis['minority_classes']:\n",
    "            print(f\"Minority classes: {imbalance_analysis['minority_classes']}\")\n",
    "        if imbalance_analysis['extreme_minority']:\n",
    "            print(f\"Extreme minority: {imbalance_analysis['extreme_minority']}\")\n",
    "    \n",
    "    print(f\"Enhanced features shape: {df_pandas.shape}\")\n",
    "    print()\n",
    "    \n",
    "    pyspark_features_data[log_source] = {\n",
    "        'spark_df': df_spark,\n",
    "        'pandas_df': df_pandas,\n",
    "        'content_col': content_col,\n",
    "        'total_count': total_count,\n",
    "        'imbalance_analysis': imbalance_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d375b7",
   "metadata": {},
   "source": [
    "Drain Template Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5411c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Template Extraction for Apache_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 6 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 1\n",
      "Templates with high anomaly association: 4\n",
      "\n",
      "Enhanced Template Extraction for BGL_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 105 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 32\n",
      "Templates with high anomaly association: 59\n",
      "\n",
      "Enhanced Template Extraction for Hadoop_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 102 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 1\n",
      "Templates with high anomaly association: 21\n",
      "\n",
      "Enhanced Template Extraction for HDFS_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 16 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 0\n",
      "Templates with high anomaly association: 1\n",
      "\n",
      "Enhanced Template Extraction for HPC_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 45 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 6\n",
      "Templates with high anomaly association: 20\n",
      "\n",
      "Enhanced Template Extraction for Linux_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 110 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 32\n",
      "Templates with high anomaly association: 45\n",
      "\n",
      "Enhanced Template Extraction for OpenSSH_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 23 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 17\n",
      "Templates with high anomaly association: 18\n",
      "\n",
      "Enhanced Template Extraction for Proxifier_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 403 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 0\n",
      "Templates with high anomaly association: 35\n",
      "\n",
      "Enhanced Template Extraction for Zookeeper_2k\n",
      "Processing log messages for template extraction...\n",
      "Extracted 46 unique templates\n",
      "Enhanced template features shape: (2000, 15)\n",
      "Template features per sample: 15\n",
      "Templates with high minority class association: 1\n",
      "Templates with high anomaly association: 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_data = {}\n",
    "\n",
    "for log_source, data_dict in pyspark_features_data.items():\n",
    "    print(f\"Enhanced Template Extraction for {log_source}\")\n",
    "    \n",
    "    df_pandas = data_dict['pandas_df']\n",
    "    content_col = data_dict['content_col']\n",
    "    \n",
    "    source_config = drain_configs.get(log_source, drain_configs['default'])\n",
    "    \n",
    "    drain_config = TemplateMinerConfig()\n",
    "    drain_config.drain_sim_th = source_config['sim_th']\n",
    "    drain_config.drain_depth = source_config['depth']\n",
    "    drain_config.drain_max_children = 100\n",
    "    \n",
    "    # Enhanced masking instructions for better anomaly detection\n",
    "    drain_config.masking_instructions = [\n",
    "        MaskingInstruction(r'\\d+', \"<NUM>\"),\n",
    "        MaskingInstruction(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', \"<UUID>\"),\n",
    "        MaskingInstruction(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', \"<IP>\"),\n",
    "        MaskingInstruction(r'/[^\\s]*', \"<PATH>\"),\n",
    "        MaskingInstruction(r'\\b[0-9a-fA-F]{8,}\\b', \"<HEX>\"),  # Hexadecimal values\n",
    "        MaskingInstruction(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', \"<DATE>\"),  # Date patterns\n",
    "        MaskingInstruction(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', \"<TIME>\"),  # Time patterns\n",
    "    ]\n",
    "    \n",
    "    template_miner = TemplateMiner(config=drain_config)\n",
    "    \n",
    "    templates = {}\n",
    "    template_ids = []\n",
    "    \n",
    "    labels = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    \n",
    "    print(\"Processing log messages for template extraction...\")\n",
    "    for idx, content in enumerate(df_pandas[content_col].fillna(\"\").astype(str)):\n",
    "        if content.strip() == \"\":\n",
    "            template_ids.append(-1)\n",
    "            continue\n",
    "        \n",
    "        result = template_miner.add_log_message(content.strip())\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        \n",
    "        if tid not in templates:\n",
    "            templates[tid] = {\n",
    "                'template': result[\"template_mined\"],\n",
    "                'count': 1,\n",
    "                'class_dist': [0] * 7,\n",
    "                'anomaly_score': 0.0,\n",
    "                'minority_score': 0.0\n",
    "            }\n",
    "        else:\n",
    "            templates[tid]['count'] += 1\n",
    "        \n",
    "        if labels is not None:\n",
    "            lbl = int(labels[idx])\n",
    "            templates[tid]['class_dist'][lbl] += 1\n",
    "    \n",
    "    print(f\"Extracted {len(templates)} unique templates\")\n",
    "    \n",
    "    # Calculate enhanced template scores\n",
    "    for tid, template_info in templates.items():\n",
    "        class_probs = np.array(template_info['class_dist']) / (template_info['count'] + 1e-6)\n",
    "        \n",
    "        # Anomaly score (all non-normal classes)\n",
    "        template_info['anomaly_score'] = sum(class_probs[c] for c in [1, 2, 3, 4, 5, 6])\n",
    "        \n",
    "        # Minority score (focus on classes with low coverage)\n",
    "        template_info['minority_score'] = sum(class_probs[c] for c in [1, 3, 6])\n",
    "    \n",
    "    # Create enhanced template features using the improved function\n",
    "    enhanced_template_features = create_imbalance_aware_template_features(\n",
    "        {log_source: {'templates': templates, 'template_ids': template_ids}}, \n",
    "        labels\n",
    "    )\n",
    "    \n",
    "    template_data[log_source] = {\n",
    "        'templates': templates,\n",
    "        'template_ids': template_ids,\n",
    "        'enhanced_features': enhanced_template_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Enhanced template features shape: {template_data[log_source]['enhanced_features'].shape}\")\n",
    "    print(f\"Template features per sample: {template_data[log_source]['enhanced_features'].shape[1]}\")\n",
    "    \n",
    "    # Template analysis for imbalanced data\n",
    "    if labels is not None:\n",
    "        minority_templates = []\n",
    "        anomaly_templates = []\n",
    "        \n",
    "        for tid, template_info in templates.items():\n",
    "            if template_info['minority_score'] > 0.5:\n",
    "                minority_templates.append(tid)\n",
    "            if template_info['anomaly_score'] > 0.3:\n",
    "                anomaly_templates.append(tid)\n",
    "        \n",
    "        print(f\"Templates with high minority class association: {len(minority_templates)}\")\n",
    "        print(f\"Templates with high anomaly association: {len(anomaly_templates)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e6e79",
   "metadata": {},
   "source": [
    "Bert Embeddings and Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60084bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced BERT Feature Generation for Apache_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for BGL_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for Hadoop_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for HDFS_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for HPC_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for Linux_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for OpenSSH_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for Proxifier_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n",
      "Enhanced BERT Feature Generation for Zookeeper_2k\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Calculating enhanced statistical features...\n",
      "Enhanced statistical features shape: (2000, 30)\n",
      "Calculating anomaly-specific BERT features...\n",
      "Sentence-level features shape: (2000, 5)\n",
      "Total BERT-based features: 803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_features_data = {}\n",
    "\n",
    "for log_source, data_dict in pyspark_features_data.items():\n",
    "    print(f\"Enhanced BERT Feature Generation for {log_source}\")\n",
    "    \n",
    "    df_pandas = data_dict['pandas_df']\n",
    "    content_col = data_dict['content_col']\n",
    "    \n",
    "    texts = df_pandas[content_col].fillna(\"\").astype(str).tolist()\n",
    "    labels = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts...\")\n",
    "    all_embeddings = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Generate BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = bert_model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f\"  Processed {i}/{len(texts)} texts\")\n",
    "    \n",
    "    bert_embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "    \n",
    "    # Enhanced statistical features for imbalanced data\n",
    "    print(\"Calculating enhanced statistical features...\")\n",
    "    \n",
    "    # Multiple window sizes for different temporal patterns\n",
    "    window_sizes = [5, 10, 20, 50]\n",
    "    statistical_features = []\n",
    "    \n",
    "    for i in range(len(bert_embeddings)):\n",
    "        sample_stats = []\n",
    "        \n",
    "        for window_size in window_sizes:\n",
    "            start = max(0, i - window_size)\n",
    "            window = bert_embeddings[start:i+1]\n",
    "            \n",
    "            # Basic statistics\n",
    "            mean_emb = np.mean(window, axis=0)\n",
    "            std_emb = np.std(window, axis=0)\n",
    "            distance_from_mean = np.linalg.norm(bert_embeddings[i] - mean_emb)\n",
    "            avg_std = np.mean(std_emb)\n",
    "            \n",
    "            # Distance-based features\n",
    "            if len(window) > 1:\n",
    "                distances = [np.linalg.norm(bert_embeddings[i] - w) for w in window]\n",
    "                min_dist = np.min(distances)\n",
    "                max_dist = np.max(distances)\n",
    "                median_dist = np.median(distances)\n",
    "                \n",
    "                # Outlier detection features\n",
    "                q75, q25 = np.percentile(distances, [75, 25])\n",
    "                iqr = q75 - q25\n",
    "                outlier_threshold = q75 + 1.5 * iqr\n",
    "                is_outlier = 1 if distance_from_mean > outlier_threshold else 0\n",
    "            else:\n",
    "                min_dist = max_dist = median_dist = 0\n",
    "                is_outlier = 0\n",
    "            \n",
    "            # Anomaly-specific features\n",
    "            cosine_sim_mean = np.dot(bert_embeddings[i], mean_emb) / (\n",
    "                np.linalg.norm(bert_embeddings[i]) * np.linalg.norm(mean_emb) + 1e-8\n",
    "            )\n",
    "            \n",
    "            sample_stats.extend([\n",
    "                distance_from_mean, avg_std, min_dist, max_dist, \n",
    "                median_dist, is_outlier, cosine_sim_mean\n",
    "            ])\n",
    "        \n",
    "        # Class-specific features (if labels available)\n",
    "        if labels is not None:\n",
    "            current_label = labels[i]\n",
    "            \n",
    "            # Find similar class samples in window\n",
    "            window_labels = labels[start:i+1] if i > 0 else [current_label]\n",
    "            same_class_ratio = sum(1 for l in window_labels if l == current_label) / len(window_labels)\n",
    "            minority_class_indicator = 1 if current_label in [1, 3, 6] else 0\n",
    "            \n",
    "            sample_stats.extend([same_class_ratio, minority_class_indicator])\n",
    "        else:\n",
    "            sample_stats.extend([0, 0])\n",
    "        \n",
    "        statistical_features.append(sample_stats)\n",
    "    \n",
    "    statistical_features = np.array(statistical_features)\n",
    "    print(f\"Enhanced statistical features shape: {statistical_features.shape}\")\n",
    "    \n",
    "    # Additional anomaly-specific BERT features\n",
    "    print(\"Calculating anomaly-specific BERT features...\")\n",
    "    \n",
    "    # Sentence-level features\n",
    "    sentence_features = []\n",
    "    for i, text in enumerate(texts):\n",
    "        text_len = len(text)\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        # Embedding magnitude (anomalies might have different magnitudes)\n",
    "        emb_magnitude = np.linalg.norm(bert_embeddings[i])\n",
    "        \n",
    "        # Embedding sparsity (count of near-zero values)\n",
    "        emb_sparsity = np.sum(np.abs(bert_embeddings[i]) < 0.01) / len(bert_embeddings[i])\n",
    "        \n",
    "        # Embedding entropy (measure of information content)\n",
    "        emb_normalized = np.abs(bert_embeddings[i]) / (np.sum(np.abs(bert_embeddings[i])) + 1e-8)\n",
    "        emb_entropy = -np.sum(emb_normalized * np.log(emb_normalized + 1e-8))\n",
    "        \n",
    "        sentence_features.append([\n",
    "            text_len, word_count, emb_magnitude, emb_sparsity, emb_entropy\n",
    "        ])\n",
    "    \n",
    "    sentence_features = np.array(sentence_features)\n",
    "    print(f\"Sentence-level features shape: {sentence_features.shape}\")\n",
    "    \n",
    "    bert_features_data[log_source] = {\n",
    "        'embeddings': bert_embeddings,\n",
    "        'statistical_features': statistical_features,\n",
    "        'sentence_features': sentence_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Total BERT-based features: {bert_embeddings.shape[1] + statistical_features.shape[1] + sentence_features.shape[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e4424",
   "metadata": {},
   "source": [
    "Hybrid Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a438a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Feature Assembly for Apache_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1405 (70.25%)\n",
      "  1 (security_anomaly): 32 (1.60%)\n",
      "  2 (system_failure): 563 (28.15%)\n",
      "\n",
      "Enhanced Feature Assembly for BGL_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 501 (25.05%)\n",
      "  1 (security_anomaly): 150 (7.50%)\n",
      "  2 (system_failure): 913 (45.65%)\n",
      "  5 (config_error): 73 (3.65%)\n",
      "  6 (hardware_issue): 363 (18.15%)\n",
      "\n",
      "Enhanced Feature Assembly for Hadoop_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1217 (60.85%)\n",
      "  2 (system_failure): 156 (7.80%)\n",
      "  3 (performance_issue): 1 (0.05%)\n",
      "  4 (network_anomaly): 625 (31.25%)\n",
      "  5 (config_error): 1 (0.05%)\n",
      "\n",
      "Enhanced Feature Assembly for HDFS_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1920 (96.00%)\n",
      "  2 (system_failure): 80 (4.00%)\n",
      "\n",
      "Enhanced Feature Assembly for HPC_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1804 (90.20%)\n",
      "  2 (system_failure): 60 (3.00%)\n",
      "  3 (performance_issue): 29 (1.45%)\n",
      "  4 (network_anomaly): 85 (4.25%)\n",
      "  5 (config_error): 4 (0.20%)\n",
      "  6 (hardware_issue): 18 (0.90%)\n",
      "\n",
      "Enhanced Feature Assembly for Linux_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1359 (67.95%)\n",
      "  1 (security_anomaly): 527 (26.35%)\n",
      "  2 (system_failure): 46 (2.30%)\n",
      "  3 (performance_issue): 15 (0.75%)\n",
      "  4 (network_anomaly): 34 (1.70%)\n",
      "  5 (config_error): 5 (0.25%)\n",
      "  6 (hardware_issue): 14 (0.70%)\n",
      "\n",
      "Enhanced Feature Assembly for OpenSSH_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 424 (21.20%)\n",
      "  1 (security_anomaly): 1569 (78.45%)\n",
      "  5 (config_error): 7 (0.35%)\n",
      "\n",
      "Enhanced Feature Assembly for Proxifier_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1903 (95.15%)\n",
      "  4 (network_anomaly): 97 (4.85%)\n",
      "\n",
      "Enhanced Feature Assembly for Zookeeper_2k\n",
      "Enhanced feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 30)\n",
      "  BERT sentence: (2000, 5)\n",
      "  Template enhanced: (2000, 15)\n",
      "  Temporal: (2000, 12)\n",
      "  Statistical: (2000, 7)\n",
      "  Message complexity: (2000, 9)\n",
      "  Rolling statistics: (2000, 40)\n",
      "  Time-based: (2000, 10)\n",
      "\n",
      "Applying feature selection for imbalanced classes...\n",
      "Selecting top 200 features for imbalanced learning...\n",
      "Selected 200 features for imbalanced learning\n",
      "\n",
      "Created 6 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Label distribution:\n",
      "  0 (normal): 1075 (53.75%)\n",
      "  2 (system_failure): 4 (0.20%)\n",
      "  3 (performance_issue): 37 (1.85%)\n",
      "  4 (network_anomaly): 873 (43.65%)\n",
      "  5 (config_error): 6 (0.30%)\n",
      "  6 (hardware_issue): 5 (0.25%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def select_features_for_imbalanced_classes(X, y, feature_names, top_k=200):\n",
    "    \"\"\"Select features that are most informative for minority classes\"\"\"\n",
    "    \n",
    "    print(f\"Selecting top {top_k} features for imbalanced learning...\")\n",
    "    \n",
    "    # Method 1: Mutual Information (handles imbalanced data well)\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k=min(top_k, X.shape[1]))\n",
    "    mi_selector.fit(X, y)\n",
    "    mi_scores = mi_selector.scores_\n",
    "    \n",
    "    # Method 2: Random Forest Feature Importance (with balanced class weights)\n",
    "    rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    rf_importance = rf.feature_importances_\n",
    "    \n",
    "    # Combine scores (60% MI, 40% RF)\n",
    "    combined_scores = 0.6 * (mi_scores / np.max(mi_scores)) + 0.4 * (rf_importance / np.max(rf_importance))\n",
    "    \n",
    "    # Select top features\n",
    "    top_indices = np.argsort(combined_scores)[-top_k:]\n",
    "    selected_features = [feature_names[i] for i in top_indices] if feature_names else list(range(top_k))\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for imbalanced learning\")\n",
    "    \n",
    "    return top_indices, selected_features, combined_scores\n",
    "\n",
    "hybrid_features_data = {}\n",
    "\n",
    "for log_source in pyspark_features_data.keys():\n",
    "    if log_source not in bert_features_data or log_source not in template_data:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Enhanced Feature Assembly for {log_source}\")\n",
    "    \n",
    "    df_pandas = pyspark_features_data[log_source]['pandas_df']\n",
    "    bert_emb = bert_features_data[log_source]['embeddings']\n",
    "    bert_stat_features = bert_features_data[log_source]['statistical_features']\n",
    "    bert_sentence_features = bert_features_data[log_source]['sentence_features']\n",
    "    template_features = template_data[log_source]['enhanced_features']\n",
    "    \n",
    "    # Enhanced feature collection\n",
    "    temporal_cols = [\n",
    "        'hour', 'day_of_week', 'is_weekend', 'is_business_hours', \n",
    "        'time_diff_seconds', 'logs_last_minute', 'is_night',\n",
    "        'is_off_hours', 'is_weekend_night', 'is_holiday_period',\n",
    "        'is_burst', 'is_isolated'\n",
    "    ]\n",
    "    \n",
    "    statistical_cols = [\n",
    "        'content_length', 'word_count', 'content_length_mean_10', \n",
    "        'content_length_std_10', 'time_diff_mean_10', 'time_diff_std_10',\n",
    "        'hour_frequency'\n",
    "    ]\n",
    "    \n",
    "    # Anomaly-specific features\n",
    "    anomaly_cols = [col for col in df_pandas.columns if col.startswith('has_')]\n",
    "    complexity_cols = [\n",
    "        'msg_length', 'msg_word_count', 'msg_unique_chars', 'msg_entropy',\n",
    "        'special_char_ratio', 'number_ratio', 'uppercase_ratio',\n",
    "        'repeated_words', 'repeated_chars'\n",
    "    ]\n",
    "    \n",
    "    # Rolling window features\n",
    "    rolling_cols = [col for col in df_pandas.columns if any(x in col for x in ['_mean_', '_std_', '_zscore_', '_outlier_'])]\n",
    "    \n",
    "    # Time-based features\n",
    "    time_cols = [col for col in df_pandas.columns if 'log_count_' in col or 'error_density_' in col]\n",
    "    \n",
    "    # Collect available features\n",
    "    available_temporal = [c for c in temporal_cols if c in df_pandas.columns]\n",
    "    available_statistical = [c for c in statistical_cols if c in df_pandas.columns]\n",
    "    available_anomaly = [c for c in anomaly_cols if c in df_pandas.columns]\n",
    "    available_complexity = [c for c in complexity_cols if c in df_pandas.columns]\n",
    "    available_rolling = [c for c in rolling_cols if c in df_pandas.columns]\n",
    "    available_time = [c for c in time_cols if c in df_pandas.columns]\n",
    "    \n",
    "    # Extract feature arrays\n",
    "    temporal_features = df_pandas[available_temporal].fillna(0).values if available_temporal else None\n",
    "    statistical_features = df_pandas[available_statistical].fillna(0).values if available_statistical else None\n",
    "    anomaly_features = df_pandas[available_anomaly].fillna(0).values if available_anomaly else None\n",
    "    complexity_features = df_pandas[available_complexity].fillna(0).values if available_complexity else None\n",
    "    rolling_features = df_pandas[available_rolling].fillna(0).values if available_rolling else None\n",
    "    time_features = df_pandas[available_time].fillna(0).values if available_time else None\n",
    "    \n",
    "    print(f\"Enhanced feature dimensions:\")\n",
    "    print(f\"  BERT embeddings: {bert_emb.shape}\")\n",
    "    print(f\"  BERT statistical: {bert_stat_features.shape}\")\n",
    "    print(f\"  BERT sentence: {bert_sentence_features.shape}\")\n",
    "    print(f\"  Template enhanced: {template_features.shape}\")\n",
    "    if temporal_features is not None:\n",
    "        print(f\"  Temporal: {temporal_features.shape}\")\n",
    "    if statistical_features is not None:\n",
    "        print(f\"  Statistical: {statistical_features.shape}\")\n",
    "    if anomaly_features is not None:\n",
    "        print(f\"  Anomaly patterns: {anomaly_features.shape}\")\n",
    "    if complexity_features is not None:\n",
    "        print(f\"  Message complexity: {complexity_features.shape}\")\n",
    "    if rolling_features is not None:\n",
    "        print(f\"  Rolling statistics: {rolling_features.shape}\")\n",
    "    if time_features is not None:\n",
    "        print(f\"  Time-based: {time_features.shape}\")\n",
    "    \n",
    "    # Create enhanced feature variants\n",
    "    feature_variants = {}\n",
    "    \n",
    "    # Basic variants\n",
    "    feature_variants['bert_only'] = bert_emb\n",
    "    feature_variants['bert_enhanced'] = np.hstack([bert_emb, bert_stat_features, bert_sentence_features])\n",
    "    feature_variants['template_enhanced'] = template_features\n",
    "    \n",
    "    # Imbalance-aware variants\n",
    "    imbalance_components = [bert_emb, bert_stat_features, template_features]\n",
    "    \n",
    "    if anomaly_features is not None:\n",
    "        imbalance_components.append(anomaly_features)\n",
    "        feature_variants['anomaly_focused'] = np.hstack([bert_emb, anomaly_features, template_features])\n",
    "    \n",
    "    if complexity_features is not None:\n",
    "        imbalance_components.append(complexity_features)\n",
    "    \n",
    "    if temporal_features is not None:\n",
    "        imbalance_components.append(temporal_features)\n",
    "    \n",
    "    if statistical_features is not None:\n",
    "        imbalance_components.append(statistical_features)\n",
    "    \n",
    "    if rolling_features is not None:\n",
    "        imbalance_components.append(rolling_features)\n",
    "    \n",
    "    if time_features is not None:\n",
    "        imbalance_components.append(time_features)\n",
    "    \n",
    "    # Comprehensive feature set\n",
    "    feature_variants['imbalance_aware_full'] = np.hstack(imbalance_components)\n",
    "    \n",
    "    # Sentence-level focused variant\n",
    "    if bert_sentence_features is not None:\n",
    "        sentence_components = [bert_emb, bert_sentence_features, template_features]\n",
    "        if complexity_features is not None:\n",
    "            sentence_components.append(complexity_features)\n",
    "        feature_variants['sentence_focused'] = np.hstack(sentence_components)\n",
    "    \n",
    "    labels = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    \n",
    "    # Feature selection for imbalanced data\n",
    "    if labels is not None and len(np.unique(labels)) > 1:\n",
    "        print(\"\\nApplying feature selection for imbalanced classes...\")\n",
    "        \n",
    "        # Create feature names\n",
    "        feature_names = []\n",
    "        feature_names.extend([f'bert_{i}' for i in range(bert_emb.shape[1])])\n",
    "        feature_names.extend([f'bert_stat_{i}' for i in range(bert_stat_features.shape[1])])\n",
    "        feature_names.extend([f'template_{i}' for i in range(template_features.shape[1])])\n",
    "        feature_names.extend(available_anomaly)\n",
    "        feature_names.extend(available_complexity)\n",
    "        feature_names.extend(available_temporal)\n",
    "        feature_names.extend(available_statistical)\n",
    "        feature_names.extend(available_rolling)\n",
    "        feature_names.extend(available_time)\n",
    "        \n",
    "        # Apply feature selection\n",
    "        full_features = feature_variants['imbalance_aware_full']\n",
    "        top_indices, selected_features, feature_scores = select_features_for_imbalanced_classes(\n",
    "            full_features, labels, feature_names, top_k=min(200, full_features.shape[1])\n",
    "        )\n",
    "        \n",
    "        feature_variants['selected_imbalanced'] = full_features[:, top_indices]\n",
    "        \n",
    "        # Store feature selection info\n",
    "        feature_selection_info = {\n",
    "            'selected_indices': top_indices,\n",
    "            'selected_features': selected_features,\n",
    "            'feature_scores': feature_scores,\n",
    "            'total_features': full_features.shape[1]\n",
    "        }\n",
    "    else:\n",
    "        feature_selection_info = None\n",
    "    \n",
    "    hybrid_features_data[log_source] = {\n",
    "        'feature_variants': feature_variants,\n",
    "        'labels': labels,\n",
    "        'texts': df_pandas[pyspark_features_data[log_source]['content_col']].tolist(),\n",
    "        'feature_selection_info': feature_selection_info,\n",
    "        'imbalance_analysis': pyspark_features_data[log_source]['imbalance_analysis']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCreated {len(feature_variants)} enhanced feature variants:\")\n",
    "    for variant_name, features in feature_variants.items():\n",
    "        print(f\"  - {variant_name}: {features.shape[1]} features\")\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\nLabel distribution:\")\n",
    "        for lbl, cnt in zip(unique, counts):\n",
    "            lbl_name = PROJECT_CONFIG['label_map'].get(int(lbl), 'unknown')\n",
    "            print(f\"  {int(lbl)} ({lbl_name}): {cnt} ({cnt/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd2de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_imbalanced_features.pkl\n"
     ]
    }
   ],
   "source": [
    "features_save_path = FEATURES_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "with open(features_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'hybrid_features_data': hybrid_features_data,\n",
    "        'template_data': template_data,\n",
    "        'bert_features_data': bert_features_data,\n",
    "        'pyspark_features_data': {k: {\n",
    "            'imbalance_analysis': v['imbalance_analysis'],\n",
    "            'total_count': v['total_count'],\n",
    "            'content_col': v['content_col']\n",
    "        } for k, v in pyspark_features_data.items()},\n",
    "        'feature_types': list(hybrid_features_data[list(hybrid_features_data.keys())[0]]['feature_variants'].keys()),\n",
    "        'config': PROJECT_CONFIG,\n",
    "        'enhancement_info': {\n",
    "            'anomaly_patterns_added': True,\n",
    "            'temporal_features_enhanced': True,\n",
    "            'statistical_features_enhanced': True,\n",
    "            'template_features_enhanced': True,\n",
    "            'bert_features_enhanced': True,\n",
    "            'feature_selection_applied': True,\n",
    "            'imbalance_analysis_included': True\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nSaved: {features_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588df34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_cross_source_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "cross_source_splits = []\n",
    "\n",
    "for test_source in hybrid_features_data.keys():\n",
    "    train_sources = [s for s in hybrid_features_data.keys() if s != test_source]\n",
    "    \n",
    "    if hybrid_features_data[test_source]['labels'] is None:\n",
    "        continue\n",
    "    \n",
    "    test_samples = len(hybrid_features_data[test_source]['labels'])\n",
    "    train_samples = sum(len(hybrid_features_data[s]['labels']) \n",
    "                       for s in train_sources \n",
    "                       if hybrid_features_data[s]['labels'] is not None)\n",
    "    \n",
    "    # Add imbalance analysis for this split\n",
    "    test_imbalance = hybrid_features_data[test_source]['imbalance_analysis']\n",
    "    \n",
    "    # Calculate combined train imbalance\n",
    "    train_label_counts = Counter()\n",
    "    for s in train_sources:\n",
    "        if hybrid_features_data[s]['labels'] is not None:\n",
    "            for label in hybrid_features_data[s]['labels']:\n",
    "                train_label_counts[label] += 1\n",
    "    \n",
    "    train_imbalance_ratio = max(train_label_counts.values()) / min(train_label_counts.values()) if train_label_counts else 1.0\n",
    "    \n",
    "    cross_source_splits.append({\n",
    "        'test_source': test_source,\n",
    "        'train_sources': train_sources,\n",
    "        'test_samples': test_samples,\n",
    "        'train_samples': train_samples,\n",
    "        'test_imbalance_analysis': test_imbalance,\n",
    "        'train_imbalance_ratio': train_imbalance_ratio,\n",
    "        'train_label_distribution': dict(train_label_counts)\n",
    "    })\n",
    "\n",
    "splits_save_path = FEATURES_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "with open(splits_save_path, 'wb') as f:\n",
    "    pickle.dump({'splits': cross_source_splits}, f)\n",
    "\n",
    "print(f\"Saved: {splits_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c438dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9 log sources with enhanced features\n",
      "\n",
      " Enhanced Feature Variants Created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 15 features\n",
      "  - imbalance_aware_full: 891 features\n",
      "  - sentence_focused: 797 features\n",
      "  - selected_imbalanced: 200 features\n",
      "\n",
      "Imbalance Analysis Summary:\n",
      "  - Sources with extreme imbalance (>100:1): 5\n",
      "  - Sources with high imbalance (>10:1): 4\n",
      "\n",
      "Class Coverage Across Sources:\n",
      "   Class 0 (normal): 9/9 sources (100.0%)\n",
      "   Class 1 (security_anomaly): 4/9 sources (44.4%)\n",
      "   Class 2 (system_failure): 7/9 sources (77.8%)\n",
      "   Class 3 (performance_issue): 4/9 sources (44.4%)\n",
      "   Class 4 (network_anomaly): 5/9 sources (55.6%)\n",
      "   Class 5 (config_error): 6/9 sources (66.7%)\n",
      "   Class 6 (hardware_issue): 4/9 sources (44.4%)\n",
      "Files Saved:\n",
      "  - Enhanced features: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_imbalanced_features.pkl\n",
      "  - Enhanced splits: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_cross_source_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "total_sources = len(hybrid_features_data)\n",
    "print(f\"Processed {total_sources} log sources with enhanced features\")\n",
    "\n",
    "# Feature variant analysis\n",
    "if hybrid_features_data:\n",
    "    sample_source = list(hybrid_features_data.keys())[0]\n",
    "    feature_variants = hybrid_features_data[sample_source]['feature_variants']\n",
    "    \n",
    "    print(f\"\\n Enhanced Feature Variants Created:\")\n",
    "    for variant_name, features in feature_variants.items():\n",
    "        print(f\"  - {variant_name}: {features.shape[1]} features\")\n",
    "    \n",
    "    # Imbalance analysis summary\n",
    "    print(f\"\\nImbalance Analysis Summary:\")\n",
    "    extreme_imbalance_sources = []\n",
    "    high_imbalance_sources = []\n",
    "    minority_class_coverage = {i: 0 for i in range(7)}\n",
    "    \n",
    "    for source, data in hybrid_features_data.items():\n",
    "        if data['imbalance_analysis']:\n",
    "            imbalance_ratio = data['imbalance_analysis']['imbalance_ratio']\n",
    "            if imbalance_ratio > 100:\n",
    "                extreme_imbalance_sources.append(source)\n",
    "            elif imbalance_ratio > 10:\n",
    "                high_imbalance_sources.append(source)\n",
    "            \n",
    "            # Count class coverage\n",
    "            for class_id, count in data['imbalance_analysis']['class_distribution'].items():\n",
    "                if count > 0:\n",
    "                    minority_class_coverage[class_id] += 1\n",
    "    \n",
    "    print(f\"  - Sources with extreme imbalance (>100:1): {len(extreme_imbalance_sources)}\")\n",
    "    print(f\"  - Sources with high imbalance (>10:1): {len(high_imbalance_sources)}\")\n",
    "    \n",
    "    print(f\"\\nClass Coverage Across Sources:\")\n",
    "    for class_id, coverage in minority_class_coverage.items():\n",
    "        class_name = PROJECT_CONFIG['label_map'][class_id]\n",
    "        coverage_pct = (coverage / total_sources) * 100\n",
    "        status = \"\" if coverage_pct > 50 else \"\" if coverage_pct > 25 else \"\"\n",
    "        print(f\"  {status} Class {class_id} ({class_name}): {coverage}/{total_sources} sources ({coverage_pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"Files Saved:\")\n",
    "    print(f\"  - Enhanced features: {features_save_path}\")\n",
    "    print(f\"  - Enhanced splits: {splits_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
