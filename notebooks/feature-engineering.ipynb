{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21da223",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95dca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import drain3\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.masking import MaskingInstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac1a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "NORMALIZED_DATA_PATH = LABELED_DATA_PATH / \"normalized\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\" / \"cross_source_transfer\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\"\n",
    "FEATURES_PATH = PROJECT_ROOT / \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402d6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset registry with 6 sources\n",
      "Project config loaded for 6 log sources\n"
     ]
    }
   ],
   "source": [
    "FEATURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "registry_path = RESULTS_PATH / \"dataset_registry.json\"\n",
    "if registry_path.exists():\n",
    "    with open(registry_path, 'r') as f:\n",
    "        registry_data = json.load(f)\n",
    "        dataset_registry = registry_data['dataset_registry']\n",
    "        PROJECT_CONFIG = registry_data['project_config']\n",
    "    print(f\"Loaded dataset registry with {len(dataset_registry)} sources\")\n",
    "    print(f\"Project config loaded for {len(PROJECT_CONFIG['log_sources'])} log sources\")\n",
    "else:\n",
    "    print(\"Dataset registry not found. Please run data processing notebook first.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa3e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering environment initialized\n",
      "Spark session ready: 3.4.1\n",
      "BERT tokenizer loaded: bert-base-uncased\n",
      "Features will be saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .appName(\"FeatureEngineering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "\n",
    "print(f\"Feature engineering environment initialized\")\n",
    "print(f\"Spark session ready: {spark.version}\")\n",
    "print(f\"BERT tokenizer loaded: {PROJECT_CONFIG['bert_model_name']}\")\n",
    "print(f\"Features will be saved to: {FEATURES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b457a91",
   "metadata": {},
   "source": [
    "Template Extraction using Drain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ce50bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drain3 configuration set successfully!\n",
      "Similarity threshold: 0.4\n",
      "Masking instructions loaded: 4\n"
     ]
    }
   ],
   "source": [
    "drain_config = TemplateMinerConfig()\n",
    "drain_config.drain_sim_th = 0.4\n",
    "drain_config.drain_depth = 4\n",
    "drain_config.drain_max_children = 100\n",
    "drain_config.masking_instructions = [\n",
    "    MaskingInstruction(r'\\d+', \"<NUM>\"),\n",
    "    MaskingInstruction(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', \"<UUID>\"),\n",
    "    MaskingInstruction(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', \"<IP>\"),\n",
    "    MaskingInstruction(r'/[^\\s]*', \"<PATH>\")\n",
    "]\n",
    "\n",
    "print(\"Drain3 configuration set successfully!\")\n",
    "print(f\"Similarity threshold: {drain_config.drain_sim_th}\")\n",
    "print(f\"Masking instructions loaded: {len(drain_config.masking_instructions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df327681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_templates_per_source(log_source, content_data, max_lines=10000):\n",
    "    print(f\"\\n--- Processing {log_source} ---\")\n",
    "    \n",
    "    template_miner = TemplateMiner(config=drain_config)\n",
    "    \n",
    "    if len(content_data) > max_lines:\n",
    "        sampled_data = content_data.sample(n=max_lines, random_state=RANDOM_SEED)\n",
    "        print(f\"Sampled {max_lines} lines from {len(content_data)} total lines\")\n",
    "    else:\n",
    "        sampled_data = content_data\n",
    "        print(f\"Processing all {len(content_data)} lines\")\n",
    "    \n",
    "    templates = {}\n",
    "    template_ids = []\n",
    "    \n",
    "    for idx, content in enumerate(sampled_data):\n",
    "        if pd.isna(content) or content.strip() == \"\":\n",
    "            template_ids.append(-1)  \n",
    "            continue\n",
    "            \n",
    "        result = template_miner.add_log_message(str(content).strip())\n",
    "        template_id = result[\"cluster_id\"]\n",
    "        template_ids.append(template_id)\n",
    "        \n",
    "        if template_id not in templates:\n",
    "            templates[template_id] = {\n",
    "                'template': result[\"template_mined\"],\n",
    "                'count': 1,\n",
    "                'size': result[\"cluster_size\"]\n",
    "            }\n",
    "        else:\n",
    "            templates[template_id]['count'] += 1\n",
    "    \n",
    "    print(f\"Extracted {len(templates)} unique templates\")\n",
    "    print(f\"Template coverage: {len([t for t in template_ids if t != -1])/len(template_ids)*100:.1f}%\")\n",
    "    \n",
    "    sorted_templates = sorted(templates.items(), key=lambda x: x[1]['count'], reverse=True)\n",
    "    print(f\"\\nTop 5 templates:\")\n",
    "    for i, (tid, info) in enumerate(sorted_templates[:5], 1):\n",
    "        print(f\"   {i}. [ID: {tid}] ({info['count']} instances) {info['template'][:100]}...\")\n",
    "    \n",
    "    return templates, template_ids, template_miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65213299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Apache ---\n",
      "Processing all 2000 lines\n",
      "Extracted 6 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 3] (836 instances) jk<<NUM>>_init() Found child <<NUM>> in scoreboard slot <<NUM>>...\n",
      "   2. [ID: 1] (569 instances) workerEnv.init() ok <<PATH>>...\n",
      "   3. [ID: 2] (539 instances) mod_jk child workerEnv in error state <<NUM>>...\n",
      "   4. [ID: 4] (32 instances) [client <<NUM>>.<<NUM>>.<<NUM>>.<<NUM>>] Directory index forbidden by rule: <<PATH>>...\n",
      "   5. [ID: 5] (12 instances) jk<<NUM>>_init() Can't find child <<NUM>> in scoreboard...\n",
      "\n",
      "--- Processing BGL ---\n",
      "Processing all 2000 lines\n",
      "Extracted 105 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 5] (721 instances) generating core.<<NUM>>...\n",
      "   2. [ID: 69] (208 instances) iar <<NUM>>a<<NUM>>fc dear <<NUM>>b<<NUM>>e<<NUM>>...\n",
      "   3. [ID: 55] (121 instances) <<NUM>> floating point alignment exceptions...\n",
      "   4. [ID: 2] (109 instances) <<NUM>> double-hummer alignment exceptions...\n",
      "   5. [ID: 3] (92 instances) CE sym <<NUM>>, at <<NUM>>x<<NUM>>b<<NUM>>eee<<NUM>>, mask <<NUM>>x<<NUM>>...\n",
      "\n",
      "--- Processing HPC ---\n",
      "Processing all 2000 lines\n",
      "Extracted 45 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 40] (394 instances) Linkerror event interval expired...\n",
      "   2. [ID: 38] (292 instances) ambient=<<NUM>>...\n",
      "   3. [ID: 36] (206 instances) normal...\n",
      "   4. [ID: 37] (205 instances) warning...\n",
      "   5. [ID: 25] (162 instances) running...\n",
      "\n",
      "--- Processing OpenSSH ---\n",
      "Processing all 2000 lines\n",
      "Extracted 23 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 8] (414 instances) Received disconnect from <<NUM>>.<<NUM>>.<<NUM>>.<<NUM>>: <<NUM>>: Bye Bye [preauth]...\n",
      "   2. [ID: 9] (384 instances) pam_unix(sshd:auth): authentication failure; logname= uid=<<NUM>> euid=<<NUM>> tty=ssh ruser= rhost=...\n",
      "   3. [ID: 10] (383 instances) Failed password for root from <<NUM>>.<<NUM>>.<<NUM>>.<<NUM>> port <<NUM>> ssh<<NUM>>...\n",
      "   4. [ID: 6] (139 instances) Failed password for invalid user webmaster from <<NUM>>.<<NUM>>.<<NUM>>.<<NUM>> port <<NUM>> ssh<<NU...\n",
      "   5. [ID: 4] (135 instances) pam_unix(sshd:auth): check pass; user unknown...\n",
      "\n",
      "--- Processing Proxifier ---\n",
      "Processing all 2000 lines\n",
      "Extracted 403 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 1] (457 instances) proxy.cse.cuhk.edu.hk:<<NUM>> open through proxy proxy.cse.cuhk.edu.hk:<<NUM>> HTTPS...\n",
      "   2. [ID: 239] (137 instances) rm.api.weibo.com:<<NUM>> open through proxy proxy.cse.cuhk.edu.hk:<<NUM>> HTTPS...\n",
      "   3. [ID: 2] (110 instances) proxy.cse.cuhk.edu.hk:<<NUM>> close, <<NUM>> bytes sent, <<NUM>> bytes received, lifetime <<NUM>>:<<...\n",
      "   4. [ID: 5] (107 instances) proxy.cse.cuhk.edu.hk:<<NUM>> close, <<NUM>> bytes (<<NUM>>.<<NUM>> KB) sent, <<NUM>> bytes (<<NUM>>...\n",
      "   5. [ID: 6] (93 instances) proxy.cse.cuhk.edu.hk:<<NUM>> close, <<NUM>> bytes sent, <<NUM>> bytes (<<NUM>>.<<NUM>> KB) received...\n",
      "\n",
      "--- Processing Zookeeper ---\n",
      "Processing all 2000 lines\n",
      "Extracted 46 unique templates\n",
      "Template coverage: 100.0%\n",
      "\n",
      "Top 5 templates:\n",
      "   1. [ID: 4] (314 instances) Interrupted while waiting for message on queue...\n",
      "   2. [ID: 2] (299 instances) Received connection request <<PATH>>...\n",
      "   3. [ID: 5] (291 instances) Connection broken for id <<NUM>>, my id = <<NUM>>, error =...\n",
      "   4. [ID: 6] (266 instances) Interrupting SendWorker...\n",
      "   5. [ID: 3] (262 instances) Send worker leaving thread...\n"
     ]
    }
   ],
   "source": [
    "template_data = {}\n",
    "all_templates = {}\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in dataset_registry:\n",
    "        continue\n",
    "        \n",
    "    file_path = dataset_registry[log_source]['file_path']\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    content_col = None\n",
    "    for col in ['Content', 'content', 'Message', 'message', 'Text', 'text']:\n",
    "        if col in df.columns:\n",
    "            content_col = col\n",
    "            break\n",
    "    \n",
    "    if content_col is None:\n",
    "        print(f\"No content column found for {log_source}\")\n",
    "        continue\n",
    "    \n",
    "    content_data = df[content_col].fillna(\"\")\n",
    "    templates, template_ids, miner = extract_templates_per_source(log_source, content_data)\n",
    "    \n",
    "    template_data[log_source] = {\n",
    "        'templates': templates,\n",
    "        'template_ids': template_ids,\n",
    "        'miner': miner,\n",
    "        'content_column': content_col,\n",
    "        'original_data': df\n",
    "    }\n",
    "    \n",
    "    for tid, template_info in templates.items():\n",
    "        global_key = f\"{log_source}_{tid}\"\n",
    "        all_templates[global_key] = {\n",
    "            'source': log_source,\n",
    "            'template_id': tid,\n",
    "            'template': template_info['template'],\n",
    "            'count': template_info['count'],\n",
    "            'frequency': template_info['count'] / len(content_data)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a7aa9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity threshold: 0.4\n",
      "Masking instructions loaded: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity threshold: {drain_config.drain_sim_th}\")\n",
    "print(f\"Masking instructions loaded: {len(drain_config.masking_instructions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17d4b72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 log sources\n",
      "Total unique templates across all sources: 628\n",
      "Templates per source - Min: 6, Max: 403, Avg: 104.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed {len(template_data)} log sources\")\n",
    "print(f\"Total unique templates across all sources: {len(all_templates)}\")\n",
    "\n",
    "template_counts = [len(info['templates']) for info in template_data.values()]\n",
    "print(f\"Templates per source - Min: {min(template_counts)}, Max: {max(template_counts)}, Avg: {np.mean(template_counts):.1f}\")\n",
    "\n",
    "templates_path = FEATURES_PATH / \"template_extraction_results.pkl\"\n",
    "with open(templates_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'template_data': template_data,\n",
    "        'all_templates': all_templates,\n",
    "        'config': drain_config\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e674ee4",
   "metadata": {},
   "source": [
    "BERT Text Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8259f13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from transformers import AutoModel\n",
    "bert_model = AutoModel.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f965b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(texts, batch_size=16, max_length=512):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = bert_model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e501c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_source_for_embeddings(log_source, max_samples=5000):\n",
    "    print(f\"\\n--- Generating BERT embeddings for {log_source} ---\")\n",
    "    \n",
    "    if log_source not in template_data:\n",
    "        print(f\"Template data not available for {log_source}\")\n",
    "        return None\n",
    "    \n",
    "    source_data = template_data[log_source]\n",
    "    df = source_data['original_data']\n",
    "    content_col = source_data['content_column']\n",
    "    \n",
    "    texts = df[content_col].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    if len(texts) > max_samples:\n",
    "        sample_indices = np.random.choice(len(texts), max_samples, replace=False)\n",
    "        sample_indices.sort() \n",
    "        texts = [texts[i] for i in sample_indices]\n",
    "        sampled_df = df.iloc[sample_indices].copy()\n",
    "        print(f\"Sampled {max_samples} texts from {len(df)} total\")\n",
    "    else:\n",
    "        sampled_df = df.copy()\n",
    "        sample_indices = None\n",
    "        print(f\"Processing all {len(texts)} texts\")\n",
    "    \n",
    "    print(\"Generating BERT embeddings...\")\n",
    "    embeddings = generate_bert_embeddings(texts, batch_size=16) \n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    original_labels = None\n",
    "    binary_labels = None\n",
    "    \n",
    "    if 'AnomalyLabel' in sampled_df.columns:\n",
    "        original_labels = sampled_df['AnomalyLabel'].values\n",
    "        \n",
    "        binary_labels = (original_labels != 0).astype(int)\n",
    "        \n",
    "        anomaly_rate = np.mean(binary_labels) * 100\n",
    "        print(f\"Label distribution: {np.sum(binary_labels == 0)} normal, {np.sum(binary_labels == 1)} anomaly ({anomaly_rate:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': binary_labels, \n",
    "        'texts': texts,\n",
    "        'sample_indices': sample_indices,\n",
    "        'original_labels': original_labels, \n",
    "        'dataframe': sampled_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b629bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating BERT embeddings for Apache ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 1405 normal, 595 anomaly (29.8%)\n",
      "\n",
      "--- Generating BERT embeddings for BGL ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 501 normal, 1499 anomaly (75.0%)\n",
      "\n",
      "--- Generating BERT embeddings for HPC ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 1804 normal, 196 anomaly (9.8%)\n",
      "\n",
      "--- Generating BERT embeddings for OpenSSH ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 424 normal, 1576 anomaly (78.8%)\n",
      "\n",
      "--- Generating BERT embeddings for Proxifier ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 1903 normal, 97 anomaly (4.9%)\n",
      "\n",
      "--- Generating BERT embeddings for Zookeeper ---\n",
      "Processing all 2000 texts\n",
      "Generating BERT embeddings...\n",
      "Generated embeddings shape: (2000, 768)\n",
      "Label distribution: 1075 normal, 925 anomaly (46.2%)\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings_data = {}\n",
    "embedding_stats = []\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in dataset_registry:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        embedding_result = process_source_for_embeddings(log_source)\n",
    "        if embedding_result is not None:\n",
    "            bert_embeddings_data[log_source] = embedding_result\n",
    "            \n",
    "            stats = {\n",
    "                'source': log_source,\n",
    "                'n_samples': embedding_result['embeddings'].shape[0],\n",
    "                'embedding_dim': embedding_result['embeddings'].shape[1],\n",
    "                'has_labels': embedding_result['labels'] is not None,\n",
    "                'anomaly_rate': np.mean(embedding_result['labels']) * 100 if embedding_result['labels'] is not None else None\n",
    "            }\n",
    "            embedding_stats.append(stats)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {log_source}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4835383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 6 sources\n",
      "   Apache       |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:  29.8%\n",
      "   BGL          |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:  75.0%\n",
      "   HPC          |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:   9.8%\n",
      "   OpenSSH      |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:  78.8%\n",
      "   Proxifier    |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:   4.9%\n",
      "   Zookeeper    |  2,000 samples | Dim: 768 | Labels: ✓ | Anomaly:  46.2%\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings_path = FEATURES_PATH / \"bert_embeddings.pkl\"\n",
    "with open(bert_embeddings_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'embeddings_data': bert_embeddings_data,\n",
    "        'model_name': PROJECT_CONFIG['bert_model_name'],\n",
    "        'max_length': PROJECT_CONFIG['max_sequence_length'],\n",
    "        'stats': embedding_stats\n",
    "    }, f)\n",
    "\n",
    "print(f\"Successfully processed {len(bert_embeddings_data)} sources\")\n",
    "for stats in embedding_stats:\n",
    "    print(f\"   {stats['source']:<12} | {stats['n_samples']:>6,} samples | \"\n",
    "          f\"Dim: {stats['embedding_dim']:>3} | Labels: {'✓' if stats['has_labels'] else '✗'} | \"\n",
    "          f\"Anomaly: {stats['anomaly_rate']:>5.1f}%\" if stats['anomaly_rate'] is not None else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b234a",
   "metadata": {},
   "source": [
    "Template + BERT Hybrid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f22d2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template_embeddings(templates, embedding_dim=50):\n",
    "    template_ids = list(templates.keys())\n",
    "    n_templates = len(template_ids)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    template_embeddings = np.random.normal(0, 0.1, (n_templates, embedding_dim))\n",
    "    template_to_idx = {tid: idx for idx, tid in enumerate(template_ids)}\n",
    "    return template_embeddings, template_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa11811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template_features(template_ids, templates, method='frequency', embedding_dim=50):\n",
    "    features = {}\n",
    "    template_counts = Counter(template_ids)\n",
    "    total_logs = len(template_ids)\n",
    "    \n",
    "    if method == 'frequency':\n",
    "        template_freq = {}\n",
    "        for tid in template_ids:\n",
    "            if tid == -1:\n",
    "                template_freq[tid] = 0.0\n",
    "            else:\n",
    "                template_freq[tid] = template_counts[tid] / total_logs\n",
    "        \n",
    "        unique_templates = sorted(set(tid for tid in template_ids if tid != -1))\n",
    "        feature_vector = []\n",
    "        for tid in template_ids:\n",
    "            vec = [0.0] * len(unique_templates)\n",
    "            if tid != -1 and tid in unique_templates:\n",
    "                idx = unique_templates.index(tid)\n",
    "                vec[idx] = template_freq[tid]\n",
    "            feature_vector.append(vec)\n",
    "        \n",
    "        return np.array(feature_vector), unique_templates\n",
    "    \n",
    "    elif method == 'onehot':\n",
    "        unique_templates = sorted(set(tid for tid in template_ids if tid != -1))\n",
    "        feature_vector = []\n",
    "        for tid in template_ids:\n",
    "            vec = [0.0] * len(unique_templates)\n",
    "            if tid != -1 and tid in unique_templates:\n",
    "                idx = unique_templates.index(tid)\n",
    "                vec[idx] = 1.0\n",
    "            feature_vector.append(vec)\n",
    "        \n",
    "        return np.array(feature_vector), unique_templates\n",
    "    \n",
    "    elif method == 'embedding':\n",
    "        unique_templates = sorted(set(tid for tid in template_ids if tid != -1))\n",
    "        template_embeddings, template_to_idx = create_template_embeddings(\n",
    "            {tid: None for tid in unique_templates},\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        feature_vector = []\n",
    "        for tid in template_ids:\n",
    "            if tid == -1 or tid not in template_to_idx:\n",
    "                vec = np.zeros(template_embeddings.shape[1])\n",
    "            else:\n",
    "                vec = template_embeddings[template_to_idx[tid]]\n",
    "            feature_vector.append(vec)\n",
    "        \n",
    "        return np.array(feature_vector), unique_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cf21f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_template_bert_features(bert_emb, template_features, method='concat'):\n",
    "    if method == 'concat':\n",
    "        return np.hstack([bert_emb, template_features])\n",
    "    \n",
    "    elif method == 'weighted':\n",
    "        bert_normalized = bert_emb / np.linalg.norm(bert_emb, axis=1, keepdims=True)\n",
    "        template_normalized = template_features / (np.linalg.norm(template_features, axis=1, keepdims=True) + 1e-8)\n",
    "        return 0.7 * bert_normalized + 0.3 * template_normalized\n",
    "    \n",
    "    elif method == 'attention':\n",
    "        return np.hstack([bert_emb, template_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d83acf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating hybrid features for Apache ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 6), Frequency: (2000, 6), Embedding: (2000, 768)\n",
      "Unique templates: 6\n",
      "Created 4 hybrid feature variants\n",
      "\n",
      "--- Creating hybrid features for BGL ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 105), Frequency: (2000, 105), Embedding: (2000, 768)\n",
      "Unique templates: 105\n",
      "Created 4 hybrid feature variants\n",
      "\n",
      "--- Creating hybrid features for HPC ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 45), Frequency: (2000, 45), Embedding: (2000, 768)\n",
      "Unique templates: 45\n",
      "Created 4 hybrid feature variants\n",
      "\n",
      "--- Creating hybrid features for OpenSSH ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 23), Frequency: (2000, 23), Embedding: (2000, 768)\n",
      "Unique templates: 23\n",
      "Created 4 hybrid feature variants\n",
      "\n",
      "--- Creating hybrid features for Proxifier ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 403), Frequency: (2000, 403), Embedding: (2000, 768)\n",
      "Unique templates: 403\n",
      "Created 4 hybrid feature variants\n",
      "\n",
      "--- Creating hybrid features for Zookeeper ---\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Template features - OneHot: (2000, 46), Frequency: (2000, 46), Embedding: (2000, 768)\n",
      "Unique templates: 46\n",
      "Created 4 hybrid feature variants\n"
     ]
    }
   ],
   "source": [
    "hybrid_features_data = {}\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in bert_embeddings_data or log_source not in template_data:\n",
    "        print(f\"⚠️  Missing data for {log_source}, skipping hybrid features\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n--- Creating hybrid features for {log_source} ---\")\n",
    "    \n",
    "    bert_data = bert_embeddings_data[log_source]\n",
    "    bert_embeddings = bert_data['embeddings']\n",
    "    bert_dim = bert_embeddings.shape[1]\n",
    "    \n",
    "    template_info = template_data[log_source]\n",
    "    template_ids = template_info['template_ids']\n",
    "    templates = template_info['templates']\n",
    "    \n",
    "    if bert_data['sample_indices'] is not None:\n",
    "        template_ids = [template_ids[i] for i in bert_data['sample_indices']]\n",
    "    \n",
    "    template_variants = {}\n",
    "    \n",
    "    template_onehot, unique_templates = create_template_features(\n",
    "        template_ids, templates, method='onehot'\n",
    "    )\n",
    "    template_variants['onehot'] = template_onehot\n",
    "    \n",
    "    template_freq, _ = create_template_features(\n",
    "        template_ids, templates, method='frequency'\n",
    "    )\n",
    "    template_variants['frequency'] = template_freq\n",
    "    \n",
    "    template_emb, _ = create_template_features(\n",
    "        template_ids, templates, method='embedding', embedding_dim=bert_dim\n",
    "    )\n",
    "    template_variants['embedding'] = template_emb\n",
    "    \n",
    "    print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "    print(f\"Template features - OneHot: {template_onehot.shape}, \"\n",
    "          f\"Frequency: {template_freq.shape}, Embedding: {template_emb.shape}\")\n",
    "    print(f\"Unique templates: {len(unique_templates)}\")\n",
    "    \n",
    "    hybrid_variants = {}\n",
    "    \n",
    "    for template_type, template_feats in template_variants.items():\n",
    "        hybrid_concat = combine_template_bert_features(\n",
    "            bert_embeddings, template_feats, method='concat'\n",
    "        )\n",
    "        hybrid_variants[f'bert_{template_type}_concat'] = hybrid_concat\n",
    "        \n",
    "        if template_type == 'embedding':\n",
    "            hybrid_weighted = combine_template_bert_features(\n",
    "                bert_embeddings, template_feats, method='weighted'\n",
    "            )\n",
    "            hybrid_variants[f'bert_{template_type}_weighted'] = hybrid_weighted\n",
    "    \n",
    "    hybrid_features_data[log_source] = {\n",
    "        'bert_only': bert_embeddings,\n",
    "        'template_variants': template_variants,\n",
    "        'hybrid_variants': hybrid_variants,\n",
    "        'labels': bert_data['labels'],\n",
    "        'unique_templates': unique_templates,\n",
    "        'template_ids': template_ids,\n",
    "        'texts': bert_data['texts']\n",
    "    }\n",
    "    \n",
    "    print(f\"Created {len(hybrid_variants)} hybrid feature variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e95e1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_features_path = FEATURES_PATH / \"hybrid_features.pkl\"\n",
    "with open(hybrid_features_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'hybrid_features_data': hybrid_features_data,\n",
    "        'feature_types': ['bert_only', 'template_onehot', 'template_frequency', \n",
    "                          'template_embedding', 'bert_onehot_concat', 'bert_frequency_concat',\n",
    "                          'bert_embedding_concat', 'bert_embedding_weighted'],\n",
    "        'creation_timestamp': datetime.now().isoformat()\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1907ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created hybrid features for 6 sources\n",
      "\n",
      "Apache:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 29.8%\n",
      "\n",
      "BGL:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 75.0%\n",
      "\n",
      "HPC:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 9.8%\n",
      "\n",
      "OpenSSH:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 78.8%\n",
      "\n",
      "Proxifier:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 4.9%\n",
      "\n",
      "Zookeeper:\n",
      "    BERT only: (2000, 768)\n",
      "    Template variants: 3\n",
      "    Hybrid variants: 4\n",
      "    Anomaly rate: 46.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Successfully created hybrid features for {len(hybrid_features_data)} sources\")\n",
    "\n",
    "for log_source, data in hybrid_features_data.items():\n",
    "    print(f\"\\n{log_source}:\")\n",
    "    print(f\"    BERT only: {data['bert_only'].shape}\")\n",
    "    print(f\"    Template variants: {len(data['template_variants'])}\")\n",
    "    print(f\"    Hybrid variants: {len(data['hybrid_variants'])}\")\n",
    "    if data['labels'] is not None:\n",
    "        anomaly_rate = np.mean(data['labels']) * 100\n",
    "        print(f\"    Anomaly rate: {anomaly_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d824c96",
   "metadata": {},
   "source": [
    "Cross-Source Feature Analysis and Leave-One-Out Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef01e1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source compatibility analysis:\n",
      "   Apache       | BERT: 768 | Templates:   6 | Samples:  2,000 | Labels: Yes\n",
      "   BGL          | BERT: 768 | Templates: 105 | Samples:  2,000 | Labels: Yes\n",
      "   HPC          | BERT: 768 | Templates:  45 | Samples:  2,000 | Labels: Yes\n",
      "   OpenSSH      | BERT: 768 | Templates:  23 | Samples:  2,000 | Labels: Yes\n",
      "   Proxifier    | BERT: 768 | Templates: 403 | Samples:  2,000 | Labels: Yes\n",
      "   Zookeeper    | BERT: 768 | Templates:  46 | Samples:  2,000 | Labels: Yes\n",
      "\n",
      "Compatibility check:\n",
      "   BERT dimensions consistent: True (all 768)\n",
      "   Template counts - Min: 6, Max: 403, Avg: 104.7\n",
      "   Sample counts - Min: 2,000, Max: 2,000, Total: 12,000\n"
     ]
    }
   ],
   "source": [
    "bert_dims = []\n",
    "template_counts = []\n",
    "sample_counts = []\n",
    "\n",
    "print(\"Source compatibility analysis:\")\n",
    "for source, data in hybrid_features_data.items():\n",
    "    bert_dim = data['bert_only'].shape[1]\n",
    "    n_templates = len(data['unique_templates'])\n",
    "    n_samples = data['bert_only'].shape[0]\n",
    "    has_labels = data['labels'] is not None\n",
    "    \n",
    "    bert_dims.append(bert_dim)\n",
    "    template_counts.append(n_templates)\n",
    "    sample_counts.append(n_samples)\n",
    "    \n",
    "    print(f\"   {source:<12} | BERT: {bert_dim} | Templates: {n_templates:>3} | Samples: {n_samples:>6,} | Labels: {'Yes' if has_labels else 'NA'}\")\n",
    "\n",
    "print(f\"\\nCompatibility check:\")\n",
    "print(f\"   BERT dimensions consistent: {len(set(bert_dims)) == 1} (all {bert_dims[0] if len(set(bert_dims)) == 1 else 'different'})\")\n",
    "print(f\"   Template counts - Min: {min(template_counts)}, Max: {max(template_counts)}, Avg: {np.mean(template_counts):.1f}\")\n",
    "print(f\"   Sample counts - Min: {min(sample_counts):,}, Max: {max(sample_counts):,}, Total: {sum(sample_counts):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a388653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leave_one_out_splits():\n",
    "    splits = []\n",
    "    \n",
    "    for test_source in hybrid_features_data.keys():\n",
    "        train_sources = [s for s in hybrid_features_data.keys() if s != test_source]\n",
    "        \n",
    "        splits.append({\n",
    "            'test_source': test_source,\n",
    "            'train_sources': train_sources,\n",
    "            'test_samples': hybrid_features_data[test_source]['bert_only'].shape[0],\n",
    "            'train_samples': sum(hybrid_features_data[s]['bert_only'].shape[0] for s in train_sources)\n",
    "        })\n",
    "    \n",
    "    return splits\n",
    "\n",
    "cross_source_splits = create_leave_one_out_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcf7729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 6\n",
      " 1. Test: Apache       | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n",
      " 2. Test: BGL          | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n",
      " 3. Test: HPC          | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n",
      " 4. Test: OpenSSH      | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n",
      " 5. Test: Proxifier    | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n",
      " 6. Test: Zookeeper    | Train: 5 sources | Test samples:  2,000 | Train samples:  10,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total experiments: {len(cross_source_splits)}\")\n",
    "\n",
    "for i, split in enumerate(cross_source_splits, 1):\n",
    "    print(f\"{i:2d}. Test: {split['test_source']:<12} | Train: {len(split['train_sources'])} sources | \"\n",
    "          f\"Test samples: {split['test_samples']:>6,} | Train samples: {split['train_samples']:>7,}\")\n",
    "\n",
    "cross_source_path = FEATURES_PATH / \"cross_source_splits.pkl\"\n",
    "with open(cross_source_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'splits': cross_source_splits,\n",
    "        'feature_compatibility': {\n",
    "            'bert_dims': bert_dims,\n",
    "            'template_counts': template_counts,\n",
    "            'sample_counts': sample_counts\n",
    "        }\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
