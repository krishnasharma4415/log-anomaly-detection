{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21da223",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import drain3\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.masking import MaskingInstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac1a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "NORMALIZED_DATA_PATH = LABELED_DATA_PATH / \"normalized\"\n",
    "FEATURES_PATH = PROJECT_ROOT / \"features\"\n",
    "FEATURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .appName(\"MultiClassFeatureEngineering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa3e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 log sources\n",
      "Label mapping: {0: 'normal', 1: 'security_anomaly', 2: 'system_failure', 3: 'performance_issue', 4: 'network_anomaly', 5: 'config_error', 6: 'hardware_issue'}\n"
     ]
    }
   ],
   "source": [
    "PROJECT_CONFIG = {\n",
    "    'bert_model_name': 'bert-base-uncased',\n",
    "    'max_sequence_length': 512,\n",
    "    'num_classes': 7,\n",
    "    'label_map': {\n",
    "        0: 'normal',\n",
    "        1: 'security_anomaly',\n",
    "        2: 'system_failure',\n",
    "        3: 'performance_issue',\n",
    "        4: 'network_anomaly',\n",
    "        5: 'config_error',\n",
    "        6: 'hardware_issue'\n",
    "    },\n",
    "    'log_sources': []\n",
    "}\n",
    "\n",
    "dataset_registry = {}\n",
    "\n",
    "enhanced_files = list(NORMALIZED_DATA_PATH.glob(\"*_enhanced.csv\"))\n",
    "for file_path in enhanced_files:\n",
    "    source_name = file_path.stem.replace('_enhanced', '')\n",
    "    PROJECT_CONFIG['log_sources'].append(source_name)\n",
    "    dataset_registry[source_name] = {\n",
    "        'file_path': str(file_path),\n",
    "        'log_type': source_name\n",
    "    }\n",
    "\n",
    "print(f\"Loaded {len(dataset_registry)} log sources\")\n",
    "print(f\"Label mapping: {PROJECT_CONFIG['label_map']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5824bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model = AutoModel.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "drain_configs = {\n",
    "    'hdfs': {'sim_th': 0.5, 'depth': 4},\n",
    "    'bgl': {'sim_th': 0.3, 'depth': 5},\n",
    "    'hadoop': {'sim_th': 0.4, 'depth': 4},\n",
    "    'apache': {'sim_th': 0.4, 'depth': 4},\n",
    "    'default': {'sim_th': 0.4, 'depth': 4}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e69c19",
   "metadata": {},
   "source": [
    "Temporal and Statistical Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be303a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Apache_2k\n",
      "Loaded 2000 rows with 21 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,405 (70.25%)\n",
      "  1 (security_anomaly): 32 (1.60%)\n",
      "  2 (system_failure): 563 (28.15%)\n",
      "Converted to Pandas: (2000, 31)\n",
      "\n",
      "\n",
      "Processing: BGL_2k\n",
      "Loaded 2000 rows with 28 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 501 (25.05%)\n",
      "  1 (security_anomaly): 150 (7.50%)\n",
      "  2 (system_failure): 913 (45.65%)\n",
      "  5 (config_error): 73 (3.65%)\n",
      "  6 (hardware_issue): 363 (18.15%)\n",
      "Converted to Pandas: (2000, 37)\n",
      "\n",
      "\n",
      "Processing: Hadoop_2k\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,217 (60.85%)\n",
      "  2 (system_failure): 156 (7.80%)\n",
      "  3 (performance_issue): 1 (0.05%)\n",
      "  4 (network_anomaly): 625 (31.25%)\n",
      "  5 (config_error): 1 (0.05%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "\n",
      "\n",
      "Processing: HDFS_2k\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,920 (96.00%)\n",
      "  2 (system_failure): 80 (4.00%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "\n",
      "\n",
      "Processing: HPC_2k\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,804 (90.20%)\n",
      "  2 (system_failure): 60 (3.00%)\n",
      "  3 (performance_issue): 29 (1.45%)\n",
      "  4 (network_anomaly): 85 (4.25%)\n",
      "  5 (config_error): 4 (0.20%)\n",
      "  6 (hardware_issue): 18 (0.90%)\n",
      "Converted to Pandas: (2000, 35)\n",
      "\n",
      "\n",
      "Processing: Linux_2k\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,359 (67.95%)\n",
      "  1 (security_anomaly): 527 (26.35%)\n",
      "  2 (system_failure): 46 (2.30%)\n",
      "  3 (performance_issue): 15 (0.75%)\n",
      "  4 (network_anomaly): 34 (1.70%)\n",
      "  5 (config_error): 5 (0.25%)\n",
      "  6 (hardware_issue): 14 (0.70%)\n",
      "Converted to Pandas: (2000, 36)\n",
      "\n",
      "\n",
      "Processing: OpenSSH_2k\n",
      "Loaded 2000 rows with 24 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 424 (21.20%)\n",
      "  1 (security_anomaly): 1,569 (78.45%)\n",
      "  5 (config_error): 7 (0.35%)\n",
      "Converted to Pandas: (2000, 34)\n",
      "\n",
      "\n",
      "Processing: Proxifier_2k\n",
      "Loaded 2000 rows with 21 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,903 (95.15%)\n",
      "  4 (network_anomaly): 97 (4.85%)\n",
      "Converted to Pandas: (2000, 31)\n",
      "\n",
      "\n",
      "Processing: Zookeeper_2k\n",
      "Loaded 2000 rows with 25 columns\n",
      "Extracting temporal features\n",
      "Calculating statistical features\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,075 (53.75%)\n",
      "  2 (system_failure): 4 (0.20%)\n",
      "  3 (performance_issue): 37 (1.85%)\n",
      "  4 (network_anomaly): 873 (43.65%)\n",
      "  5 (config_error): 6 (0.30%)\n",
      "  6 (hardware_issue): 5 (0.25%)\n",
      "Converted to Pandas: (2000, 35)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_features_data = {}\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in dataset_registry:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing: {log_source}\")\n",
    "    \n",
    "    file_path = dataset_registry[log_source]['file_path']\n",
    "    \n",
    "    df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(f\"Loaded {df_spark.count()} rows with {len(df_spark.columns)} columns\")\n",
    "    \n",
    "    content_col = None\n",
    "    for col in ['Content', 'content', 'Message', 'message', 'Text', 'text']:\n",
    "        if col in df_spark.columns:\n",
    "            content_col = col\n",
    "            break\n",
    "    \n",
    "    if content_col is None:\n",
    "        print(f\"No content column found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    if 'timestamp_dt' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.col('timestamp_dt').cast(TimestampType()))\n",
    "    elif 'timestamp_normalized' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.to_timestamp('timestamp_normalized'))\n",
    "    else:\n",
    "        print(\"No timestamp column found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Extracting temporal features\")\n",
    "    \n",
    "    df_spark = df_spark.withColumn('hour', F.hour('timestamp')) \\\n",
    "                       .withColumn('day_of_week', F.dayofweek('timestamp')) \\\n",
    "                       .withColumn('day_of_month', F.dayofmonth('timestamp')) \\\n",
    "                       .withColumn('month', F.month('timestamp')) \\\n",
    "                       .withColumn('is_weekend', F.when(F.dayofweek('timestamp').isin([1, 7]), 1).otherwise(0)) \\\n",
    "                       .withColumn('is_business_hours', F.when(F.hour('timestamp').between(9, 17), 1).otherwise(0)) \\\n",
    "                       .withColumn('is_night', F.when(F.hour('timestamp').between(0, 6), 1).otherwise(0))\n",
    "    \n",
    "    window_spec = Window.orderBy('timestamp')\n",
    "    window_spec_1min = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-60, 0)\n",
    "    \n",
    "    df_spark = df_spark.withColumn('log_index', F.monotonically_increasing_id())\n",
    "    \n",
    "    df_spark = df_spark.withColumn('prev_timestamp', F.lag('timestamp', 1).over(window_spec))\n",
    "    df_spark = df_spark.withColumn('time_diff_seconds', \n",
    "                                    F.when(F.col('prev_timestamp').isNotNull(), \n",
    "                                           F.unix_timestamp('timestamp') - F.unix_timestamp('prev_timestamp'))\n",
    "                                    .otherwise(0))\n",
    "    \n",
    "    df_spark = df_spark.withColumn('logs_last_minute', \n",
    "                                    F.count('*').over(window_spec_1min))\n",
    "    \n",
    "    print(\"Calculating statistical features\")\n",
    "    \n",
    "    df_spark = df_spark.withColumn('content_length', F.length(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('word_count', F.size(F.split(F.col(content_col), ' ')))\n",
    "    \n",
    "    window_10 = Window.orderBy('timestamp').rowsBetween(-9, 0)\n",
    "    \n",
    "    df_spark = df_spark.withColumn('content_length_mean_10', F.avg('content_length').over(window_10))\n",
    "    df_spark = df_spark.withColumn('content_length_std_10', F.stddev('content_length').over(window_10))\n",
    "    df_spark = df_spark.withColumn('time_diff_mean_10', F.avg('time_diff_seconds').over(window_10))\n",
    "    df_spark = df_spark.withColumn('time_diff_std_10', F.stddev('time_diff_seconds').over(window_10))\n",
    "    \n",
    "    hour_counts = df_spark.groupBy('hour').count().withColumnRenamed('count', 'hour_frequency')\n",
    "    df_spark = df_spark.join(hour_counts, on='hour', how='left')\n",
    "    \n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', F.col('AnomalyLabel').cast(IntegerType()))\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', \n",
    "                                        F.when(F.col('AnomalyLabel').isNull(), 0)\n",
    "                                        .when(F.col('AnomalyLabel') < 0, 0)\n",
    "                                        .when(F.col('AnomalyLabel') > 6, 0)\n",
    "                                        .otherwise(F.col('AnomalyLabel')))\n",
    "    \n",
    "    df_spark.cache()\n",
    "    \n",
    "    total_count = df_spark.count()\n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        label_dist = df_spark.groupBy('AnomalyLabel').count().orderBy('AnomalyLabel').collect()\n",
    "        print(f\"Total: {total_count:,}\")\n",
    "        print(\"Label distribution:\")\n",
    "        for row in label_dist:\n",
    "            lbl = row['AnomalyLabel']\n",
    "            cnt = row['count']\n",
    "            lbl_name = PROJECT_CONFIG['label_map'].get(lbl, 'unknown')\n",
    "            print(f\"  {lbl} ({lbl_name}): {cnt:,} ({cnt/total_count*100:.2f}%)\")\n",
    "    \n",
    "    max_samples = 5000\n",
    "    if total_count > max_samples:\n",
    "        sample_fraction = max_samples / total_count\n",
    "        df_spark_sampled = df_spark.sample(withReplacement=False, fraction=sample_fraction, seed=RANDOM_SEED)\n",
    "        print(f\"Sampled {df_spark_sampled.count()} rows\")\n",
    "    else:\n",
    "        df_spark_sampled = df_spark\n",
    "    \n",
    "    df_pandas = df_spark_sampled.toPandas()\n",
    "    \n",
    "    print(f\"Converted to Pandas: {df_pandas.shape}\")\n",
    "    print()\n",
    "    \n",
    "    pyspark_features_data[log_source] = {\n",
    "        'spark_df': df_spark,\n",
    "        'pandas_df': df_pandas,\n",
    "        'content_col': content_col,\n",
    "        'total_count': total_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d375b7",
   "metadata": {},
   "source": [
    "Drain Template Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5411c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting templates for Apache_2k\n",
      "Extracted 6 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for BGL_2k\n",
      "Extracted 105 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for Hadoop_2k\n",
      "Extracted 102 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for HDFS_2k\n",
      "Extracted 16 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for HPC_2k\n",
      "Extracted 45 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for Linux_2k\n",
      "Extracted 110 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for OpenSSH_2k\n",
      "Extracted 23 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for Proxifier_2k\n",
      "Extracted 403 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n",
      "\n",
      "Extracting templates for Zookeeper_2k\n",
      "Extracted 46 unique templates\n",
      "Enhanced template features shape: (2000, 11)\n"
     ]
    }
   ],
   "source": [
    "template_data = {}\n",
    "\n",
    "for log_source, data_dict in pyspark_features_data.items():\n",
    "    print(f\"\\nExtracting templates for {log_source}\")\n",
    "    \n",
    "    df_pandas = data_dict['pandas_df']\n",
    "    content_col = data_dict['content_col']\n",
    "    \n",
    "    source_config = drain_configs.get(log_source, drain_configs['default'])\n",
    "    \n",
    "    drain_config = TemplateMinerConfig()\n",
    "    drain_config.drain_sim_th = source_config['sim_th']\n",
    "    drain_config.drain_depth = source_config['depth']\n",
    "    drain_config.drain_max_children = 100\n",
    "    drain_config.masking_instructions = [\n",
    "        MaskingInstruction(r'\\d+', \"<NUM>\"),\n",
    "        MaskingInstruction(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', \"<UUID>\"),\n",
    "        MaskingInstruction(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', \"<IP>\"),\n",
    "        MaskingInstruction(r'/[^\\s]*', \"<PATH>\")\n",
    "    ]\n",
    "    \n",
    "    template_miner = TemplateMiner(config=drain_config)\n",
    "    \n",
    "    templates = {}\n",
    "    template_ids = []\n",
    "    template_class_dist = {}\n",
    "    \n",
    "    labels = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    \n",
    "    for idx, content in enumerate(df_pandas[content_col].fillna(\"\").astype(str)):\n",
    "        if content.strip() == \"\":\n",
    "            template_ids.append(-1)\n",
    "            continue\n",
    "        \n",
    "        result = template_miner.add_log_message(content.strip())\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        \n",
    "        if tid not in templates:\n",
    "            templates[tid] = {\n",
    "                'template': result[\"template_mined\"],\n",
    "                'count': 1,\n",
    "                'class_dist': [0] * 7\n",
    "            }\n",
    "        else:\n",
    "            templates[tid]['count'] += 1\n",
    "        \n",
    "        if labels is not None:\n",
    "            lbl = int(labels[idx])\n",
    "            templates[tid]['class_dist'][lbl] += 1\n",
    "    \n",
    "    print(f\"Extracted {len(templates)} unique templates\")\n",
    "    \n",
    "    template_counts = Counter(template_ids)\n",
    "    total = len(template_ids)\n",
    "    \n",
    "    enhanced_template_features = []\n",
    "    for idx, tid in enumerate(template_ids):\n",
    "        if tid == -1:\n",
    "            enhanced_template_features.append([0, 0, 0, 0] + [0]*7)\n",
    "            continue\n",
    "        \n",
    "        frequency = template_counts[tid] / total\n",
    "        rarity = 1.0 / (frequency + 1e-6)\n",
    "        template_text = templates[tid]['template']\n",
    "        length = len(template_text.split())\n",
    "        n_wildcards = sum([template_text.count(w) for w in ['<NUM>', '<IP>', '<PATH>', '<UUID>']])\n",
    "        \n",
    "        class_probs = np.array(templates[tid]['class_dist']) / (templates[tid]['count'] + 1e-6)\n",
    "        \n",
    "        enhanced_template_features.append([rarity, length, n_wildcards, frequency] + class_probs.tolist())\n",
    "    \n",
    "    template_data[log_source] = {\n",
    "        'templates': templates,\n",
    "        'template_ids': template_ids,\n",
    "        'enhanced_features': np.array(enhanced_template_features)\n",
    "    }\n",
    "    \n",
    "    print(f\"Enhanced template features shape: {template_data[log_source]['enhanced_features'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e6e79",
   "metadata": {},
   "source": [
    "Bert Embeddings and Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60084bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating BERT embeddings for Apache_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for BGL_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for Hadoop_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for HDFS_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for HPC_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for Linux_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for OpenSSH_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for Proxifier_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n",
      "\n",
      "--- Generating BERT embeddings for Zookeeper_2k ---\n",
      "Processing 2000 texts...\n",
      "  Processed 0/2000 texts\n",
      "  Processed 160/2000 texts\n",
      "  Processed 320/2000 texts\n",
      "  Processed 480/2000 texts\n",
      "  Processed 640/2000 texts\n",
      "  Processed 800/2000 texts\n",
      "  Processed 960/2000 texts\n",
      "  Processed 1120/2000 texts\n",
      "  Processed 1280/2000 texts\n",
      "  Processed 1440/2000 texts\n",
      "  Processed 1600/2000 texts\n",
      "  Processed 1760/2000 texts\n",
      "  Processed 1920/2000 texts\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Statistical features shape: (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "bert_features_data = {}\n",
    "\n",
    "for log_source, data_dict in pyspark_features_data.items():\n",
    "    print(f\"\\n--- Generating BERT embeddings for {log_source} ---\")\n",
    "    \n",
    "    df_pandas = data_dict['pandas_df']\n",
    "    content_col = data_dict['content_col']\n",
    "    \n",
    "    texts = df_pandas[content_col].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts...\")\n",
    "    all_embeddings = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = bert_model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f\"  Processed {i}/{len(texts)} texts\")\n",
    "    \n",
    "    bert_embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "    \n",
    "    window_size = 10\n",
    "    statistical_features = []\n",
    "    \n",
    "    for i in range(len(bert_embeddings)):\n",
    "        start = max(0, i - window_size)\n",
    "        window = bert_embeddings[start:i+1]\n",
    "        \n",
    "        mean_emb = np.mean(window, axis=0)\n",
    "        std_emb = np.std(window, axis=0)\n",
    "        distance_from_mean = np.linalg.norm(bert_embeddings[i] - mean_emb)\n",
    "        avg_std = np.mean(std_emb)\n",
    "        \n",
    "        if len(window) > 1:\n",
    "            distances = [np.linalg.norm(bert_embeddings[i] - w) for w in window]\n",
    "            min_dist = np.min(distances)\n",
    "            max_dist = np.max(distances)\n",
    "        else:\n",
    "            min_dist = max_dist = 0\n",
    "        \n",
    "        statistical_features.append([distance_from_mean, avg_std, min_dist, max_dist])\n",
    "    \n",
    "    statistical_features = np.array(statistical_features)\n",
    "    print(f\"Statistical features shape: {statistical_features.shape}\")\n",
    "    \n",
    "    bert_features_data[log_source] = {\n",
    "        'embeddings': bert_embeddings,\n",
    "        'statistical_features': statistical_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e4424",
   "metadata": {},
   "source": [
    "Hybrid Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a438a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Assembling features for Apache_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 6)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1405 (70.25%)\n",
      "  1 (security_anomaly): 32 (1.60%)\n",
      "  2 (system_failure): 563 (28.15%)\n",
      "\n",
      "--- Assembling features for BGL_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 105)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 501 (25.05%)\n",
      "  1 (security_anomaly): 150 (7.50%)\n",
      "  2 (system_failure): 913 (45.65%)\n",
      "  5 (config_error): 73 (3.65%)\n",
      "  6 (hardware_issue): 363 (18.15%)\n",
      "\n",
      "--- Assembling features for Hadoop_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 102)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1217 (60.85%)\n",
      "  2 (system_failure): 156 (7.80%)\n",
      "  3 (performance_issue): 1 (0.05%)\n",
      "  4 (network_anomaly): 625 (31.25%)\n",
      "  5 (config_error): 1 (0.05%)\n",
      "\n",
      "--- Assembling features for HDFS_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 16)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1920 (96.00%)\n",
      "  2 (system_failure): 80 (4.00%)\n",
      "\n",
      "--- Assembling features for HPC_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 45)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1804 (90.20%)\n",
      "  2 (system_failure): 60 (3.00%)\n",
      "  3 (performance_issue): 29 (1.45%)\n",
      "  4 (network_anomaly): 85 (4.25%)\n",
      "  5 (config_error): 4 (0.20%)\n",
      "  6 (hardware_issue): 18 (0.90%)\n",
      "\n",
      "--- Assembling features for Linux_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 110)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1359 (67.95%)\n",
      "  1 (security_anomaly): 527 (26.35%)\n",
      "  2 (system_failure): 46 (2.30%)\n",
      "  3 (performance_issue): 15 (0.75%)\n",
      "  4 (network_anomaly): 34 (1.70%)\n",
      "  5 (config_error): 5 (0.25%)\n",
      "  6 (hardware_issue): 14 (0.70%)\n",
      "\n",
      "--- Assembling features for OpenSSH_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 23)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 424 (21.20%)\n",
      "  1 (security_anomaly): 1569 (78.45%)\n",
      "  5 (config_error): 7 (0.35%)\n",
      "\n",
      "--- Assembling features for Proxifier_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 403)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1903 (95.15%)\n",
      "  4 (network_anomaly): 97 (4.85%)\n",
      "\n",
      "--- Assembling features for Zookeeper_2k ---\n",
      "Feature dimensions:\n",
      "  BERT embeddings: (2000, 768)\n",
      "  BERT statistical: (2000, 4)\n",
      "  Template enhanced: (2000, 11)\n",
      "  Template one-hot: (2000, 46)\n",
      "  Temporal: (2000, 7)\n",
      "  PySpark statistical: (2000, 7)\n",
      "Created 6 feature variants\n",
      "Label distribution:\n",
      "  0 (normal): 1075 (53.75%)\n",
      "  2 (system_failure): 4 (0.20%)\n",
      "  3 (performance_issue): 37 (1.85%)\n",
      "  4 (network_anomaly): 873 (43.65%)\n",
      "  5 (config_error): 6 (0.30%)\n",
      "  6 (hardware_issue): 5 (0.25%)\n"
     ]
    }
   ],
   "source": [
    "hybrid_features_data = {}\n",
    "\n",
    "for log_source in pyspark_features_data.keys():\n",
    "    if log_source not in bert_features_data or log_source not in template_data:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n--- Assembling features for {log_source} ---\")\n",
    "    \n",
    "    df_pandas = pyspark_features_data[log_source]['pandas_df']\n",
    "    bert_emb = bert_features_data[log_source]['embeddings']\n",
    "    stat_features = bert_features_data[log_source]['statistical_features']\n",
    "    template_features = template_data[log_source]['enhanced_features']\n",
    "    \n",
    "    temporal_cols = ['hour', 'day_of_week', 'is_weekend', 'is_business_hours', \n",
    "                     'time_diff_seconds', 'logs_last_minute', 'is_night']\n",
    "    \n",
    "    statistical_cols = ['content_length', 'word_count', 'content_length_mean_10', \n",
    "                       'content_length_std_10', 'time_diff_mean_10', 'time_diff_std_10',\n",
    "                       'hour_frequency']\n",
    "    \n",
    "    available_temporal = [c for c in temporal_cols if c in df_pandas.columns]\n",
    "    available_statistical = [c for c in statistical_cols if c in df_pandas.columns]\n",
    "    \n",
    "    temporal_features = df_pandas[available_temporal].fillna(0).values if available_temporal else None\n",
    "    pyspark_statistical = df_pandas[available_statistical].fillna(0).values if available_statistical else None\n",
    "    \n",
    "    unique_templates = sorted(set(tid for tid in template_data[log_source]['template_ids'] if tid != -1))\n",
    "    template_onehot = np.zeros((len(template_data[log_source]['template_ids']), len(unique_templates)))\n",
    "    \n",
    "    for i, tid in enumerate(template_data[log_source]['template_ids']):\n",
    "        if tid != -1 and tid in unique_templates:\n",
    "            idx = unique_templates.index(tid)\n",
    "            template_onehot[i, idx] = 1.0\n",
    "    \n",
    "    print(f\"Feature dimensions:\")\n",
    "    print(f\"  BERT embeddings: {bert_emb.shape}\")\n",
    "    print(f\"  BERT statistical: {stat_features.shape}\")\n",
    "    print(f\"  Template enhanced: {template_features.shape}\")\n",
    "    print(f\"  Template one-hot: {template_onehot.shape}\")\n",
    "    if temporal_features is not None:\n",
    "        print(f\"  Temporal: {temporal_features.shape}\")\n",
    "    if pyspark_statistical is not None:\n",
    "        print(f\"  PySpark statistical: {pyspark_statistical.shape}\")\n",
    "    \n",
    "    feature_variants = {}\n",
    "    \n",
    "    feature_variants['bert_only'] = bert_emb\n",
    "    \n",
    "    feature_variants['bert_statistical'] = np.hstack([bert_emb, stat_features])\n",
    "    \n",
    "    feature_variants['bert_template_enhanced'] = np.hstack([bert_emb, template_features])\n",
    "    \n",
    "    feature_variants['bert_statistical_template'] = np.hstack([bert_emb, stat_features, template_features])\n",
    "    \n",
    "    if temporal_features is not None:\n",
    "        feature_variants['bert_statistical_template_temporal'] = np.hstack([\n",
    "            bert_emb, stat_features, template_features, temporal_features\n",
    "        ])\n",
    "    \n",
    "    all_feature_components = [bert_emb, stat_features, template_features]\n",
    "    if temporal_features is not None:\n",
    "        all_feature_components.append(temporal_features)\n",
    "    if pyspark_statistical is not None:\n",
    "        all_feature_components.append(pyspark_statistical)\n",
    "    \n",
    "    feature_variants['all_features'] = np.hstack(all_feature_components)\n",
    "    \n",
    "    labels = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    \n",
    "    hybrid_features_data[log_source] = {\n",
    "        'feature_variants': feature_variants,\n",
    "        'labels': labels,\n",
    "        'texts': df_pandas[pyspark_features_data[log_source]['content_col']].tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"Created {len(feature_variants)} feature variants\")\n",
    "    if labels is not None:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"Label distribution:\")\n",
    "        for lbl, cnt in zip(unique, counts):\n",
    "            lbl_name = PROJECT_CONFIG['label_map'].get(int(lbl), 'unknown')\n",
    "            print(f\"  {int(lbl)} ({lbl_name}): {cnt} ({cnt/len(labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd2de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\multiclass_hybrid_features.pkl\n"
     ]
    }
   ],
   "source": [
    "features_save_path = FEATURES_PATH / \"multiclass_hybrid_features.pkl\"\n",
    "with open(features_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'hybrid_features_data': hybrid_features_data,\n",
    "        'template_data': template_data,\n",
    "        'feature_types': list(hybrid_features_data[list(hybrid_features_data.keys())[0]]['feature_variants'].keys()),\n",
    "        'config': PROJECT_CONFIG,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nSaved: {features_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588df34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\multiclass_cross_source_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "cross_source_splits = []\n",
    "\n",
    "for test_source in hybrid_features_data.keys():\n",
    "    train_sources = [s for s in hybrid_features_data.keys() if s != test_source]\n",
    "    \n",
    "    if hybrid_features_data[test_source]['labels'] is None:\n",
    "        continue\n",
    "    \n",
    "    test_samples = len(hybrid_features_data[test_source]['labels'])\n",
    "    train_samples = sum(len(hybrid_features_data[s]['labels']) \n",
    "                       for s in train_sources \n",
    "                       if hybrid_features_data[s]['labels'] is not None)\n",
    "    \n",
    "    cross_source_splits.append({\n",
    "        'test_source': test_source,\n",
    "        'train_sources': train_sources,\n",
    "        'test_samples': test_samples,\n",
    "        'train_samples': train_samples\n",
    "    })\n",
    "\n",
    "splits_save_path = FEATURES_PATH / \"multiclass_cross_source_splits.pkl\"\n",
    "with open(splits_save_path, 'wb') as f:\n",
    "    pickle.dump({'splits': cross_source_splits}, f)\n",
    "\n",
    "print(f\"Saved: {splits_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c438dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - bert_only: 768 features\n",
      "  - bert_statistical: 772 features\n",
      "  - bert_template_enhanced: 779 features\n",
      "  - bert_statistical_template: 783 features\n",
      "  - bert_statistical_template_temporal: 790 features\n",
      "  - all_features: 797 features\n"
     ]
    }
   ],
   "source": [
    "for ft in list(hybrid_features_data[list(hybrid_features_data.keys())[0]]['feature_variants'].keys()):\n",
    "    shape = hybrid_features_data[list(hybrid_features_data.keys())[0]]['feature_variants'][ft].shape\n",
    "    print(f\"  - {ft}: {shape[1]} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
