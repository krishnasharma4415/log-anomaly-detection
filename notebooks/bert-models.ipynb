{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059d921d",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3469fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd91d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded 6 sources\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "RES_PATH = ROOT / \"results\" / \"cross_source_transfer\" / \"advanced\"\n",
    "RES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "with open(FEAT_PATH / \"hybrid_features.pkl\", 'rb') as f:\n",
    "    feat_data = pickle.load(f)['hybrid_features_data']\n",
    "\n",
    "with open(FEAT_PATH / \"cross_source_splits.pkl\", 'rb') as f:\n",
    "    splits = pickle.load(f)['splits']\n",
    "\n",
    "cfg = {\n",
    "    'hidden': 768,\n",
    "    'batch': 32,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 15,\n",
    "    'dropout': 0.3,\n",
    "    'patience': 3\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(feat_data)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50198f3",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b225b319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Apache (2000 samples)\n",
      "Train: 5 sources (10000 samples)\n"
     ]
    }
   ],
   "source": [
    "exp = splits[0]\n",
    "test_src = exp['test_source']\n",
    "train_srcs = exp['train_sources']\n",
    "\n",
    "test_X = feat_data[test_src]['bert_only']\n",
    "test_y = feat_data[test_src]['labels']\n",
    "\n",
    "train_X_list = []\n",
    "train_y_list = []\n",
    "\n",
    "for src in train_srcs:\n",
    "    if feat_data[src]['labels'] is not None:\n",
    "        train_X_list.append(feat_data[src]['bert_only'])\n",
    "        train_y_list.append(feat_data[src]['labels'])\n",
    "\n",
    "train_X = np.vstack(train_X_list)\n",
    "train_y = np.concatenate(train_y_list)\n",
    "\n",
    "print(f\"Test: {test_src} ({len(test_y)} samples)\")\n",
    "print(f\"Train: {len(train_srcs)} sources ({len(train_y)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f34e79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train class distribution: Normal=5707, Anomaly=4293\n",
      "Test class distribution: Normal=1405, Anomaly=595\n",
      "Train anomaly rate: 0.429\n",
      "Test anomaly rate: 0.297\n"
     ]
    }
   ],
   "source": [
    "train_counts = np.bincount(train_y)\n",
    "test_counts = np.bincount(test_y)\n",
    "print(f\"\\nTrain class distribution: Normal={train_counts[0]}, Anomaly={train_counts[1]}\")\n",
    "print(f\"Test class distribution: Normal={test_counts[0]}, Anomaly={test_counts[1]}\")\n",
    "print(f\"Train anomaly rate: {np.mean(train_y):.3f}\")\n",
    "print(f\"Test anomaly rate: {np.mean(test_y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e757b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.87611705 1.1646867 ]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2aebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_t = torch.FloatTensor(train_X_scaled)\n",
    "train_y_t = torch.LongTensor(train_y)\n",
    "test_X_t = torch.FloatTensor(test_X_scaled)\n",
    "test_y_t = torch.LongTensor(test_y)\n",
    "\n",
    "train_ds = TensorDataset(train_X_t, train_y_t)\n",
    "test_ds = TensorDataset(test_X_t, test_y_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=cfg['batch'], shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=cfg['batch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ce729",
   "metadata": {},
   "source": [
    "Bert Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ec5e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCls(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden'], 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BertCls().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=cfg['lr'])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3073e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Loss=0.0817 | Val F1=0.818 | Val MCC=0.750\n",
      "Epoch  2: Loss=0.0296 | Val F1=0.661 | Val MCC=0.518\n",
      "Epoch  3: Loss=0.0171 | Val F1=0.816 | Val MCC=0.747\n",
      "Epoch  4: Loss=0.0161 | Val F1=0.555 | Val MCC=0.350\n",
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertCls(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Dropout(p=0.3, inplace=False)\n",
       "    (11): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_dl:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_probs = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dl:\n",
    "            X_batch = X_batch.to(device)\n",
    "            out = model(X_batch)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            \n",
    "            val_preds.extend(pred.cpu().numpy())\n",
    "            val_probs.extend(prob[:, 1].cpu().numpy())\n",
    "            val_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "    val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f} | Val F1={val_f1:.3f} | Val MCC={val_mcc:.3f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), RES_PATH / 'best_bert_cls.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= cfg['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(RES_PATH / 'best_bert_cls.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91e7b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier Final Results:\n",
      "F1: 0.818 | PR-AUC: 0.999 | MCC: 0.750 | ROC-AUC: 1.000\n",
      "Accuracy: 0.868 | Balanced Accuracy: 0.906\n",
      "Sensitivity: 1.000 | Specificity: 0.812\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "probs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        out = model(X_batch)\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        \n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        probs.extend(prob[:, 1].cpu().numpy())\n",
    "        labels.extend(y_batch.numpy())\n",
    "\n",
    "y_true = np.array(labels)\n",
    "y_pred = np.array(preds)\n",
    "y_prob = np.array(probs)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "bal_acc = (sens + spec) / 2\n",
    "\n",
    "try:\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    p, r, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(r, p)\n",
    "except:\n",
    "    roc = 0\n",
    "    pr_auc = 0\n",
    "\n",
    "print(f\"BERT Classifier Final Results:\")\n",
    "print(f\"F1: {f1:.3f} | PR-AUC: {pr_auc:.3f} | MCC: {mcc:.3f} | ROC-AUC: {roc:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f} | Balanced Accuracy: {bal_acc:.3f}\")\n",
    "print(f\"Sensitivity: {sens:.3f} | Specificity: {spec:.3f}\")\n",
    "\n",
    "bert_results = {\n",
    "    'f1': f1, 'pr_auc': pr_auc, 'mcc': mcc, 'roc': roc,\n",
    "    'acc': acc, 'bal_acc': bal_acc, 'sens': sens, 'spec': spec,\n",
    "    'preds': y_pred, 'probs': y_prob\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dec408",
   "metadata": {},
   "source": [
    "DANN-BERT (Domain Adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db38d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.neg() * ctx.alpha, None\n",
    "\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, n_domains):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden'], 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Anomaly classifier\n",
    "        self.anomaly = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Domain classifier\n",
    "        self.domain = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, n_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        feat = self.feat(x)\n",
    "        anomaly_out = self.anomaly(feat)\n",
    "        \n",
    "        # Reverse gradient for domain classifier\n",
    "        reversed_feat = GradReverse.apply(feat, alpha)\n",
    "        domain_out = self.domain(reversed_feat)\n",
    "        \n",
    "        return anomaly_out, domain_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4e6ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded 6 sources\n",
      "\n",
      "Test: Apache (2000 samples)\n",
      "Train: 5 sources (10000 samples)\n",
      "After split: Train=8000, Val=2000, Test=2000\n",
      "\n",
      "Train class distribution: Normal=4566, Anomaly=3434\n",
      "Val class distribution: Normal=1141, Anomaly=859\n",
      "Test class distribution: Normal=1405, Anomaly=595\n",
      "Train anomaly rate: 0.429\n",
      "Val anomaly rate: 0.429\n",
      "Test anomaly rate: 0.297\n",
      "\n",
      "Class weights: [0.8760403 1.1648223]\n",
      "\n",
      "================================================================================\n",
      "TRAINING BERT CLASSIFIER\n",
      "================================================================================\n",
      "Epoch  1: Loss=0.1437 | Val F1=0.988 | Val MCC=0.980\n",
      "Epoch  2: Loss=0.0299 | Val F1=0.988 | Val MCC=0.980\n",
      "Epoch  3: Loss=0.0186 | Val F1=0.988 | Val MCC=0.980\n",
      "Epoch  4: Loss=0.0229 | Val F1=0.991 | Val MCC=0.985\n",
      "Epoch  5: Loss=0.0169 | Val F1=0.994 | Val MCC=0.989\n",
      "Epoch  6: Loss=0.0150 | Val F1=0.994 | Val MCC=0.989\n",
      "Epoch  7: Loss=0.0097 | Val F1=0.995 | Val MCC=0.992\n",
      "Epoch  8: Loss=0.0086 | Val F1=0.995 | Val MCC=0.991\n",
      "Epoch  9: Loss=0.0079 | Val F1=0.992 | Val MCC=0.987\n",
      "Epoch 10: Loss=0.0092 | Val F1=0.996 | Val MCC=0.993\n",
      "Epoch 11: Loss=0.0098 | Val F1=0.995 | Val MCC=0.991\n",
      "Epoch 12: Loss=0.0083 | Val F1=0.995 | Val MCC=0.991\n",
      "Epoch 13: Loss=0.0118 | Val F1=0.995 | Val MCC=0.992\n",
      "Epoch 14: Loss=0.0048 | Val F1=0.994 | Val MCC=0.990\n",
      "Epoch 15: Loss=0.0032 | Val F1=0.995 | Val MCC=0.991\n",
      "Early stopping at epoch 15\n",
      "\n",
      "BERT Classifier Final Results:\n",
      "F1: 0.826 | PR-AUC: 0.983 | MCC: 0.756 | ROC-AUC: 0.988\n",
      "Accuracy: 0.877 | Balanced Accuracy: 0.906\n",
      "Sensitivity: 0.976 | Specificity: 0.836\n",
      "\n",
      "================================================================================\n",
      "TRAINING DANN-BERT\n",
      "================================================================================\n",
      "Epoch  1: Anom=0.1323 | Dom=1.4512 | Alpha=0.000 | Val F1=0.990 | Val MCC=0.982\n",
      "Epoch  2: Anom=0.0344 | Dom=1.4043 | Alpha=0.245 | Val F1=0.993 | Val MCC=0.989\n",
      "Epoch  3: Anom=0.0191 | Dom=1.4014 | Alpha=0.462 | Val F1=0.994 | Val MCC=0.990\n",
      "Epoch  4: Anom=0.0162 | Dom=1.3969 | Alpha=0.635 | Val F1=0.995 | Val MCC=0.991\n",
      "Epoch  5: Anom=0.0161 | Dom=1.3968 | Alpha=0.762 | Val F1=0.996 | Val MCC=0.992\n",
      "Epoch  6: Anom=0.0118 | Dom=1.3939 | Alpha=0.848 | Val F1=0.998 | Val MCC=0.996\n",
      "Epoch  7: Anom=0.0088 | Dom=1.3907 | Alpha=0.905 | Val F1=0.996 | Val MCC=0.994\n",
      "Epoch  8: Anom=0.0092 | Dom=1.3917 | Alpha=0.941 | Val F1=0.994 | Val MCC=0.990\n",
      "Epoch  9: Anom=0.0082 | Dom=1.3903 | Alpha=0.964 | Val F1=0.998 | Val MCC=0.996\n",
      "Epoch 10: Anom=0.0072 | Dom=1.3894 | Alpha=0.978 | Val F1=0.996 | Val MCC=0.992\n",
      "Epoch 11: Anom=0.0042 | Dom=1.3896 | Alpha=0.987 | Val F1=0.996 | Val MCC=0.992\n",
      "Epoch 12: Anom=0.0062 | Dom=1.3892 | Alpha=0.992 | Val F1=0.996 | Val MCC=0.992\n",
      "Epoch 13: Anom=0.0024 | Dom=1.3880 | Alpha=0.995 | Val F1=0.996 | Val MCC=0.992\n",
      "Epoch 14: Anom=0.0029 | Dom=1.3881 | Alpha=0.997 | Val F1=0.996 | Val MCC=0.994\n",
      "Early stopping at epoch 14\n",
      "\n",
      "DANN-BERT Final Results:\n",
      "F1: 0.722 | PR-AUC: 0.962 | MCC: 0.602 | ROC-AUC: 0.964\n",
      "Accuracy: 0.785 | Balanced Accuracy: 0.829\n",
      "Sensitivity: 0.936 | Specificity: 0.721\n",
      "\n",
      "================================================================================\n",
      "TRAINING HYBRID CLASSIFIER\n",
      "================================================================================\n",
      "Hybrid features: 1536 dimensions\n",
      "Epoch  1: Loss=0.1128 | Val F1=0.988 | Val MCC=0.979\n",
      "Epoch  2: Loss=0.0257 | Val F1=0.988 | Val MCC=0.980\n",
      "Epoch  3: Loss=0.0194 | Val F1=0.991 | Val MCC=0.984\n",
      "Epoch  4: Loss=0.0125 | Val F1=0.990 | Val MCC=0.982\n",
      "Epoch  5: Loss=0.0089 | Val F1=0.988 | Val MCC=0.980\n",
      "Epoch  6: Loss=0.0138 | Val F1=0.990 | Val MCC=0.983\n",
      "Epoch  7: Loss=0.0068 | Val F1=0.991 | Val MCC=0.985\n",
      "Epoch  8: Loss=0.0071 | Val F1=0.991 | Val MCC=0.984\n",
      "Epoch  9: Loss=0.0058 | Val F1=0.991 | Val MCC=0.984\n",
      "Epoch 10: Loss=0.0060 | Val F1=0.992 | Val MCC=0.987\n",
      "Epoch 11: Loss=0.0040 | Val F1=0.993 | Val MCC=0.988\n",
      "Epoch 12: Loss=0.0026 | Val F1=0.992 | Val MCC=0.987\n",
      "Epoch 13: Loss=0.0048 | Val F1=0.991 | Val MCC=0.985\n",
      "Epoch 14: Loss=0.0018 | Val F1=0.992 | Val MCC=0.986\n",
      "Epoch 15: Loss=0.0029 | Val F1=0.992 | Val MCC=0.986\n",
      "Epoch 16: Loss=0.0030 | Val F1=0.993 | Val MCC=0.988\n",
      "Early stopping at epoch 16\n",
      "\n",
      "Hybrid Classifier Final Results:\n",
      "F1: 0.451 | PR-AUC: 0.270 | MCC: -0.119 | ROC-AUC: 0.447\n",
      "Accuracy: 0.291 | Balanced Accuracy: 0.490\n",
      "Sensitivity: 0.980 | Specificity: 0.000\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "            Model    F1  PR-AUC    MCC  ROC-AUC  Accuracy  Bal-Acc  Sensitivity  Specificity\n",
      "  BERT Classifier 0.826   0.983  0.756    0.988     0.878    0.906        0.976        0.836\n",
      "        DANN-BERT 0.722   0.962  0.602    0.964     0.785    0.829        0.936        0.721\n",
      "Hybrid Classifier 0.451   0.270 -0.119    0.447     0.292    0.490        0.980        0.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "RES_PATH = ROOT / \"results\" / \"cross_source_transfer\" / \"advanced\"\n",
    "RES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load data\n",
    "with open(FEAT_PATH / \"hybrid_features.pkl\", 'rb') as f:\n",
    "    feat_data = pickle.load(f)['hybrid_features_data']\n",
    "\n",
    "with open(FEAT_PATH / \"cross_source_splits.pkl\", 'rb') as f:\n",
    "    splits = pickle.load(f)['splits']\n",
    "\n",
    "# Updated config\n",
    "cfg = {\n",
    "    'hidden': 768,\n",
    "    'batch': 32,\n",
    "    'lr': 5e-4,  # Lower learning rate for stability\n",
    "    'epochs': 20,\n",
    "    'dropout': 0.4,  # More dropout to prevent overfitting\n",
    "    'patience': 5  # More patience\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(feat_data)} sources\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION - ADD VALIDATION SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "exp = splits[0]\n",
    "test_src = exp['test_source']\n",
    "train_srcs = exp['train_sources']\n",
    "\n",
    "test_X = feat_data[test_src]['bert_only']\n",
    "test_y = feat_data[test_src]['labels']\n",
    "\n",
    "train_X_list = []\n",
    "train_y_list = []\n",
    "\n",
    "for src in train_srcs:\n",
    "    if feat_data[src]['labels'] is not None:\n",
    "        train_X_list.append(feat_data[src]['bert_only'])\n",
    "        train_y_list.append(feat_data[src]['labels'])\n",
    "\n",
    "train_X = np.vstack(train_X_list)\n",
    "train_y = np.concatenate(train_y_list)\n",
    "\n",
    "print(f\"\\nTest: {test_src} ({len(test_y)} samples)\")\n",
    "print(f\"Train: {len(train_srcs)} sources ({len(train_y)} samples)\")\n",
    "\n",
    "# Create train/validation split (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    train_X, train_y, test_size=0.2, random_state=SEED, stratify=train_y\n",
    ")\n",
    "\n",
    "print(f\"After split: Train={len(train_y)}, Val={len(val_y)}, Test={len(test_y)}\")\n",
    "\n",
    "# Check class distribution\n",
    "train_counts = np.bincount(train_y)\n",
    "val_counts = np.bincount(val_y)\n",
    "test_counts = np.bincount(test_y)\n",
    "print(f\"\\nTrain class distribution: Normal={train_counts[0]}, Anomaly={train_counts[1]}\")\n",
    "print(f\"Val class distribution: Normal={val_counts[0]}, Anomaly={val_counts[1]}\")\n",
    "print(f\"Test class distribution: Normal={test_counts[0]}, Anomaly={test_counts[1]}\")\n",
    "print(f\"Train anomaly rate: {np.mean(train_y):.3f}\")\n",
    "print(f\"Val anomaly rate: {np.mean(val_y):.3f}\")\n",
    "print(f\"Test anomaly rate: {np.mean(test_y):.3f}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "val_X_scaled = scaler.transform(val_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"\\nClass weights: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# Convert to tensors\n",
    "train_X_t = torch.FloatTensor(train_X_scaled)\n",
    "train_y_t = torch.LongTensor(train_y)\n",
    "val_X_t = torch.FloatTensor(val_X_scaled)\n",
    "val_y_t = torch.LongTensor(val_y)\n",
    "test_X_t = torch.FloatTensor(test_X_scaled)\n",
    "test_y_t = torch.LongTensor(test_y)\n",
    "\n",
    "train_ds = TensorDataset(train_X_t, train_y_t)\n",
    "val_ds = TensorDataset(val_X_t, val_y_t)\n",
    "test_ds = TensorDataset(test_X_t, test_y_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=cfg['batch'], shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=cfg['batch'])\n",
    "test_dl = DataLoader(test_ds, batch_size=cfg['batch'])\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: BERT CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING BERT CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class BertCls(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden'], 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BertCls().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=cfg['lr'])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_dl:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_probs = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dl:\n",
    "            X_batch = X_batch.to(device)\n",
    "            out = model(X_batch)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            \n",
    "            val_preds.extend(pred.cpu().numpy())\n",
    "            val_probs.extend(prob[:, 1].cpu().numpy())\n",
    "            val_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "    val_mcc = matthews_corrcoef(val_labels, val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f} | Val F1={val_f1:.3f} | Val MCC={val_mcc:.3f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), RES_PATH / 'best_bert_cls.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= cfg['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(RES_PATH / 'best_bert_cls.pt'))\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "probs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        out = model(X_batch)\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        \n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        probs.extend(prob[:, 1].cpu().numpy())\n",
    "        labels.extend(y_batch.numpy())\n",
    "\n",
    "y_true = np.array(labels)\n",
    "y_pred = np.array(preds)\n",
    "y_prob = np.array(probs)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "bal_acc = (sens + spec) / 2\n",
    "\n",
    "try:\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    p, r, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(r, p)\n",
    "except:\n",
    "    roc = 0\n",
    "    pr_auc = 0\n",
    "\n",
    "print(f\"\\nBERT Classifier Final Results:\")\n",
    "print(f\"F1: {f1:.3f} | PR-AUC: {pr_auc:.3f} | MCC: {mcc:.3f} | ROC-AUC: {roc:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f} | Balanced Accuracy: {bal_acc:.3f}\")\n",
    "print(f\"Sensitivity: {sens:.3f} | Specificity: {spec:.3f}\")\n",
    "\n",
    "bert_results = {\n",
    "    'f1': f1, 'pr_auc': pr_auc, 'mcc': mcc, 'roc': roc,\n",
    "    'acc': acc, 'bal_acc': bal_acc, 'sens': sens, 'spec': spec,\n",
    "    'preds': y_pred, 'probs': y_prob\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: DANN-BERT (Domain Adversarial)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DANN-BERT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.neg() * ctx.alpha, None\n",
    "\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, n_domains):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden'], 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Anomaly classifier\n",
    "        self.anomaly = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Domain classifier\n",
    "        self.domain = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, n_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        feat = self.feat(x)\n",
    "        anomaly_out = self.anomaly(feat)\n",
    "        \n",
    "        # Reverse gradient for domain classifier\n",
    "        reversed_feat = GradReverse.apply(feat, alpha)\n",
    "        domain_out = self.domain(reversed_feat)\n",
    "        \n",
    "        return anomaly_out, domain_out\n",
    "\n",
    "# Prepare domain labels - use validation split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "domain_map = {src: i for i, src in enumerate(train_srcs)}\n",
    "train_domains_full = []\n",
    "train_X_dann_list = []\n",
    "train_y_dann_list = []\n",
    "\n",
    "for src in train_srcs:\n",
    "    if feat_data[src]['labels'] is not None:\n",
    "        n_samples = len(feat_data[src]['labels'])\n",
    "        train_domains_full.extend([domain_map[src]] * n_samples)\n",
    "\n",
    "# Split with domains preserved\n",
    "combined_indices = np.arange(len(train_X))\n",
    "train_idx, val_idx = tts(\n",
    "    combined_indices, test_size=0.2, random_state=SEED, stratify=train_y\n",
    ")\n",
    "\n",
    "# Get validation data for DANN\n",
    "val_X_dann = train_X[val_idx]\n",
    "val_y_dann = train_y[val_idx]\n",
    "train_X_dann = train_X[train_idx]\n",
    "train_y_dann = train_y[train_idx]\n",
    "train_domains = [train_domains_full[i] for i in train_idx]\n",
    "\n",
    "# Scale\n",
    "train_X_dann_scaled = scaler.transform(train_X_dann)\n",
    "val_X_dann_scaled = scaler.transform(val_X_dann)\n",
    "\n",
    "train_X_dann_t = torch.FloatTensor(train_X_dann_scaled)\n",
    "train_y_dann_t = torch.LongTensor(train_y_dann)\n",
    "val_X_dann_t = torch.FloatTensor(val_X_dann_scaled)\n",
    "val_y_dann_t = torch.LongTensor(val_y_dann)\n",
    "train_dom_t = torch.LongTensor(train_domains)\n",
    "\n",
    "train_ds_dann = TensorDataset(train_X_dann_t, train_y_dann_t, train_dom_t)\n",
    "val_ds_dann = TensorDataset(val_X_dann_t, val_y_dann_t)\n",
    "train_dl_dann = DataLoader(train_ds_dann, batch_size=cfg['batch'], shuffle=True)\n",
    "val_dl_dann = DataLoader(val_ds_dann, batch_size=cfg['batch'])\n",
    "\n",
    "# Initialize DANN\n",
    "dann = DANN(len(domain_map)).to(device)\n",
    "opt = AdamW(dann.parameters(), lr=cfg['lr'])\n",
    "crit_anom = nn.CrossEntropyLoss(weight=class_weights)\n",
    "crit_dom = nn.CrossEntropyLoss()\n",
    "scheduler_dann = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_f1_dann = 0\n",
    "patience_counter_dann = 0\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    # Training\n",
    "    dann.train()\n",
    "    total_anom_loss = 0\n",
    "    total_dom_loss = 0\n",
    "    \n",
    "    # Dynamic lambda (standard DANN schedule: 0 to 1)\n",
    "    p = float(epoch) / float(cfg['epochs'])\n",
    "    alpha = 2. / (1. + np.exp(-10. * p)) - 1.\n",
    "    \n",
    "    for X_batch, y_batch, d_batch in train_dl_dann:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        d_batch = d_batch.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        anom_out, dom_out = dann(X_batch, alpha)\n",
    "        \n",
    "        # Losses\n",
    "        anom_loss = crit_anom(anom_out, y_batch)\n",
    "        dom_loss = crit_dom(dom_out, d_batch)\n",
    "        total_loss = anom_loss + dom_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_anom_loss += anom_loss.item()\n",
    "        total_dom_loss += dom_loss.item()\n",
    "    \n",
    "    avg_anom = total_anom_loss / len(train_dl_dann)\n",
    "    avg_dom = total_dom_loss / len(train_dl_dann)\n",
    "    \n",
    "    # Evaluation on validation set\n",
    "    dann.eval()\n",
    "    val_preds_dann = []\n",
    "    val_probs_dann = []\n",
    "    val_labels_dann = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dl_dann:\n",
    "            X_batch = X_batch.to(device)\n",
    "            anom_out, _ = dann(X_batch)\n",
    "            prob = F.softmax(anom_out, dim=1)\n",
    "            pred = torch.argmax(anom_out, dim=1)\n",
    "            \n",
    "            val_preds_dann.extend(pred.cpu().numpy())\n",
    "            val_probs_dann.extend(prob[:, 1].cpu().numpy())\n",
    "            val_labels_dann.extend(y_batch.numpy())\n",
    "    \n",
    "    val_f1_dann = f1_score(val_labels_dann, val_preds_dann, zero_division=0)\n",
    "    val_mcc_dann = matthews_corrcoef(val_labels_dann, val_preds_dann)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Anom={avg_anom:.4f} | Dom={avg_dom:.4f} | Alpha={alpha:.3f} | Val F1={val_f1_dann:.3f} | Val MCC={val_mcc_dann:.3f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler_dann.step(val_f1_dann)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1_dann > best_f1_dann:\n",
    "        best_f1_dann = val_f1_dann\n",
    "        torch.save(dann.state_dict(), RES_PATH / 'best_dann.pt')\n",
    "        patience_counter_dann = 0\n",
    "    else:\n",
    "        patience_counter_dann += 1\n",
    "        if patience_counter_dann >= cfg['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate\n",
    "dann.load_state_dict(torch.load(RES_PATH / 'best_dann.pt'))\n",
    "dann.eval()\n",
    "\n",
    "preds_dann = []\n",
    "probs_dann = []\n",
    "labels_dann = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        anom_out, _ = dann(X_batch)\n",
    "        prob = F.softmax(anom_out, dim=1)\n",
    "        pred = torch.argmax(anom_out, dim=1)\n",
    "        \n",
    "        preds_dann.extend(pred.cpu().numpy())\n",
    "        probs_dann.extend(prob[:, 1].cpu().numpy())\n",
    "        labels_dann.extend(y_batch.numpy())\n",
    "\n",
    "y_pred_dann = np.array(preds_dann)\n",
    "y_prob_dann = np.array(probs_dann)\n",
    "\n",
    "# Calculate metrics\n",
    "f1_dann = f1_score(y_true, y_pred_dann, zero_division=0)\n",
    "mcc_dann = matthews_corrcoef(y_true, y_pred_dann)\n",
    "acc_dann = accuracy_score(y_true, y_pred_dann)\n",
    "cm_dann = confusion_matrix(y_true, y_pred_dann)\n",
    "tn_d, fp_d, fn_d, tp_d = cm_dann.ravel()\n",
    "sens_dann = tp_d / (tp_d + fn_d) if (tp_d + fn_d) > 0 else 0\n",
    "spec_dann = tn_d / (tn_d + fp_d) if (tn_d + fp_d) > 0 else 0\n",
    "bal_acc_dann = (sens_dann + spec_dann) / 2\n",
    "\n",
    "try:\n",
    "    roc_dann = roc_auc_score(y_true, y_prob_dann)\n",
    "    p_d, r_d, _ = precision_recall_curve(y_true, y_prob_dann)\n",
    "    pr_auc_dann = auc(r_d, p_d)\n",
    "except:\n",
    "    roc_dann = 0\n",
    "    pr_auc_dann = 0\n",
    "\n",
    "print(f\"\\nDANN-BERT Final Results:\")\n",
    "print(f\"F1: {f1_dann:.3f} | PR-AUC: {pr_auc_dann:.3f} | MCC: {mcc_dann:.3f} | ROC-AUC: {roc_dann:.3f}\")\n",
    "print(f\"Accuracy: {acc_dann:.3f} | Balanced Accuracy: {bal_acc_dann:.3f}\")\n",
    "print(f\"Sensitivity: {sens_dann:.3f} | Specificity: {spec_dann:.3f}\")\n",
    "\n",
    "dann_results = {\n",
    "    'f1': f1_dann, 'pr_auc': pr_auc_dann, 'mcc': mcc_dann, 'roc': roc_dann,\n",
    "    'acc': acc_dann, 'bal_acc': bal_acc_dann, 'sens': sens_dann, 'spec': spec_dann,\n",
    "    'preds': y_pred_dann, 'probs': y_prob_dann\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: HYBRID CLASSIFIER (Template + BERT)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING HYBRID CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_X_hyb = feat_data[test_src]['hybrid_variants']['bert_embedding_concat']\n",
    "\n",
    "train_X_hyb_list = []\n",
    "train_y_hyb_list = []\n",
    "\n",
    "for src in train_srcs:\n",
    "    if (feat_data[src]['labels'] is not None and \n",
    "        'bert_embedding_concat' in feat_data[src]['hybrid_variants']):\n",
    "        train_X_hyb_list.append(feat_data[src]['hybrid_variants']['bert_embedding_concat'])\n",
    "        train_y_hyb_list.append(feat_data[src]['labels'])\n",
    "\n",
    "train_X_hyb = np.vstack(train_X_hyb_list)\n",
    "train_y_hyb = np.concatenate(train_y_hyb_list)\n",
    "\n",
    "# Split hybrid data for validation\n",
    "train_X_hyb, val_X_hyb, train_y_hyb, val_y_hyb = train_test_split(\n",
    "    train_X_hyb, train_y_hyb, test_size=0.2, random_state=SEED, stratify=train_y_hyb\n",
    ")\n",
    "\n",
    "print(f\"Hybrid features: {train_X_hyb.shape[1]} dimensions\")\n",
    "\n",
    "# Normalize hybrid features\n",
    "scaler_hyb = StandardScaler()\n",
    "train_X_hyb_scaled = scaler_hyb.fit_transform(train_X_hyb)\n",
    "val_X_hyb_scaled = scaler_hyb.transform(val_X_hyb)\n",
    "test_X_hyb_scaled = scaler_hyb.transform(test_X_hyb)\n",
    "\n",
    "train_X_hyb_t = torch.FloatTensor(train_X_hyb_scaled)\n",
    "train_y_hyb_t = torch.LongTensor(train_y_hyb)\n",
    "val_X_hyb_t = torch.FloatTensor(val_X_hyb_scaled)\n",
    "val_y_hyb_t = torch.LongTensor(val_y_hyb)\n",
    "test_X_hyb_t = torch.FloatTensor(test_X_hyb_scaled)\n",
    "\n",
    "train_ds_hyb = TensorDataset(train_X_hyb_t, train_y_hyb_t)\n",
    "val_ds_hyb = TensorDataset(val_X_hyb_t, val_y_hyb_t)\n",
    "test_ds_hyb = TensorDataset(test_X_hyb_t, test_y_t)\n",
    "train_dl_hyb = DataLoader(train_ds_hyb, batch_size=cfg['batch'], shuffle=True)\n",
    "val_dl_hyb = DataLoader(val_ds_hyb, batch_size=cfg['batch'])\n",
    "test_dl_hyb = DataLoader(test_ds_hyb, batch_size=cfg['batch'])\n",
    "\n",
    "class HybridCls(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "hybrid = HybridCls(train_X_hyb.shape[1]).to(device)\n",
    "opt_hyb = AdamW(hybrid.parameters(), lr=cfg['lr'])\n",
    "crit_hyb = nn.CrossEntropyLoss(weight=class_weights)\n",
    "scheduler_hyb = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_hyb, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_f1_hyb = 0\n",
    "patience_counter_hyb = 0\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    # Training\n",
    "    hybrid.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_dl_hyb:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        opt_hyb.zero_grad()\n",
    "        out = hybrid(X_batch)\n",
    "        loss = crit_hyb(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt_hyb.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss_hyb = total_loss / len(train_dl_hyb)\n",
    "    \n",
    "    # Evaluation on validation set\n",
    "    hybrid.eval()\n",
    "    val_preds_hyb = []\n",
    "    val_probs_hyb = []\n",
    "    val_labels_hyb = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dl_hyb:\n",
    "            X_batch = X_batch.to(device)\n",
    "            out = hybrid(X_batch)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            \n",
    "            val_preds_hyb.extend(pred.cpu().numpy())\n",
    "            val_probs_hyb.extend(prob[:, 1].cpu().numpy())\n",
    "            val_labels_hyb.extend(y_batch.numpy())\n",
    "    \n",
    "    val_f1_hyb = f1_score(val_labels_hyb, val_preds_hyb, zero_division=0)\n",
    "    val_mcc_hyb = matthews_corrcoef(val_labels_hyb, val_preds_hyb)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss_hyb:.4f} | Val F1={val_f1_hyb:.3f} | Val MCC={val_mcc_hyb:.3f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler_hyb.step(val_f1_hyb)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1_hyb > best_f1_hyb:\n",
    "        best_f1_hyb = val_f1_hyb\n",
    "        torch.save(hybrid.state_dict(), RES_PATH / 'best_hybrid.pt')\n",
    "        patience_counter_hyb = 0\n",
    "    else:\n",
    "        patience_counter_hyb += 1\n",
    "        if patience_counter_hyb >= cfg['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate\n",
    "hybrid.load_state_dict(torch.load(RES_PATH / 'best_hybrid.pt'))\n",
    "hybrid.eval()\n",
    "\n",
    "preds_hyb = []\n",
    "probs_hyb = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_dl_hyb:\n",
    "        X_batch = X_batch.to(device)\n",
    "        out = hybrid(X_batch)\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        \n",
    "        preds_hyb.extend(pred.cpu().numpy())\n",
    "        probs_hyb.extend(prob[:, 1].cpu().numpy())\n",
    "\n",
    "y_pred_hyb = np.array(preds_hyb)\n",
    "y_prob_hyb = np.array(probs_hyb)\n",
    "\n",
    "# Calculate metrics\n",
    "f1_hyb = f1_score(y_true, y_pred_hyb, zero_division=0)\n",
    "mcc_hyb = matthews_corrcoef(y_true, y_pred_hyb)\n",
    "acc_hyb = accuracy_score(y_true, y_pred_hyb)\n",
    "cm_hyb = confusion_matrix(y_true, y_pred_hyb)\n",
    "tn_h, fp_h, fn_h, tp_h = cm_hyb.ravel()\n",
    "sens_hyb = tp_h / (tp_h + fn_h) if (tp_h + fn_h) > 0 else 0\n",
    "spec_hyb = tn_h / (tn_h + fp_h) if (tn_h + fp_h) > 0 else 0\n",
    "bal_acc_hyb = (sens_hyb + spec_hyb) / 2\n",
    "\n",
    "try:\n",
    "    roc_hyb = roc_auc_score(y_true, y_prob_hyb)\n",
    "    p_h, r_h, _ = precision_recall_curve(y_true, y_prob_hyb)\n",
    "    pr_auc_hyb = auc(r_h, p_h)\n",
    "except:\n",
    "    roc_hyb = 0\n",
    "    pr_auc_hyb = 0\n",
    "\n",
    "print(f\"\\nHybrid Classifier Final Results:\")\n",
    "print(f\"F1: {f1_hyb:.3f} | PR-AUC: {pr_auc_hyb:.3f} | MCC: {mcc_hyb:.3f} | ROC-AUC: {roc_hyb:.3f}\")\n",
    "print(f\"Accuracy: {acc_hyb:.3f} | Balanced Accuracy: {bal_acc_hyb:.3f}\")\n",
    "print(f\"Sensitivity: {sens_hyb:.3f} | Specificity: {spec_hyb:.3f}\")\n",
    "\n",
    "hybrid_results = {\n",
    "    'f1': f1_hyb, 'pr_auc': pr_auc_hyb, 'mcc': mcc_hyb, 'roc': roc_hyb,\n",
    "    'acc': acc_hyb, 'bal_acc': bal_acc_hyb, 'sens': sens_hyb, 'spec': spec_hyb,\n",
    "    'preds': y_pred_hyb, 'probs': y_prob_hyb\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS COMPARISON AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all results\n",
    "results = {\n",
    "    'BERT Classifier': bert_results,\n",
    "    'DANN-BERT': dann_results,\n",
    "    'Hybrid Classifier': hybrid_results\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model,\n",
    "        'F1': res['f1'],\n",
    "        'PR-AUC': res['pr_auc'],\n",
    "        'MCC': res['mcc'],\n",
    "        'ROC-AUC': res['roc'],\n",
    "        'Accuracy': res['acc'],\n",
    "        'Bal-Acc': res['bal_acc'],\n",
    "        'Sensitivity': res['sens'],\n",
    "        'Specificity': res['spec']\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(comp_df.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d1251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
