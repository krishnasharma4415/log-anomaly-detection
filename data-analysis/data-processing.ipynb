{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f35fa6",
   "metadata": {},
   "source": [
    "IMPORTS AND INITIAL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfd5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels:\n",
      "  0: normal\n",
      "  1: anomaly\n",
      "Dataset path: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\dataset\\labeled_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: 'normal',\n",
    "    1: 'anomaly'  # All anomaly types combined\n",
    "}\n",
    "\n",
    "# Original 7-class mapping for reference\n",
    "ORIGINAL_LABEL_MAP = {\n",
    "    0: 'normal',\n",
    "    1: 'security_anomaly',\n",
    "    2: 'system_failure',\n",
    "    3: 'performance_issue',\n",
    "    4: 'network_anomaly',\n",
    "    5: 'config_error',\n",
    "    6: 'hardware_issue'\n",
    "}\n",
    "\n",
    "print(\"Class Labels:\")\n",
    "for label_id, label_name in LABEL_MAP.items():\n",
    "    print(f\"  {label_id}: {label_name}\")\n",
    "print(f\"Dataset path: {LABELED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ba07c",
   "metadata": {},
   "source": [
    "Timestamp parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da424d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_android_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}-{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_apache_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(time_str, \"%a %b %d %H:%M:%S %Y\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_bgl_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "        return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hadoop_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hdfs_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        year = \"20\" + date_str[:2]\n",
    "        month = date_str[2:4]\n",
    "        day = date_str[4:6]\n",
    "        hour = time_str[:2]\n",
    "        minute = time_str[2:4]\n",
    "        second = time_str[4:6]\n",
    "        dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_healthapp_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) >= 4:\n",
    "            time_str = ':'.join(parts[:-1]) + '.' + parts[-1]\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d-%H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_hpc_timestamp(row):\n",
    "    try:\n",
    "        timestamp = int(str(row['Time']).strip())\n",
    "        dt = datetime.fromtimestamp(timestamp)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_linux_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_mac_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Month']).strip()\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {date_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openssh_timestamp(row):\n",
    "    try:\n",
    "        month_str = str(row['Date']).strip()\n",
    "        day_str = str(row['Day']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_openstack_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_proxifier_timestamp(row):\n",
    "    try:\n",
    "        time_str = str(row['Time']).strip()\n",
    "        current_year = datetime.now().year\n",
    "        dt = datetime.strptime(f\"{current_year}.{time_str}\", \"%Y.%m.%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_spark_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"20{date_str} {time_str}\", \"%Y/%m/%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_thunderbird_timestamp(row):\n",
    "    try:\n",
    "        if 'Month' in row and 'Day' in row and 'Time' in row:\n",
    "            month_str = str(row['Month']).strip()\n",
    "            day_str = str(row['Day']).strip()\n",
    "            time_str = str(row['Time']).strip()\n",
    "            current_year = datetime.now().year\n",
    "            dt = datetime.strptime(f\"{current_year} {month_str} {day_str} {time_str}\", \"%Y %b %d %H:%M:%S\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "        elif 'Date' in row:\n",
    "            date_str = str(row['Date']).strip()\n",
    "            dt = datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "            return dt.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_windows_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip()\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.000\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_zookeeper_timestamp(row):\n",
    "    try:\n",
    "        date_str = str(row['Date']).strip()\n",
    "        time_str = str(row['Time']).strip().replace(',', '.')\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc9ba4",
   "metadata": {},
   "source": [
    "LOG TYPE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b03e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_log_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'android' in filename:\n",
    "        return 'android'\n",
    "    elif 'apache' in filename:\n",
    "        return 'apache'\n",
    "    elif 'bgl' in filename:\n",
    "        return 'bgl'\n",
    "    elif 'hadoop' in filename:\n",
    "        return 'hadoop'\n",
    "    elif 'hdfs' in filename:\n",
    "        return 'hdfs'\n",
    "    elif 'health' in filename:\n",
    "        return 'healthapp'\n",
    "    elif 'hpc' in filename:\n",
    "        return 'hpc'\n",
    "    elif 'linux' in filename:\n",
    "        return 'linux'\n",
    "    elif 'mac' in filename:\n",
    "        return 'mac'\n",
    "    elif 'openssh' in filename:\n",
    "        return 'openssh'\n",
    "    elif 'openstack' in filename:\n",
    "        return 'openstack'\n",
    "    elif 'proxifier' in filename:\n",
    "        return 'proxifier'\n",
    "    elif 'spark' in filename:\n",
    "        return 'spark'\n",
    "    elif 'thunderbird' in filename:\n",
    "        return 'thunderbird'\n",
    "    elif 'windows' in filename:\n",
    "        return 'windows'\n",
    "    elif 'zookeeper' in filename or 'zookeper' in filename:\n",
    "        return 'zookeeper'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "timestamp_parsers = {\n",
    "    'android': parse_android_timestamp,\n",
    "    'apache': parse_apache_timestamp,\n",
    "    'bgl': parse_bgl_timestamp,\n",
    "    'hadoop': parse_hadoop_timestamp,\n",
    "    'hdfs': parse_hdfs_timestamp,\n",
    "    'healthapp': parse_healthapp_timestamp,\n",
    "    'hpc': parse_hpc_timestamp,\n",
    "    'linux': parse_linux_timestamp,\n",
    "    'mac': parse_mac_timestamp,\n",
    "    'openssh': parse_openssh_timestamp,\n",
    "    'openstack': parse_openstack_timestamp,\n",
    "    'proxifier': parse_proxifier_timestamp,\n",
    "    'spark': parse_spark_timestamp,\n",
    "    'thunderbird': parse_thunderbird_timestamp,\n",
    "    'windows': parse_windows_timestamp,\n",
    "    'zookeeper': parse_zookeeper_timestamp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5ac3c",
   "metadata": {},
   "source": [
    "LOAD AND PROCESS ALL LABELED CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1330be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 labeled CSV files\n"
     ]
    }
   ],
   "source": [
    "csv_files = list(LABELED_DATA_PATH.glob(\"*_labeled.csv\"))\n",
    "print(f\"Found {len(csv_files)} labeled CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c590c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: Android_2k_labeled.csv\n",
      "Size: 0.47 MB\n",
      "Detected log type: android\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,974 (98.70%)\n",
      "    1 (anomaly): 26 (1.30%)\n",
      "\n",
      "  Imbalance ratio: 75.92:1HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Apache_2k_labeled.csv\n",
      "Size: 0.29 MB\n",
      "Detected log type: apache\n",
      "Loaded dataframe: (2000, 9)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,429 (71.45%)\n",
      "    1 (anomaly): 571 (28.55%)\n",
      "\n",
      "  Imbalance ratio: 2.50:1 ✓\n",
      "\n",
      "================================================================================\n",
      "Processing: BGL_2k_labeled.csv\n",
      "Size: 0.44 MB\n",
      "Detected log type: bgl\n",
      "Loaded dataframe: (2000, 16)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,832 (91.60%)\n",
      "    1 (anomaly): 168 (8.40%)\n",
      "\n",
      "  Imbalance ratio: 10.90:1HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Hadoop_2k_labeled.csv\n",
      "Size: 0.56 MB\n",
      "Detected log type: hadoop\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 690 (34.50%)\n",
      "    1 (anomaly): 1,310 (65.50%)\n",
      "\n",
      "  Imbalance ratio: 1.90:1 ✓\n",
      "\n",
      "================================================================================\n",
      "Processing: HDFS_2k_labeled.csv\n",
      "Size: 0.42 MB\n",
      "Detected log type: hdfs\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 237/2000 (11.8%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 1/2\n",
      "  Present: ['normal']\n",
      " Missing: ['anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 2,000 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "Processing: HealthApp_2k_labeled.csv\n",
      "Size: 0.31 MB\n",
      "Detected log type: healthapp\n",
      "Loaded dataframe: (2000, 10)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,989 (99.45%)\n",
      "    1 (anomaly): 11 (0.55%)\n",
      "\n",
      "  Imbalance ratio: 180.82:1EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: HPC_2k_labeled.csv\n",
      "Size: 0.25 MB\n",
      "Detected log type: hpc\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,110 (55.50%)\n",
      "    1 (anomaly): 890 (44.50%)\n",
      "\n",
      "  Imbalance ratio: 1.25:1 ✓\n",
      "\n",
      "================================================================================\n",
      "Processing: Linux_2k_labeled.csv\n",
      "Size: 0.37 MB\n",
      "Detected log type: linux\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 94 (4.70%)\n",
      "    1 (anomaly): 1,906 (95.30%)\n",
      "\n",
      "  Imbalance ratio: 20.28:1HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Mac_2k_labeled.csv\n",
      "Size: 0.51 MB\n",
      "Detected log type: mac\n",
      "Loaded dataframe: (2000, 14)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,564 (78.20%)\n",
      "    1 (anomaly): 436 (21.80%)\n",
      "\n",
      "  Imbalance ratio: 3.59:1 ✓\n",
      "\n",
      "================================================================================\n",
      "Processing: OpenSSH_2k_labeled.csv\n",
      "Size: 0.40 MB\n",
      "Detected log type: openssh\n",
      "Loaded dataframe: (2000, 12)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 1/2\n",
      "  Present: ['anomaly']\n",
      " Missing: ['normal']\n",
      "\n",
      "  Distribution:\n",
      "    1 (anomaly): 2,000 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "Processing: OpenStack_2k_labeled.csv\n",
      "Size: 0.73 MB\n",
      "Detected log type: openstack\n",
      "Loaded dataframe: (2000, 14)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 1/2\n",
      "  Present: ['normal']\n",
      " Missing: ['anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 2,000 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "Processing: Proxifier_2k_labeled.csv\n",
      "Size: 0.38 MB\n",
      "Detected log type: proxifier\n",
      "Loaded dataframe: (2000, 9)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,903 (95.15%)\n",
      "    1 (anomaly): 97 (4.85%)\n",
      "\n",
      "  Imbalance ratio: 19.62:1HIGH IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Spark_2k_labeled.csv\n",
      "Size: 0.33 MB\n",
      "Detected log type: spark\n",
      "Loaded dataframe: (2000, 11)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,992 (99.60%)\n",
      "    1 (anomaly): 8 (0.40%)\n",
      "\n",
      "  Imbalance ratio: 249.00:1EXTREME IMBALANCE!\n",
      "\n",
      "================================================================================\n",
      "Processing: Thunderbird_2k_labeled.csv\n",
      "Size: 0.48 MB\n",
      "Detected log type: thunderbird\n",
      "Loaded dataframe: (2000, 17)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,805 (90.25%)\n",
      "    1 (anomaly): 195 (9.75%)\n",
      "\n",
      "  Imbalance ratio: 9.26:1MODERATE IMBALANCE\n",
      "\n",
      "================================================================================\n",
      "Processing: Windows_2k_labeled.csv\n",
      "Size: 0.43 MB\n",
      "Detected log type: windows\n",
      "Loaded dataframe: (2000, 11)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,209 (60.45%)\n",
      "    1 (anomaly): 791 (39.55%)\n",
      "\n",
      "  Imbalance ratio: 1.53:1 ✓\n",
      "\n",
      "================================================================================\n",
      "Processing: Zookeeper_2k_labeled.csv\n",
      "Size: 0.40 MB\n",
      "Detected log type: zookeeper\n",
      "Loaded dataframe: (2000, 13)\n",
      "Normalized timestamps: 2000/2000 (100.0%)\n",
      "\n",
      "BINARY CLASS DISTRIBUTION ANALYSIS:\n",
      "  Classes present: 2/2\n",
      "  Present: ['normal', 'anomaly']\n",
      "\n",
      "  Distribution:\n",
      "    0 (normal): 1,486 (74.30%)\n",
      "    1 (anomaly): 514 (25.70%)\n",
      "\n",
      "  Imbalance ratio: 2.89:1 ✓\n"
     ]
    }
   ],
   "source": [
    "processed_files = {}\n",
    "source_class_analysis = {}\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {file_path.name}\")\n",
    "    print(f\"Size: {file_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    log_type = detect_log_type(file_path.name)\n",
    "    print(f\"Detected log type: {log_type}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded dataframe: {df.shape}\")\n",
    "        \n",
    "        if log_type != 'unknown' and log_type in timestamp_parsers:\n",
    "            parser_func = timestamp_parsers[log_type]\n",
    "            df['timestamp_normalized'] = df.apply(parser_func, axis=1)\n",
    "            \n",
    "            successful = df['timestamp_normalized'].notna().sum()\n",
    "            total = len(df)\n",
    "            print(f\"Normalized timestamps: {successful}/{total} ({successful/total*100:.1f}%)\")\n",
    "            \n",
    "            df['timestamp_dt'] = pd.to_datetime(df['timestamp_normalized'], errors='coerce')\n",
    "            \n",
    "            df['hour'] = df['timestamp_dt'].dt.hour\n",
    "            df['day_of_week'] = df['timestamp_dt'].dt.dayofweek\n",
    "            df['day_of_month'] = df['timestamp_dt'].dt.day\n",
    "            df['month'] = df['timestamp_dt'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "            df['is_business_hours'] = df['hour'].between(9, 17).astype(int)\n",
    "            df['is_night'] = df['hour'].between(0, 6).astype(int)\n",
    "            \n",
    "            df = df.sort_values('timestamp_dt').reset_index(drop=True)\n",
    "            df['time_diff_seconds'] = df['timestamp_dt'].diff().dt.total_seconds().fillna(0)\n",
    "            \n",
    "            df['log_index'] = range(len(df))\n",
    "            df['logs_last_10'] = df.groupby(pd.Grouper(key='timestamp_dt', freq='1min'))['log_index'].transform('count')\n",
    "            \n",
    "            if 'AnomalyLabel' in df.columns:\n",
    "                # Convert to binary: 0=normal, 1=anomaly (any non-zero becomes 1)\n",
    "                df['AnomalyLabel'] = df['AnomalyLabel'].fillna(0).astype(int).clip(0, 6)\n",
    "                df['AnomalyLabel'] = (df['AnomalyLabel'] > 0).astype(int)\n",
    "                \n",
    "                unique_labels = df['AnomalyLabel'].unique()\n",
    "                present_classes = sorted([int(x) for x in unique_labels])\n",
    "                missing_classes = [i for i in range(2) if i not in present_classes]  # Binary: 0,1\n",
    "                \n",
    "                label_counts = df['AnomalyLabel'].value_counts().sort_index()\n",
    "                \n",
    "                print(f\"\\nBINARY CLASS DISTRIBUTION ANALYSIS:\")\n",
    "                print(f\"  Classes present: {len(present_classes)}/2\")\n",
    "                print(f\"  Present: {[LABEL_MAP[i] for i in present_classes]}\")\n",
    "                if missing_classes:\n",
    "                    print(f\" Missing: {[LABEL_MAP[i] for i in missing_classes]}\")\n",
    "                \n",
    "                print(f\"\\n  Distribution:\")\n",
    "                for label in present_classes:\n",
    "                    count = label_counts.get(label, 0)\n",
    "                    label_name = LABEL_MAP[label]\n",
    "                    percentage = (count / len(df) * 100)\n",
    "                    print(f\"    {label} ({label_name}): {count:,} ({percentage:.2f}%)\")\n",
    "                \n",
    "                class_counts = [label_counts.get(i, 0) for i in present_classes]\n",
    "                if len(class_counts) > 1:\n",
    "                    imbalance_ratio = max(class_counts) / min([c for c in class_counts if c > 0])\n",
    "                    print(f\"\\n  Imbalance ratio: {imbalance_ratio:.2f}:1\", end=\"\")\n",
    "                    if imbalance_ratio > 100:\n",
    "                        print(\"EXTREME IMBALANCE!\")\n",
    "                    elif imbalance_ratio > 10:\n",
    "                        print(\"HIGH IMBALANCE\")\n",
    "                    elif imbalance_ratio > 5:\n",
    "                        print(\"MODERATE IMBALANCE\")\n",
    "                    else:\n",
    "                        print(\" ✓\")\n",
    "                \n",
    "                source_class_analysis[file_path.stem] = {\n",
    "                    'present_classes': present_classes,\n",
    "                    'missing_classes': missing_classes,\n",
    "                    'class_counts': {int(k): int(v) for k, v in label_counts.items()},\n",
    "                    'total_samples': len(df),\n",
    "                    'imbalance_ratio': imbalance_ratio if len(class_counts) > 1 else 0\n",
    "                }\n",
    "            \n",
    "            processed_files[file_path.name] = {\n",
    "                'dataframe': df,\n",
    "                'log_type': log_type,\n",
    "                'file_path': file_path\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(f\"Skipping - unknown type\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae440b",
   "metadata": {},
   "source": [
    "SAVE NORMALIZED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa98565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Android_2k_enhanced.csv\n",
      "Saved: Apache_2k_enhanced.csv\n",
      "Saved: BGL_2k_enhanced.csv\n",
      "Saved: Hadoop_2k_enhanced.csv\n",
      "Saved: HDFS_2k_enhanced.csv\n",
      "Saved: HealthApp_2k_enhanced.csv\n",
      "Saved: HPC_2k_enhanced.csv\n",
      "Saved: Linux_2k_enhanced.csv\n",
      "Saved: Mac_2k_enhanced.csv\n",
      "Saved: OpenSSH_2k_enhanced.csv\n",
      "Saved: OpenStack_2k_enhanced.csv\n",
      "Saved: Proxifier_2k_enhanced.csv\n",
      "Saved: Spark_2k_enhanced.csv\n",
      "Saved: Thunderbird_2k_enhanced.csv\n",
      "Saved: Windows_2k_enhanced.csv\n",
      "Saved: Zookeeper_2k_enhanced.csv\n"
     ]
    }
   ],
   "source": [
    "normalized_output_path = DATASET_PATH / \"normalized\"\n",
    "normalized_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "for filename, data in processed_files.items():\n",
    "    df = data['dataframe']\n",
    "    output_filename = filename.replace('_labeled.csv', '_enhanced.csv')\n",
    "    output_path = normalized_output_path / output_filename\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2952ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-SOURCE CLASS AVAILABILITY ANALYSIS\n",
      "\n",
      "Binary class availability across sources:\n",
      "\n",
      "0 (normal):\n",
      "  Available in: 15/16 sources (93.8%)\n",
      "  Sources: Android_2k_labeled, Apache_2k_labeled, BGL_2k_labeled, Hadoop_2k_labeled, HDFS_2k_labeled ...\n",
      "\n",
      "1 (anomaly):\n",
      "  Available in: 14/16 sources (87.5%)\n",
      "  Sources: Android_2k_labeled, Apache_2k_labeled, BGL_2k_labeled, Hadoop_2k_labeled, HealthApp_2k_labeled ...\n"
     ]
    }
   ],
   "source": [
    "print(\"CROSS-SOURCE CLASS AVAILABILITY ANALYSIS\")\n",
    "all_sources = list(source_class_analysis.keys())\n",
    "class_availability = {i: [] for i in range(2)}  # Binary: 0,1\n",
    "\n",
    "for source, analysis in source_class_analysis.items():\n",
    "    for cls in range(2):  # Binary: 0,1\n",
    "        if cls in analysis['present_classes']:\n",
    "            class_availability[cls].append(source)\n",
    "\n",
    "print(\"\\nBinary class availability across sources:\")\n",
    "for cls in range(2):  # Binary: 0,1\n",
    "    sources_with_class = class_availability[cls]\n",
    "    coverage = len(sources_with_class) / len(all_sources) * 100 if all_sources else 0\n",
    "    print(f\"\\n{cls} ({LABEL_MAP[cls]}):\")\n",
    "    print(f\"  Available in: {len(sources_with_class)}/{len(all_sources)} sources ({coverage:.1f}%)\")\n",
    "    if coverage < 50:\n",
    "        print(f\" LOW AVAILABILITY - Limited training data\")\n",
    "    if sources_with_class:\n",
    "        print(f\"  Sources: {', '.join(sources_with_class[:5])}{' ...' if len(sources_with_class) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9e6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL: 2 sources have extreme imbalance (>100:1)\n",
      "   → Use SMOTE with careful k-neighbors selection\n",
      "   → Apply class weights in model training\n",
      "   → Consider focal loss for deep learning\n"
     ]
    }
   ],
   "source": [
    "recommendations = []\n",
    "\n",
    "extreme_imbalance_sources = [s for s, a in source_class_analysis.items() if a['imbalance_ratio'] > 100]\n",
    "if extreme_imbalance_sources:\n",
    "    recommendations.append(f\"CRITICAL: {len(extreme_imbalance_sources)} sources have extreme imbalance (>100:1)\")\n",
    "    recommendations.append(\"   → Use SMOTE with careful k-neighbors selection\")\n",
    "    recommendations.append(\"   → Apply class weights in model training\")\n",
    "    recommendations.append(\"   → Consider focal loss for deep learning\")\n",
    "\n",
    "rare_classes = [cls for cls, sources in class_availability.items() if len(sources) < len(all_sources) * 0.3]\n",
    "if rare_classes:\n",
    "    rare_names = [LABEL_MAP[c] for c in rare_classes]\n",
    "    recommendations.append(f\"\\nWARNING: {len(rare_classes)} classes are rare across sources\")\n",
    "    recommendations.append(f\"   Classes: {', '.join(rare_names)}\")\n",
    "    recommendations.append(\"   → Use stratified cross-validation\")\n",
    "    recommendations.append(\"   → Consider hierarchical classification (binary first, then multi-class)\")\n",
    "    recommendations.append(\"   → Use transfer learning from sources with these classes\")\n",
    "\n",
    "missing_in_all = [cls for cls, sources in class_availability.items() if len(sources) == 0]\n",
    "if missing_in_all:\n",
    "    recommendations.append(f\"\\nCRITICAL: {len(missing_in_all)} classes missing from ALL sources!\")\n",
    "    recommendations.append(\"   → Cannot train on these classes\")\n",
    "    recommendations.append(\"   → Consider reducing to fewer classes or synthetic data generation\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"✓ Data appears reasonably balanced for multi-class training\")\n",
    "    recommendations.append(\"  → Still recommend using class weights and stratified sampling\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28220205",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_metadata = {\n",
    "    'num_classes': 2,  # Binary classification\n",
    "    'label_map': LABEL_MAP,\n",
    "    'original_label_map': ORIGINAL_LABEL_MAP,\n",
    "    'source_analysis': {k: {**v, 'present_classes': [int(x) for x in v['present_classes']], \n",
    "                             'missing_classes': [int(x) for x in v['missing_classes']]} \n",
    "                        for k, v in source_class_analysis.items()},\n",
    "    'class_availability': {int(k): v for k, v in class_availability.items()},\n",
    "    'recommendations': recommendations,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = normalized_output_path / \"imbalance_analysis.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(imbalance_metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
