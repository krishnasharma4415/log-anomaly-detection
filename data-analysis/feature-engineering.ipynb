{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21da223",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.masking import MaskingInstruction\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler as SKStandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac1a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "DATASET_PATH = PROJECT_ROOT / \"dataset\"\n",
    "LABELED_DATA_PATH = DATASET_PATH / \"labeled_data\"\n",
    "NORMALIZED_DATA_PATH = DATASET_PATH / \"normalized\"\n",
    "FEATURES_PATH = PROJECT_ROOT / \"features\"\n",
    "FEATURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402d6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .appName(\"MultiClassFeatureEngineering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa3e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 log sources\n",
      "Label mapping: {0: 'normal', 1: 'anomaly'}\n"
     ]
    }
   ],
   "source": [
    "PROJECT_CONFIG = {\n",
    "    'bert_model_name': 'bert-base-uncased',\n",
    "    'max_sequence_length': 512,\n",
    "    'num_classes': 2,\n",
    "    'label_map': {0: 'normal', 1: 'anomaly'},\n",
    "    'original_label_map': {\n",
    "        0: 'normal',\n",
    "        1: 'security_anomaly',\n",
    "        2: 'system_failure',\n",
    "        3: 'performance_issue',\n",
    "        4: 'network_anomaly',\n",
    "        5: 'config_error',\n",
    "        6: 'hardware_issue'\n",
    "    },\n",
    "    'log_sources': []\n",
    "}\n",
    "\n",
    "dataset_registry = {}\n",
    "enhanced_files = list(NORMALIZED_DATA_PATH.glob(\"*_enhanced.csv\"))\n",
    "for file_path in enhanced_files:\n",
    "    source_name = file_path.stem.replace('_enhanced', '')\n",
    "    PROJECT_CONFIG['log_sources'].append(source_name)\n",
    "    dataset_registry[source_name] = {'file_path': str(file_path), 'log_type': source_name}\n",
    "\n",
    "print(f\"Loaded {len(dataset_registry)} log sources\")\n",
    "print(f\"Label mapping: {PROJECT_CONFIG['label_map']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5824bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model = AutoModel.from_pretrained(PROJECT_CONFIG['bert_model_name'])\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "drain_configs = {'hdfs': {'sim_th': 0.5, 'depth': 4}, 'bgl': {'sim_th': 0.3, 'depth': 5}, 'hadoop': {'sim_th': 0.4, 'depth': 4}, 'apache': {'sim_th': 0.4, 'depth': 4}, 'default': {'sim_th': 0.4, 'depth': 4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e69c19",
   "metadata": {},
   "source": [
    "Temporal and Statistical Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d177084",
   "metadata": {},
   "source": [
    "Text-based Spark UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43bae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_patterns_by_source(source_type):\n",
    "    patterns = {\n",
    "        'apache': {'error_level': r'\\b(error|critical|alert|emergency)\\b', 'http_error': r'\\b(40[0-9]|50[0-9])\\b', 'security_threat': r'\\b(attack|intrusion|unauthorized|forbidden|hack)\\b', 'resource_issue': r'\\b(timeout|memory|disk|space|limit)\\b'},\n",
    "        'linux': {'kernel_panic': r'\\b(kernel|panic|oops|segfault|core dump)\\b', 'auth_failure': r'\\b(authentication failed|login failed|access denied)\\b', 'resource_exhaustion': r'\\b(out of memory|disk full|no space|quota exceeded)\\b', 'hardware_error': r'\\b(hardware|disk error|i/o error|bad sector)\\b'},\n",
    "        'hadoop': {'job_failure': r'\\b(job failed|task failed|exception|error)\\b', 'performance_issue': r'\\b(slow|timeout|latency|performance)\\b', 'network_problem': r'\\b(connection|unreachable|network|socket)\\b', 'config_error': r'\\b(configuration|config|property|setting)\\b'},\n",
    "        'openssh': {'security_breach': r'\\b(failed password|invalid user|break-in|attack)\\b', 'connection_issue': r'\\b(connection closed|timeout|refused)\\b'},\n",
    "        'bgl': {'system_failure': r'\\b(failure|failed|error|exception)\\b', 'hardware_issue': r'\\b(hardware|disk|memory|cpu|node)\\b', 'config_error': r'\\b(config|configuration|parameter)\\b'},\n",
    "        'hdfs': {'system_failure': r'\\b(block|replica|datanode|namenode|error)\\b', 'network_problem': r'\\b(connection|network|timeout)\\b'},\n",
    "        'hpc': {'system_failure': r'\\b(node|job|task|error|failure)\\b', 'performance_issue': r'\\b(slow|performance|latency|timeout)\\b', 'network_problem': r'\\b(network|connection|communication)\\b', 'hardware_issue': r'\\b(hardware|memory|disk|cpu)\\b'},\n",
    "        'proxifier': {'network_anomaly': r'\\b(connection|proxy|tunnel|network)\\b'},\n",
    "        'zookeeper': {'system_failure': r'\\b(error|exception|failure)\\b', 'performance_issue': r'\\b(timeout|slow|latency)\\b', 'network_problem': r'\\b(connection|network|socket)\\b', 'config_error': r'\\b(config|configuration|property)\\b'}\n",
    "    }\n",
    "    return patterns.get(source_type.lower(), {})\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def shannon_entropy_udf(text):\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    s = str(text)\n",
    "    if len(s) == 0:\n",
    "        return 0.0\n",
    "    cs = set(s)\n",
    "    probs = [s.count(c) / len(s) for c in cs]\n",
    "    return float(-sum(p * math.log2(p) for p in probs if p > 0))\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def repeated_words_udf(text):\n",
    "    if text is None:\n",
    "        return 0\n",
    "    words = str(text).lower().split()\n",
    "    if len(words) <= 1:\n",
    "        return 0\n",
    "    counts = Counter(words)\n",
    "    return int(sum(1 for v in counts.values() if v > 1))\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def repeated_chars_udf(text):\n",
    "    if text is None:\n",
    "        return 0\n",
    "    s = str(text)\n",
    "    cnt = 0\n",
    "    for i in range(len(s)-1):\n",
    "        if s[i] == s[i+1]:\n",
    "            cnt += 1\n",
    "    return int(cnt)\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def unique_chars_udf(text):\n",
    "    if text is None:\n",
    "        return 0\n",
    "    return int(len(set(str(text))))\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def special_char_ratio_udf(text):\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    s = str(text)\n",
    "    total = len(s)\n",
    "    specials = len([ch for ch in s if not ch.isalnum() and not ch.isspace()])\n",
    "    return float(specials / (total + 1))\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def number_ratio_udf(text):\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    s = str(text)\n",
    "    total = len(s)\n",
    "    digits = sum(ch.isdigit() for ch in s)\n",
    "    return float(digits / (total + 1))\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def uppercase_ratio_udf(text):\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    s = str(text)\n",
    "    total = len(s)\n",
    "    ups = sum(ch.isupper() for ch in s)\n",
    "    return float(ups / (total + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a375ab4",
   "metadata": {},
   "source": [
    "Spark Feature Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08dfa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spark_text_features(df_spark, source_type, content_col):\n",
    "    df_spark = df_spark.withColumn('msg_length', F.length(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('msg_word_count', F.size(F.split(F.col(content_col), ' ')))\n",
    "    df_spark = df_spark.withColumn('msg_unique_chars', unique_chars_udf(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('msg_entropy', shannon_entropy_udf(F.col(content_col)))\n",
    "    patterns = get_error_patterns_by_source(source_type)\n",
    "    for name, pattern in patterns.items():\n",
    "        df_spark = df_spark.withColumn(f'has_{name}', F.when(F.col(content_col).rlike(pattern), 1).otherwise(0))\n",
    "    df_spark = df_spark.withColumn('special_char_ratio', special_char_ratio_udf(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('number_ratio', number_ratio_udf(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('uppercase_ratio', uppercase_ratio_udf(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('repeated_words', repeated_words_udf(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('repeated_chars', repeated_chars_udf(F.col(content_col)))\n",
    "    return df_spark\n",
    "\n",
    "def add_spark_temporal_features(df_spark):\n",
    "    df_spark = df_spark.withColumn('hour', F.hour('timestamp')).withColumn('day_of_week', F.dayofweek('timestamp')).withColumn('day_of_month', F.dayofmonth('timestamp')).withColumn('month', F.month('timestamp'))\n",
    "    df_spark = df_spark.withColumn('is_weekend', F.when(F.col('day_of_week').isin([1,7]), 1).otherwise(0))\n",
    "    df_spark = df_spark.withColumn('is_business_hours', F.when(F.col('hour').between(9,17), 1).otherwise(0))\n",
    "    df_spark = df_spark.withColumn('is_night', F.when(F.col('hour').between(0,6), 1).otherwise(0))\n",
    "    window = Window.orderBy('timestamp')\n",
    "    df_spark = df_spark.withColumn('prev_timestamp', F.lag('timestamp', 1).over(window))\n",
    "    df_spark = df_spark.withColumn('time_diff_seconds', F.when(F.col('prev_timestamp').isNotNull(), F.unix_timestamp('timestamp') - F.unix_timestamp('prev_timestamp')).otherwise(0))\n",
    "    df_spark = df_spark.withColumn('is_burst', (F.col('time_diff_seconds') < 1).cast('int'))\n",
    "    df_spark = df_spark.withColumn('is_isolated', (F.col('time_diff_seconds') > 300).cast('int'))\n",
    "    w1 = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-60, 0)\n",
    "    w5 = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-300, 0)\n",
    "    w15 = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-900, 0)\n",
    "    w1h = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-3600, 0)\n",
    "    w6h = Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-21600, 0)\n",
    "    df_spark = df_spark.withColumn('log_count_1min', F.count('*').over(w1))\n",
    "    df_spark = df_spark.withColumn('log_count_5min', F.count('*').over(w5))\n",
    "    df_spark = df_spark.withColumn('log_count_15min', F.count('*').over(w15))\n",
    "    df_spark = df_spark.withColumn('log_count_1H', F.count('*').over(w1h))\n",
    "    df_spark = df_spark.withColumn('log_count_6H', F.count('*').over(w6h))\n",
    "    return df_spark\n",
    "\n",
    "def add_spark_stat_features(df_spark, content_col):\n",
    "    df_spark = df_spark.withColumn('content_length', F.length(F.col(content_col)))\n",
    "    df_spark = df_spark.withColumn('word_count', F.size(F.split(F.col(content_col), ' ')))\n",
    "    w10 = Window.orderBy('timestamp').rowsBetween(-9, 0)\n",
    "    df_spark = df_spark.withColumn('content_length_mean_10', F.avg('content_length').over(w10))\n",
    "    df_spark = df_spark.withColumn('content_length_std_10', F.stddev('content_length').over(w10))\n",
    "    df_spark = df_spark.withColumn('time_diff_mean_10', F.avg('time_diff_seconds').over(w10))\n",
    "    df_spark = df_spark.withColumn('time_diff_std_10', F.stddev('time_diff_seconds').over(w10))\n",
    "    hour_counts = df_spark.groupBy('hour').count().withColumnRenamed('count', 'hour_frequency')\n",
    "    df_spark = df_spark.join(hour_counts, on='hour', how='left')\n",
    "    has_cols = [c for c in df_spark.columns if c.startswith('has_')]\n",
    "    if has_cols:\n",
    "        for wname, wspec in [('1min', Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-60, 0)),\n",
    "                             ('5min', Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-300, 0)),\n",
    "                             ('15min', Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-900, 0)),\n",
    "                             ('1H', Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-3600, 0)),\n",
    "                             ('6H', Window.orderBy(F.col('timestamp').cast('long')).rangeBetween(-21600, 0))]:\n",
    "            s = None\n",
    "            for hc in has_cols:\n",
    "                colsum = F.sum(F.col(hc)).over(wspec)\n",
    "                s = colsum if s is None else s + colsum\n",
    "            df_spark = df_spark.withColumn(f'error_density_{wname}', s)\n",
    "    return df_spark\n",
    "\n",
    "def create_imbalance_aware_template_features(template_data, labels):\n",
    "    enhanced_features = []\n",
    "    for log_source, data_dict in template_data.items():\n",
    "        templates = data_dict['templates']\n",
    "        template_ids = data_dict['template_ids']\n",
    "        template_counts = Counter(template_ids)\n",
    "        total = len(template_ids)\n",
    "        for i, template_id in enumerate(template_ids):\n",
    "            if template_id == -1:\n",
    "                enhanced_features.append([0] * 10)\n",
    "                continue\n",
    "            frequency = template_counts[template_id] / total\n",
    "            rarity = 1.0 / (frequency + 1e-6)\n",
    "            template_text = templates[template_id]['template']\n",
    "            length = len(template_text.split())\n",
    "            n_wildcards = sum([template_text.count(w) for w in ['<NUM>', '<IP>', '<PATH>', '<UUID>']])\n",
    "            class_probs = np.array(templates[template_id]['class_dist']) / (templates[template_id]['count'] + 1e-6)\n",
    "            normal_score = class_probs[0] if len(class_probs) > 0 else 0\n",
    "            anomaly_score = class_probs[1] if len(class_probs) > 1 else 0\n",
    "            complexity_score = length * n_wildcards / (frequency + 1e-6)\n",
    "            uniqueness_score = rarity * (1 - np.max(class_probs) if len(class_probs) else 0)\n",
    "            features = [rarity, length, n_wildcards, frequency, normal_score, anomaly_score, complexity_score, uniqueness_score, *(class_probs.tolist() if len(class_probs) >= 2 else [0,0])]\n",
    "            enhanced_features.append(features)\n",
    "    return np.array(enhanced_features)\n",
    "\n",
    "def analyze_class_imbalance(df_pandas):\n",
    "    if 'AnomalyLabel' not in df_pandas.columns:\n",
    "        return None\n",
    "    analysis = {}\n",
    "    label_counts = df_pandas['AnomalyLabel'].value_counts().sort_index()\n",
    "    total_samples = len(df_pandas)\n",
    "    analysis['class_distribution'] = {}\n",
    "    analysis['class_percentages'] = {}\n",
    "    for label in range(2):\n",
    "        count = label_counts.get(label, 0)\n",
    "        analysis['class_distribution'][label] = int(count)\n",
    "        analysis['class_percentages'][label] = float((count / total_samples) * 100 if total_samples else 0)\n",
    "    present_classes = [label for label in range(2) if label_counts.get(label, 0) > 0]\n",
    "    if len(present_classes) > 1:\n",
    "        counts = [label_counts[label] for label in present_classes]\n",
    "        analysis['imbalance_ratio'] = float(max(counts) / min(counts))\n",
    "        analysis['minority_classes'] = [int(label) for label in present_classes if label_counts[label] < total_samples * 0.05]\n",
    "        analysis['extreme_minority'] = [int(label) for label in present_classes if label_counts[label] < total_samples * 0.01]\n",
    "    else:\n",
    "        analysis['imbalance_ratio'] = 1.0\n",
    "        analysis['minority_classes'] = []\n",
    "        analysis['extreme_minority'] = []\n",
    "    return analysis\n",
    "\n",
    "def select_features_for_imbalanced_classes(X, y, feature_names, top_k=200):\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k=min(top_k, X.shape[1]))\n",
    "    mi_selector.fit(X, y)\n",
    "    mi_scores = mi_selector.scores_\n",
    "    rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    rf_importance = rf.feature_importances_\n",
    "    mi_norm = mi_scores / (np.max(mi_scores) + 1e-9)\n",
    "    rf_norm = rf_importance / (np.max(rf_importance) + 1e-9)\n",
    "    combined_scores = 0.6 * mi_norm + 0.4 * rf_norm\n",
    "    top_indices = np.argsort(combined_scores)[-min(top_k, X.shape[1]):]\n",
    "    selected_features = [feature_names[i] for i in top_indices] if feature_names else list(range(min(top_k, X.shape[1])))\n",
    "    return top_indices, selected_features, combined_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919d9c6",
   "metadata": {},
   "source": [
    "MAIN PROCESSING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d82b3333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Processing: Android_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,974 (98.70%)\n",
      "  1 (anomaly): 26 (1.30%)\n",
      "Converted to Pandas: (2000, 52)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 75.92:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1974 (98.70%)\n",
      "  1 (anomaly): 26 (1.30%)\n",
      "\n",
      "============================================================\n",
      "Processing: Apache_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,429 (71.45%)\n",
      "  1 (anomaly): 571 (28.55%)\n",
      "Converted to Pandas: (2000, 48)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 2.50:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1429 (71.45%)\n",
      "  1 (anomaly): 571 (28.55%)\n",
      "\n",
      "============================================================\n",
      "Processing: BGL_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,832 (91.60%)\n",
      "  1 (anomaly): 168 (8.40%)\n",
      "Converted to Pandas: (2000, 54)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 10.90:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1832 (91.60%)\n",
      "  1 (anomaly): 168 (8.40%)\n",
      "\n",
      "============================================================\n",
      "Processing: Hadoop_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 690 (34.50%)\n",
      "  1 (anomaly): 1,310 (65.50%)\n",
      "Converted to Pandas: (2000, 51)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 1.90:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 690 (34.50%)\n",
      "  1 (anomaly): 1310 (65.50%)\n",
      "\n",
      "============================================================\n",
      "Processing: HDFS_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 2,000 (100.00%)\n",
      "Converted to Pandas: (2000, 51)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 1/2\n",
      "Imbalance ratio: 1.00:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 5 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "Label distribution:\n",
      "  0 (normal): 2000 (100.00%)\n",
      "\n",
      "============================================================\n",
      "Processing: HealthApp_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,989 (99.45%)\n",
      "  1 (anomaly): 11 (0.55%)\n",
      "Converted to Pandas: (2000, 49)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 180.82:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1989 (99.45%)\n",
      "  1 (anomaly): 11 (0.55%)\n",
      "\n",
      "============================================================\n",
      "Processing: HPC_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,110 (55.50%)\n",
      "  1 (anomaly): 890 (44.50%)\n",
      "Converted to Pandas: (2000, 52)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 1.25:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1110 (55.50%)\n",
      "  1 (anomaly): 890 (44.50%)\n",
      "\n",
      "============================================================\n",
      "Processing: Linux_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 94 (4.70%)\n",
      "  1 (anomaly): 1,906 (95.30%)\n",
      "Converted to Pandas: (2000, 53)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 20.28:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 94 (4.70%)\n",
      "  1 (anomaly): 1906 (95.30%)\n",
      "\n",
      "============================================================\n",
      "Processing: Mac_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,567 (78.35%)\n",
      "  1 (anomaly): 433 (21.65%)\n",
      "Converted to Pandas: (2000, 54)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 3.62:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1567 (78.35%)\n",
      "  1 (anomaly): 433 (21.65%)\n",
      "\n",
      "============================================================\n",
      "Processing: OpenSSH_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  1 (anomaly): 2,000 (100.00%)\n",
      "Converted to Pandas: (2000, 51)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 1/2\n",
      "Imbalance ratio: 1.00:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 5 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "Label distribution:\n",
      "  1 (anomaly): 2000 (100.00%)\n",
      "\n",
      "============================================================\n",
      "Processing: OpenStack_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 2,000 (100.00%)\n",
      "Converted to Pandas: (2000, 53)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 1/2\n",
      "Imbalance ratio: 1.00:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 5 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "Label distribution:\n",
      "  0 (normal): 2000 (100.00%)\n",
      "\n",
      "============================================================\n",
      "Processing: Proxifier_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,903 (95.15%)\n",
      "  1 (anomaly): 97 (4.85%)\n",
      "Converted to Pandas: (2000, 48)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 19.62:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1903 (95.15%)\n",
      "  1 (anomaly): 97 (4.85%)\n",
      "\n",
      "============================================================\n",
      "Processing: Spark_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,992 (99.60%)\n",
      "  1 (anomaly): 8 (0.40%)\n",
      "Converted to Pandas: (2000, 50)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 249.00:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1992 (99.60%)\n",
      "  1 (anomaly): 8 (0.40%)\n",
      "\n",
      "============================================================\n",
      "Processing: Thunderbird_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,805 (90.25%)\n",
      "  1 (anomaly): 195 (9.75%)\n",
      "Converted to Pandas: (2000, 56)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 9.26:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1805 (90.25%)\n",
      "  1 (anomaly): 195 (9.75%)\n",
      "\n",
      "============================================================\n",
      "Processing: Windows_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,209 (60.45%)\n",
      "  1 (anomaly): 791 (39.55%)\n",
      "Converted to Pandas: (2000, 50)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 1.53:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1209 (60.45%)\n",
      "  1 (anomaly): 791 (39.55%)\n",
      "\n",
      "============================================================\n",
      "Processing: Zookeeper_2k\n",
      "============================================================\n",
      "Total: 2,000\n",
      "Label distribution:\n",
      "  0 (normal): 1,486 (74.30%)\n",
      "  1 (anomaly): 514 (25.70%)\n",
      "Converted to Pandas: (2000, 52)\n",
      "IMBALANCE ANALYSIS:\n",
      "Classes present: 2/2\n",
      "Imbalance ratio: 2.89:1\n",
      "Enhanced Template Extraction\n",
      "Enhanced template features shape: (2000, 10)\n",
      "Enhanced BERT Feature Generation\n",
      "  Processed 0/2000\n",
      "  Processed 160/2000\n",
      "  Processed 320/2000\n",
      "  Processed 480/2000\n",
      "  Processed 640/2000\n",
      "  Processed 800/2000\n",
      "  Processed 960/2000\n",
      "  Processed 1120/2000\n",
      "  Processed 1280/2000\n",
      "  Processed 1440/2000\n",
      "  Processed 1600/2000\n",
      "  Processed 1760/2000\n",
      "  Processed 1920/2000\n",
      "BERT embeddings shape: (2000, 768)\n",
      "Total BERT-based features: 803\n",
      "Created 7 enhanced feature variants:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "Label distribution:\n",
      "  0 (normal): 1486 (74.30%)\n",
      "  1 (anomaly): 514 (25.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_features_data = {}\n",
    "template_data = {}\n",
    "bert_features_data = {}\n",
    "hybrid_features_data = {}\n",
    "\n",
    "for log_source in PROJECT_CONFIG['log_sources']:\n",
    "    if log_source not in dataset_registry:\n",
    "        continue\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing: {log_source}\")\n",
    "    print(\"=\"*60)\n",
    "    file_path = dataset_registry[log_source]['file_path']\n",
    "    df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    content_col = None\n",
    "    for col in ['Content', 'content', 'Message', 'message', 'Text', 'text']:\n",
    "        if col in df_spark.columns:\n",
    "            content_col = col\n",
    "            break\n",
    "    if content_col is None:\n",
    "        print(\"No content column found, skipping\")\n",
    "        continue\n",
    "    if 'timestamp_dt' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.col('timestamp_dt').cast(TimestampType()))\n",
    "    elif 'timestamp_normalized' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('timestamp', F.to_timestamp('timestamp_normalized'))\n",
    "    else:\n",
    "        print(\"No timestamp column found, skipping\")\n",
    "        continue\n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', F.col('AnomalyLabel').cast(IntegerType()))\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', F.when(F.col('AnomalyLabel').isNull(), 0).when(F.col('AnomalyLabel') < 0, 0).when(F.col('AnomalyLabel') > 6, 0).otherwise(F.col('AnomalyLabel')))\n",
    "        df_spark = df_spark.withColumn('AnomalyLabel', F.when(F.col('AnomalyLabel') > 0, 1).otherwise(0))\n",
    "    df_spark = add_spark_text_features(df_spark, log_source, content_col)\n",
    "    df_spark = add_spark_temporal_features(df_spark)\n",
    "    df_spark = add_spark_stat_features(df_spark, content_col)\n",
    "    window_10 = Window.orderBy('timestamp').rowsBetween(-9, 0)\n",
    "    df_spark = df_spark.withColumn('is_off_hours', ((F.col('hour') < 6) | (F.col('hour') > 22)).cast('int'))\n",
    "    df_spark = df_spark.withColumn('is_weekend_night', (F.col('is_weekend').cast('boolean') & F.col('is_night').cast('boolean')).cast('int'))\n",
    "    total_count = df_spark.count()\n",
    "    if 'AnomalyLabel' in df_spark.columns:\n",
    "        label_dist = df_spark.groupBy('AnomalyLabel').count().orderBy('AnomalyLabel').collect()\n",
    "        print(f\"Total: {total_count:,}\")\n",
    "        print(\"Label distribution:\")\n",
    "        for row in label_dist:\n",
    "            print(f\"  {int(row['AnomalyLabel'])} ({PROJECT_CONFIG['label_map'][int(row['AnomalyLabel'])]}): {int(row['count']):,} ({row['count']/total_count*100:.2f}%)\")\n",
    "    max_samples = 5000\n",
    "    if total_count > max_samples:\n",
    "        frac = max_samples / total_count\n",
    "        df_spark_sampled = df_spark.sample(withReplacement=False, fraction=frac, seed=RANDOM_SEED)\n",
    "        sampled_count = df_spark_sampled.count()\n",
    "        print(f\"Sampled {sampled_count} rows\")\n",
    "    else:\n",
    "        df_spark_sampled = df_spark\n",
    "    df_pandas = df_spark_sampled.toPandas()\n",
    "    print(f\"Converted to Pandas: {df_pandas.shape}\")\n",
    "    if 'AnomalyLabel' in df_pandas.columns:\n",
    "        imbalance_analysis = analyze_class_imbalance(df_pandas)\n",
    "        if imbalance_analysis:\n",
    "            print(f\"IMBALANCE ANALYSIS:\")\n",
    "            present = len([c for c in range(2) if imbalance_analysis['class_distribution'][c] > 0])\n",
    "            print(f\"Classes present: {present}/2\")\n",
    "            print(f\"Imbalance ratio: {imbalance_analysis['imbalance_ratio']:.2f}:1\")\n",
    "    else:\n",
    "        imbalance_analysis = None\n",
    "    print(\"Enhanced Template Extraction\")\n",
    "    source_config = drain_configs.get(log_source, drain_configs['default'])\n",
    "    drain_config = TemplateMinerConfig()\n",
    "    drain_config.drain_sim_th = source_config['sim_th']\n",
    "    drain_config.drain_depth = source_config['depth']\n",
    "    drain_config.drain_max_children = 100\n",
    "    drain_config.masking_instructions = [\n",
    "        MaskingInstruction(r'\\d+', \"<NUM>\"),\n",
    "        MaskingInstruction(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', \"<UUID>\"),\n",
    "        MaskingInstruction(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', \"<IP>\"),\n",
    "        MaskingInstruction(r'/[^\\s]*', \"<PATH>\"),\n",
    "        MaskingInstruction(r'\\b[0-9a-fA-F]{8,}\\b', \"<HEX>\"),\n",
    "        MaskingInstruction(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', \"<DATE>\"),\n",
    "        MaskingInstruction(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', \"<TIME>\")\n",
    "    ]\n",
    "    template_miner = TemplateMiner(config=drain_config)\n",
    "    templates = {}\n",
    "    template_ids = []\n",
    "    labels_np = df_pandas['AnomalyLabel'].values if 'AnomalyLabel' in df_pandas.columns else None\n",
    "    texts_list = df_pandas[content_col].fillna(\"\").astype(str).tolist()\n",
    "    for idx, content in enumerate(texts_list):\n",
    "        if content.strip() == \"\":\n",
    "            template_ids.append(-1)\n",
    "            continue\n",
    "        result = template_miner.add_log_message(content.strip())\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        if tid not in templates:\n",
    "            templates[tid] = {'template': result[\"template_mined\"], 'count': 1, 'class_dist': [0] * 2, 'anomaly_score': 0.0, 'normal_score': 0.0}\n",
    "        else:\n",
    "            templates[tid]['count'] += 1\n",
    "        if labels_np is not None:\n",
    "            lbl = int(labels_np[idx])\n",
    "            templates[tid]['class_dist'][lbl] += 1\n",
    "    for tid, info in templates.items():\n",
    "        probs = np.array(info['class_dist']) / (info['count'] + 1e-6)\n",
    "        info['normal_score'] = float(probs[0] if len(probs) > 0 else 0)\n",
    "        info['anomaly_score'] = float(probs[1] if len(probs) > 1 else 0)\n",
    "    enhanced_template_features = create_imbalance_aware_template_features({log_source: {'templates': templates, 'template_ids': template_ids}}, labels_np)\n",
    "    template_data[log_source] = {'templates': templates, 'template_ids': template_ids, 'enhanced_features': enhanced_template_features}\n",
    "    print(f\"Enhanced template features shape: {enhanced_template_features.shape}\")\n",
    "    print(\"Enhanced BERT Feature Generation\")\n",
    "    texts = texts_list\n",
    "    labels = labels_np\n",
    "    all_embeddings = []\n",
    "    batch_size = 16\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encoded = tokenizer(batch_texts, padding=True, truncation=True, max_length=PROJECT_CONFIG['max_sequence_length'], return_tensors='pt').to(device)\n",
    "            outputs = bert_model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f\"  Processed {i}/{len(texts)}\")\n",
    "    if len(all_embeddings) > 0:\n",
    "        bert_embeddings = np.vstack(all_embeddings)\n",
    "    else:\n",
    "        bert_embeddings = np.zeros((len(texts), bert_model.config.hidden_size))\n",
    "    print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "    window_sizes = [5, 10, 20, 50]\n",
    "    statistical_features = []\n",
    "    for i in range(len(bert_embeddings)):\n",
    "        sample_stats = []\n",
    "        for window_size in window_sizes:\n",
    "            start = max(0, i - window_size)\n",
    "            window = bert_embeddings[start:i+1]\n",
    "            mean_emb = np.mean(window, axis=0)\n",
    "            std_emb = np.std(window, axis=0)\n",
    "            distance_from_mean = float(np.linalg.norm(bert_embeddings[i] - mean_emb))\n",
    "            avg_std = float(np.mean(std_emb))\n",
    "            if len(window) > 1:\n",
    "                distances = [np.linalg.norm(bert_embeddings[i] - w) for w in window]\n",
    "                min_dist = float(np.min(distances))\n",
    "                max_dist = float(np.max(distances))\n",
    "                median_dist = float(np.median(distances))\n",
    "                q75, q25 = np.percentile(distances, [75, 25])\n",
    "                iqr = q75 - q25\n",
    "                outlier_threshold = q75 + 1.5 * iqr\n",
    "                is_outlier = int(distance_from_mean > outlier_threshold)\n",
    "            else:\n",
    "                min_dist = 0.0\n",
    "                max_dist = 0.0\n",
    "                median_dist = 0.0\n",
    "                is_outlier = 0\n",
    "            cosine_sim_mean = float(np.dot(bert_embeddings[i], mean_emb) / (np.linalg.norm(bert_embeddings[i]) * np.linalg.norm(mean_emb) + 1e-8))\n",
    "            sample_stats.extend([distance_from_mean, avg_std, min_dist, max_dist, median_dist, is_outlier, cosine_sim_mean])\n",
    "        if labels is not None:\n",
    "            current_label = labels[i]\n",
    "            start = max(0, i - window_sizes[-1])\n",
    "            window_labels = labels[start:i+1] if i > 0 else [current_label]\n",
    "            same_class_ratio = float(sum(1 for l in window_labels if l == current_label) / len(window_labels))\n",
    "            minority_class_indicator = int(current_label == 1)\n",
    "            sample_stats.extend([same_class_ratio, minority_class_indicator])\n",
    "        else:\n",
    "            sample_stats.extend([0.0, 0])\n",
    "        statistical_features.append(sample_stats)\n",
    "    statistical_features = np.array(statistical_features)\n",
    "    sentence_features = []\n",
    "    for i, text in enumerate(texts):\n",
    "        s = text if text is not None else \"\"\n",
    "        text_len = len(s)\n",
    "        word_count = len(s.split())\n",
    "        emb = bert_embeddings[i]\n",
    "        emb_magnitude = float(np.linalg.norm(emb))\n",
    "        emb_sparsity = float(np.sum(np.abs(emb) < 0.01) / len(emb))\n",
    "        emb_norm = np.abs(emb) / (np.sum(np.abs(emb)) + 1e-8)\n",
    "        emb_entropy = float(-np.sum(emb_norm * np.log(emb_norm + 1e-8)))\n",
    "        sentence_features.append([text_len, word_count, emb_magnitude, emb_sparsity, emb_entropy])\n",
    "    sentence_features = np.array(sentence_features)\n",
    "    bert_features_data[log_source] = {'embeddings': bert_embeddings, 'statistical_features': statistical_features, 'sentence_features': sentence_features}\n",
    "    print(f\"Total BERT-based features: {bert_embeddings.shape[1] + statistical_features.shape[1] + sentence_features.shape[1]}\")\n",
    "    dfp = df_pandas\n",
    "    temporal_cols = ['hour', 'day_of_week', 'is_weekend', 'is_business_hours', 'time_diff_seconds', 'log_count_1min', 'log_count_5min', 'log_count_15min', 'log_count_1H', 'log_count_6H', 'is_night', 'is_off_hours', 'is_weekend_night', 'is_burst', 'is_isolated']\n",
    "    statistical_cols = ['content_length', 'word_count', 'content_length_mean_10', 'content_length_std_10', 'time_diff_mean_10', 'time_diff_std_10', 'hour_frequency']\n",
    "    anomaly_cols = [c for c in dfp.columns if c.startswith('has_')]\n",
    "    complexity_cols = ['msg_length', 'msg_word_count', 'msg_unique_chars', 'msg_entropy', 'special_char_ratio', 'number_ratio', 'uppercase_ratio', 'repeated_words', 'repeated_chars']\n",
    "    rolling_cols = [c for c in dfp.columns if any(x in c for x in ['_mean_', '_std_', '_zscore_', '_outlier_'])]\n",
    "    time_cols = [c for c in dfp.columns if 'log_count_' in c or 'error_density_' in c]\n",
    "    available_temporal = [c for c in temporal_cols if c in dfp.columns]\n",
    "    available_statistical = [c for c in statistical_cols if c in dfp.columns]\n",
    "    available_anomaly = [c for c in anomaly_cols if c in dfp.columns]\n",
    "    available_complexity = [c for c in complexity_cols if c in dfp.columns]\n",
    "    available_rolling = [c for c in rolling_cols if c in dfp.columns]\n",
    "    available_time = [c for c in time_cols if c in dfp.columns]\n",
    "    temporal_features = dfp[available_temporal].fillna(0).values if available_temporal else None\n",
    "    statistical_num_features = dfp[available_statistical].fillna(0).values if available_statistical else None\n",
    "    anomaly_features = dfp[available_anomaly].fillna(0).values if available_anomaly else None\n",
    "    complexity_features = dfp[available_complexity].fillna(0).values if available_complexity else None\n",
    "    rolling_features = dfp[available_rolling].fillna(0).values if available_rolling else None\n",
    "    time_features = dfp[available_time].fillna(0).values if available_time else None\n",
    "    feature_variants = {}\n",
    "    feature_variants['bert_only'] = bert_embeddings\n",
    "    feature_variants['bert_enhanced'] = np.hstack([bert_embeddings, statistical_features, sentence_features])\n",
    "    feature_variants['template_enhanced'] = enhanced_template_features\n",
    "    imbalance_components = [bert_embeddings, statistical_features, enhanced_template_features]\n",
    "    if anomaly_features is not None:\n",
    "        imbalance_components.append(anomaly_features)\n",
    "        feature_variants['anomaly_focused'] = np.hstack([bert_embeddings, anomaly_features, enhanced_template_features])\n",
    "    if complexity_features is not None:\n",
    "        imbalance_components.append(complexity_features)\n",
    "    if temporal_features is not None:\n",
    "        imbalance_components.append(temporal_features)\n",
    "    if statistical_num_features is not None:\n",
    "        imbalance_components.append(statistical_num_features)\n",
    "    if rolling_features is not None:\n",
    "        imbalance_components.append(rolling_features)\n",
    "    if time_features is not None:\n",
    "        imbalance_components.append(time_features)\n",
    "    feature_variants['imbalance_aware_full'] = np.hstack(imbalance_components)\n",
    "    if sentence_features is not None:\n",
    "        sentence_components = [bert_embeddings, sentence_features, enhanced_template_features]\n",
    "        if complexity_features is not None:\n",
    "            sentence_components.append(complexity_features)\n",
    "        feature_variants['sentence_focused'] = np.hstack(sentence_components)\n",
    "    labels_arr = dfp['AnomalyLabel'].values if 'AnomalyLabel' in dfp.columns else None\n",
    "    feature_selection_info = None\n",
    "    if labels_arr is not None and len(np.unique(labels_arr)) > 1:\n",
    "        feature_names = []\n",
    "        feature_names.extend([f'bert_{i}' for i in range(bert_embeddings.shape[1])])\n",
    "        feature_names.extend([f'bert_stat_{i}' for i in range(statistical_features.shape[1])])\n",
    "        feature_names.extend([f'template_{i}' for i in range(enhanced_template_features.shape[1])])\n",
    "        feature_names.extend(available_anomaly)\n",
    "        feature_names.extend(available_complexity)\n",
    "        feature_names.extend(available_temporal)\n",
    "        feature_names.extend(available_statistical)\n",
    "        feature_names.extend(available_rolling)\n",
    "        feature_names.extend(available_time)\n",
    "        full_features = feature_variants['imbalance_aware_full']\n",
    "        scaler = SKStandardScaler()\n",
    "        full_features_scaled = scaler.fit_transform(full_features)\n",
    "        feature_variants['imbalance_aware_full_scaled'] = full_features_scaled\n",
    "        top_indices, selected_features, feature_scores = select_features_for_imbalanced_classes(full_features_scaled, labels_arr, feature_names, top_k=min(200, full_features_scaled.shape[1]))\n",
    "        feature_variants['selected_imbalanced'] = full_features_scaled[:, top_indices]\n",
    "        feature_selection_info = {'selected_indices': top_indices, 'selected_features': selected_features, 'feature_scores': feature_scores, 'total_features': full_features_scaled.shape[1]}\n",
    "    hybrid_features_data[log_source] = {'feature_variants': feature_variants, 'labels': labels_arr, 'texts': texts, 'feature_selection_info': feature_selection_info, 'imbalance_analysis': imbalance_analysis}\n",
    "    pyspark_features_data[log_source] = {'imbalance_analysis': imbalance_analysis, 'total_count': int(total_count), 'content_col': content_col}\n",
    "    print(f\"Created {len(feature_variants)} enhanced feature variants:\")\n",
    "    for variant_name, features in feature_variants.items():\n",
    "        print(f\"  - {variant_name}: {features.shape[1]} features\")\n",
    "    if labels_arr is not None:\n",
    "        unique, counts = np.unique(labels_arr, return_counts=True)\n",
    "        print(\"Label distribution:\")\n",
    "        for lbl, cnt in zip(unique, counts):\n",
    "            print(f\"  {int(lbl)} ({PROJECT_CONFIG['label_map'][int(lbl)]}): {int(cnt)} ({cnt/len(labels_arr)*100:.2f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a31c61",
   "metadata": {},
   "source": [
    "SAVE FEATURESETS AND CROSS-SOURCE SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc36f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_imbalanced_features.pkl\n"
     ]
    }
   ],
   "source": [
    "features_save_path = FEATURES_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "with open(features_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'hybrid_features_data': hybrid_features_data,\n",
    "        'template_data': template_data,\n",
    "        'bert_features_data': bert_features_data,\n",
    "        'pyspark_features_data': pyspark_features_data,\n",
    "        'feature_types': list(hybrid_features_data[list(hybrid_features_data.keys())[0]]['feature_variants'].keys()) if hybrid_features_data else [],\n",
    "        'config': PROJECT_CONFIG,\n",
    "        'enhancement_info': {\n",
    "            'anomaly_patterns_added': True,\n",
    "            'temporal_features_enhanced': True,\n",
    "            'statistical_features_enhanced': True,\n",
    "            'template_features_enhanced': True,\n",
    "            'bert_features_enhanced': True,\n",
    "            'feature_selection_applied': True,\n",
    "            'imbalance_analysis_included': True\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }, f)\n",
    "\n",
    "print(f\"Saved: {features_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b20c4e",
   "metadata": {},
   "source": [
    "CROSS-SOURCE TRAIN/TEST SPLIT CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "598338b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_cross_source_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "cross_source_splits = []\n",
    "for test_source in hybrid_features_data.keys():\n",
    "    train_sources = [s for s in hybrid_features_data.keys() if s != test_source]\n",
    "    if hybrid_features_data[test_source]['labels'] is None:\n",
    "        continue\n",
    "    test_samples = len(hybrid_features_data[test_source]['labels'])\n",
    "    train_samples = sum(len(hybrid_features_data[s]['labels']) for s in train_sources if hybrid_features_data[s]['labels'] is not None)\n",
    "    test_imbalance = hybrid_features_data[test_source]['imbalance_analysis']\n",
    "    train_label_counts = Counter()\n",
    "    for s in train_sources:\n",
    "        if hybrid_features_data[s]['labels'] is not None:\n",
    "            for label in hybrid_features_data[s]['labels']:\n",
    "                train_label_counts[int(label)] += 1\n",
    "    train_imbalance_ratio = float(max(train_label_counts.values()) / min(train_label_counts.values())) if train_label_counts and min(train_label_counts.values()) > 0 else 1.0\n",
    "    cross_source_splits.append({'test_source': test_source, 'train_sources': train_sources, 'test_samples': int(test_samples), 'train_samples': int(train_samples), 'test_imbalance_analysis': test_imbalance, 'train_imbalance_ratio': train_imbalance_ratio, 'train_label_distribution': dict(train_label_counts)})\n",
    "\n",
    "splits_save_path = FEATURES_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "with open(splits_save_path, 'wb') as f:\n",
    "    pickle.dump({'splits': cross_source_splits}, f)\n",
    "\n",
    "print(f\"Saved: {splits_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b3e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16 log sources with enhanced features\n",
      "Enhanced Feature Variants Created:\n",
      "  - bert_only: 768 features\n",
      "  - bert_enhanced: 803 features\n",
      "  - template_enhanced: 10 features\n",
      "  - imbalance_aware_full: 848 features\n",
      "  - sentence_focused: 792 features\n",
      "  - imbalance_aware_full_scaled: 848 features\n",
      "  - selected_imbalanced: 200 features\n",
      "  - Sources with extreme imbalance (>100:1): 2\n",
      "  - Sources with high imbalance (>10:1): 4\n",
      "Class Coverage Across Sources:\n",
      "  OK Class 0 (normal): 15/16 sources (93.8%)\n",
      "  OK Class 1 (anomaly): 14/16 sources (87.5%)\n",
      "Files Saved:\n",
      "  - Enhanced features: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_imbalanced_features.pkl\n",
      "  - Enhanced splits: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\features\\enhanced_cross_source_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "total_sources = len(hybrid_features_data)\n",
    "print(f\"Processed {total_sources} log sources with enhanced features\")\n",
    "if hybrid_features_data:\n",
    "    sample_source = list(hybrid_features_data.keys())[0]\n",
    "    feature_variants = hybrid_features_data[sample_source]['feature_variants']\n",
    "    print(\"Enhanced Feature Variants Created:\")\n",
    "    for variant_name, features in feature_variants.items():\n",
    "        print(f\"  - {variant_name}: {features.shape[1]} features\")\n",
    "    extreme_imbalance_sources = []\n",
    "    high_imbalance_sources = []\n",
    "    minority_class_coverage = {i: 0 for i in range(2)}\n",
    "    for source, data in hybrid_features_data.items():\n",
    "        if data['imbalance_analysis']:\n",
    "            ratio = data['imbalance_analysis']['imbalance_ratio']\n",
    "            if ratio > 100:\n",
    "                extreme_imbalance_sources.append(source)\n",
    "            elif ratio > 10:\n",
    "                high_imbalance_sources.append(source)\n",
    "            for class_id, count in data['imbalance_analysis']['class_distribution'].items():\n",
    "                if count > 0:\n",
    "                    minority_class_coverage[int(class_id)] += 1\n",
    "    print(f\"  - Sources with extreme imbalance (>100:1): {len(extreme_imbalance_sources)}\")\n",
    "    print(f\"  - Sources with high imbalance (>10:1): {len(high_imbalance_sources)}\")\n",
    "    print(\"Class Coverage Across Sources:\")\n",
    "    for class_id, coverage in minority_class_coverage.items():\n",
    "        class_name = PROJECT_CONFIG['label_map'][class_id]\n",
    "        coverage_pct = (coverage / total_sources) * 100 if total_sources else 0\n",
    "        status = \"OK\" if coverage_pct > 50 else \"WARN\" if coverage_pct > 25 else \"LOW\"\n",
    "        print(f\"  {status} Class {class_id} ({class_name}): {coverage}/{total_sources} sources ({coverage_pct:.1f}%)\")\n",
    "    print(\"Files Saved:\")\n",
    "    print(f\"  - Enhanced features: {features_save_path}\")\n",
    "    print(f\"  - Enhanced splits: {splits_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
