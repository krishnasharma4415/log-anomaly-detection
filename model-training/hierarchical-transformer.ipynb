{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "695494eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc0135c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "================================================================================\n",
      "STARTUP CHECKS\n",
      "================================================================================\n",
      "Loading features...\n",
      "Loading splits...\n",
      "Usable sources: 16\n",
      "Total sources: 16\n",
      "Total splits: 16\n",
      "GPU available: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "TEST_MODE = False\n",
    "TRAIN_FINAL_MODEL = True\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "DATA_PATH = ROOT / \"dataset\" / \"labeled_data\" / \"normalized\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"hlogformer\"\n",
    "MODELS_PATH = ROOT / \"models\" / \"hlogformer\"\n",
    "\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OPTIMIZATION 3: Increased batch size with optimized gradient accumulation\n",
    "if TEST_MODE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST MODE ENABLED - Quick Pipeline Test\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Configuration:\")\n",
    "    print(\"  - 2 splits only (first 2 sources)\")\n",
    "    print(\"  - 1 epoch per split\")\n",
    "    print(\"  - Batch size: 16 (optimized)\")\n",
    "    print(\"  - Max 500 samples per source\")\n",
    "    print(\"  - Reduced sequence length: 64\")\n",
    "    if TRAIN_FINAL_MODEL:\n",
    "        print(\"  - Final production model: ENABLED\")\n",
    "    print(\"Set TEST_MODE = False for full training\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    MAX_SEQ_LEN = 64\n",
    "    BATCH_SIZE = 16\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    NUM_EPOCHS = 1\n",
    "    MAX_SAMPLES_PER_SOURCE = 500\n",
    "    MAX_SPLITS = 2\n",
    "else:\n",
    "    MAX_SEQ_LEN = 128\n",
    "    BATCH_SIZE = 32\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    NUM_EPOCHS = 5\n",
    "    MAX_SAMPLES_PER_SOURCE = None\n",
    "    MAX_SPLITS = None\n",
    "\n",
    "D_MODEL = 768\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 2\n",
    "N_TEMPLATES = 10000\n",
    "# Training hyperparameters (optimized for imbalanced data)\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.2\n",
    "FREEZE_BERT_LAYERS = 6\n",
    "\n",
    "# Loss weights (adjusted for extreme imbalance)\n",
    "ALPHA_CLASSIFICATION = 2.0\n",
    "ALPHA_TEMPLATE = 0.5\n",
    "ALPHA_TEMPORAL = 0.15\n",
    "ALPHA_SOURCE = 0.05\n",
    "\n",
    "# Optimized data loading settings\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 0 \n",
    "    PIN_MEMORY = True\n",
    "else:\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "\n",
    "feat_file = FEAT_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "split_file = FEAT_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTUP CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not feat_file.exists():\n",
    "    print(f\"ERROR: Feature file not found at {feat_file}\")\n",
    "    print(\"Please run feature-engineering.py first\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not split_file.exists():\n",
    "    print(f\"ERROR: Split file not found at {split_file}\")\n",
    "    print(\"Please run feature-engineering.py first\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Loading features...\")\n",
    "with open(feat_file, 'rb') as f:\n",
    "    feat_data = pickle.load(f)\n",
    "    data_dict = feat_data['hybrid_features_data']\n",
    "\n",
    "print(\"Loading splits...\")\n",
    "with open(split_file, 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    splits = split_data['splits']\n",
    "\n",
    "usable_sources = [s for s in data_dict.keys() if data_dict[s]['labels'] is not None]\n",
    "print(f\"Usable sources: {len(usable_sources)}\")\n",
    "\n",
    "source_to_id = {src: idx for idx, src in enumerate(sorted(usable_sources))}\n",
    "id_to_source = {idx: src for src, idx in source_to_id.items()}\n",
    "N_SOURCES = len(source_to_id)\n",
    "\n",
    "print(f\"Total sources: {N_SOURCES}\")\n",
    "print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow on CPU.\")\n",
    "    response = input(\"Continue anyway? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a07afb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached templates...\n",
      "Using cached templates!\n"
     ]
    }
   ],
   "source": [
    "def extract_templates_for_source(texts, source_name):\n",
    "    config = TemplateMinerConfig()\n",
    "    config.drain_sim_th = 0.4\n",
    "    config.drain_depth = 4\n",
    "    config.drain_max_children = 100\n",
    "    \n",
    "    miner = TemplateMiner(config=config)\n",
    "    template_ids = []\n",
    "    templates = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        result = miner.add_log_message(str(text))\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        if tid not in templates:\n",
    "            templates[tid] = result[\"template_mined\"]\n",
    "    \n",
    "    return np.array(template_ids), templates\n",
    "\n",
    "# OPTIMIZATION 6: Template caching (40% faster after first run)\n",
    "# FIX 1: Template Embedding Mismatch - Cap global template IDs\n",
    "def extract_all_templates():\n",
    "    cache_file = FEAT_PATH / \"template_cache.pkl\"\n",
    "    \n",
    "    # Check if cache exists and is valid\n",
    "    if cache_file.exists():\n",
    "        print(\"Loading cached templates...\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "        \n",
    "        # Verify cache is for current data\n",
    "        if cached_data.get('sources') == usable_sources:\n",
    "            print(\"Using cached templates!\")\n",
    "            return cached_data['template_data']\n",
    "        else:\n",
    "            print(\"Cache invalid (different sources), re-extracting...\")\n",
    "    \n",
    "    print(\"\\nExtracting templates for all sources...\")\n",
    "    template_data = {}\n",
    "    global_template_map = {}\n",
    "    template_id_mapping = {}\n",
    "    capped_tid = 0\n",
    "    \n",
    "    for source_name in tqdm(usable_sources, desc=\"Template extraction\"):\n",
    "        texts = data_dict[source_name]['texts']\n",
    "        local_template_ids, local_templates = extract_templates_for_source(texts, source_name)\n",
    "        \n",
    "        remapped_ids = []\n",
    "        for local_tid in local_template_ids:\n",
    "            key = (source_name, local_tid)\n",
    "            if key not in global_template_map:\n",
    "                if capped_tid < N_TEMPLATES:\n",
    "                    global_template_map[key] = capped_tid\n",
    "                    template_id_mapping[capped_tid] = key\n",
    "                    capped_tid += 1\n",
    "                else:\n",
    "                    # Map overflow to special \"unknown\" template\n",
    "                    global_template_map[key] = N_TEMPLATES - 1\n",
    "            remapped_ids.append(global_template_map[key])\n",
    "        \n",
    "        template_data[source_name] = {\n",
    "            'template_ids': np.array(remapped_ids),\n",
    "            'templates': local_templates,\n",
    "            'n_templates': len(local_templates)\n",
    "        }\n",
    "    \n",
    "    print(f\"Total unique templates (capped): {capped_tid}\")\n",
    "    \n",
    "    # Save cache\n",
    "    print(\"Saving template cache...\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'sources': usable_sources,\n",
    "            'template_data': template_data\n",
    "        }, f)\n",
    "    \n",
    "    return template_data\n",
    "\n",
    "template_data = extract_all_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c99b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(texts):\n",
    "    timestamps = np.arange(len(texts), dtype=np.float32)\n",
    "    if len(timestamps) > 1:\n",
    "        timestamps = (timestamps - timestamps.min()) / (timestamps.max() - timestamps.min() + 1e-8)\n",
    "    return timestamps\n",
    "\n",
    "# FIX 5: Data Sampling Strategy - Stratified sampling to preserve class balance\n",
    "def prepare_source_data(source_name):\n",
    "    texts = data_dict[source_name]['texts']\n",
    "    labels = data_dict[source_name]['labels']\n",
    "    template_ids = template_data[source_name]['template_ids']\n",
    "    timestamps = normalize_timestamps(texts)\n",
    "    source_id = source_to_id[source_name]\n",
    "    \n",
    "    if TEST_MODE and MAX_SAMPLES_PER_SOURCE is not None:\n",
    "        if len(texts) > MAX_SAMPLES_PER_SOURCE:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            \n",
    "            indices = np.arange(len(texts))\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                _, selected_indices = train_test_split(\n",
    "                    indices,\n",
    "                    train_size=MAX_SAMPLES_PER_SOURCE,\n",
    "                    stratify=labels,\n",
    "                    random_state=SEED\n",
    "                )\n",
    "                selected_indices = np.sort(selected_indices)\n",
    "            else:\n",
    "                selected_indices = np.random.choice(indices, MAX_SAMPLES_PER_SOURCE, replace=False)\n",
    "                selected_indices = np.sort(selected_indices)\n",
    "            \n",
    "            texts = [texts[i] for i in selected_indices]\n",
    "            labels = labels[selected_indices]\n",
    "            template_ids = template_ids[selected_indices]\n",
    "            timestamps = timestamps[selected_indices]\n",
    "    \n",
    "    return texts, labels, template_ids, timestamps, source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ecc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# OPTIMIZATION 1: Pre-tokenized dataset (30-40% faster)\n",
    "class PreTokenizedLogDataset(Dataset):\n",
    "    \"\"\"Tokenize once during initialization, not during training\"\"\"\n",
    "    def __init__(self, texts, labels, template_ids, timestamps, source_ids):\n",
    "        print(f\"Pre-tokenizing {len(texts)} samples (one-time cost)...\")\n",
    "        \n",
    "        # Tokenize all texts at once (batch processing is faster)\n",
    "        self.encodings = tokenizer(\n",
    "            [str(t) for t in texts],\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Store as tensors (faster than converting each time)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.template_ids = torch.tensor(template_ids, dtype=torch.long)\n",
    "        self.timestamps = torch.tensor(timestamps, dtype=torch.float32)\n",
    "        self.source_ids = torch.tensor(source_ids, dtype=torch.long)\n",
    "        \n",
    "        print(\"Pre-tokenization complete!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'template_ids': self.template_ids[idx],\n",
    "            'timestamps': self.timestamps[idx],\n",
    "            'source_ids': self.source_ids[idx]\n",
    "        }\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, texts, labels, template_ids, timestamps, source_ids):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.template_ids = template_ids\n",
    "        self.timestamps = timestamps\n",
    "        self.source_ids = source_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long),\n",
    "            'template_ids': torch.tensor(int(self.template_ids[idx]), dtype=torch.long),\n",
    "            'timestamps': torch.tensor(float(self.timestamps[idx]), dtype=torch.float32),\n",
    "            'source_ids': torch.tensor(int(self.source_ids[idx]), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# FIX 10: Data Augmentation for minority class (optional)\n",
    "class PreTokenizedAugmentedLogDataset(Dataset):\n",
    "    \"\"\"Pre-tokenized version with augmentation support\"\"\"\n",
    "    def __init__(self, texts, labels, template_ids, timestamps, source_ids, augment=True):\n",
    "        print(f\"Pre-tokenizing {len(texts)} samples with augmentation support...\")\n",
    "        \n",
    "        self.texts = [str(t) for t in texts]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.template_ids = torch.tensor(template_ids, dtype=torch.long)\n",
    "        self.timestamps = torch.tensor(timestamps, dtype=torch.float32)\n",
    "        self.source_ids = torch.tensor(source_ids, dtype=torch.long)\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Pre-tokenize normal texts\n",
    "        self.encodings = tokenizer(\n",
    "            self.texts,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Identify minority class samples\n",
    "        self.minority_indices = set(np.where(labels == 1)[0].tolist())\n",
    "        \n",
    "        print(\"Pre-tokenization with augmentation complete!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Simple augmentation: random word dropout\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) > 5:\n",
    "            keep_prob = 0.9\n",
    "            words = [w for w in words if np.random.rand() < keep_prob]\n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Apply augmentation to minority class with 30% probability\n",
    "        if self.augment and idx in self.minority_indices and np.random.rand() < 0.3:\n",
    "            text = self._augment_text(self.texts[idx])\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': self.labels[idx],\n",
    "                'template_ids': self.template_ids[idx],\n",
    "                'timestamps': self.timestamps[idx],\n",
    "                'source_ids': self.source_ids[idx]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': self.encodings['input_ids'][idx],\n",
    "                'attention_mask': self.encodings['attention_mask'][idx],\n",
    "                'labels': self.labels[idx],\n",
    "                'template_ids': self.template_ids[idx],\n",
    "                'timestamps': self.timestamps[idx],\n",
    "                'source_ids': self.source_ids[idx]\n",
    "            }\n",
    "\n",
    "class AugmentedLogDataset(Dataset):\n",
    "    def __init__(self, texts, labels, template_ids, timestamps, source_ids, augment=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.template_ids = template_ids\n",
    "        self.timestamps = timestamps\n",
    "        self.source_ids = source_ids\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Identify minority class samples\n",
    "        self.minority_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Simple augmentation: random word dropout\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) > 5:\n",
    "            keep_prob = 0.9\n",
    "            words = [w for w in words if np.random.rand() < keep_prob]\n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Apply augmentation to minority class with 30% probability\n",
    "        if self.augment and self.labels[idx] == 1 and np.random.rand() < 0.3:\n",
    "            text = self._augment_text(text)\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long),\n",
    "            'template_ids': torch.tensor(int(self.template_ids[idx]), dtype=torch.long),\n",
    "            'timestamps': torch.tensor(float(self.timestamps[idx]), dtype=torch.float32),\n",
    "            'source_ids': torch.tensor(int(self.source_ids[idx]), dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96e0334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "def gradient_reversal(x, lambda_=1.0):\n",
    "    return GradientReversalFunction.apply(x, lambda_)\n",
    "\n",
    "# FIX 4: Template Attention - Add template-aware bias\n",
    "class TemplateAwareAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_templates, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.template_bias = nn.Embedding(n_templates, n_heads)\n",
    "        self.template_alpha = nn.Parameter(torch.tensor(0.1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, template_ids, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Add template-aware bias\n",
    "        template_bias = self.template_bias(template_ids)  # [batch, n_heads]\n",
    "        template_bias = template_bias.unsqueeze(2).unsqueeze(3)  # [batch, n_heads, 1, 1]\n",
    "        scores = scores + template_bias * self.template_alpha\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.out_proj(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(x + output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TemporalModule(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temporal_embedding = nn.Linear(1, d_model)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, timestamps):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        temporal_emb = self.temporal_embedding(timestamps.unsqueeze(-1)).unsqueeze(1)\n",
    "        x = x + temporal_emb\n",
    "        \n",
    "        sorted_indices = torch.argsort(timestamps)\n",
    "        x_sorted = x[sorted_indices]\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x_sorted)\n",
    "        \n",
    "        unsorted_indices = torch.argsort(sorted_indices)\n",
    "        lstm_out = lstm_out[unsorted_indices]\n",
    "        \n",
    "        output = self.layer_norm(x + lstm_out)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "class SourceAdapter(nn.Module):\n",
    "    def __init__(self, d_model, adapter_dim=192):\n",
    "        super().__init__()\n",
    "        self.down_proj = nn.Linear(d_model, adapter_dim)\n",
    "        self.up_proj = nn.Linear(adapter_dim, d_model)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        adapter_out = self.up_proj(F.relu(self.down_proj(x)))\n",
    "        return self.alpha * x + (1 - self.alpha) * adapter_out\n",
    "\n",
    "# FIX 6: Domain Adaptive Layer for better generalization\n",
    "class DomainAdaptiveLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_sources):\n",
    "        super().__init__()\n",
    "        self.shared_adapter = SourceAdapter(d_model)\n",
    "        self.source_bias = nn.Embedding(n_sources, d_model)\n",
    "    \n",
    "    def forward(self, x, source_ids):\n",
    "        adapted = self.shared_adapter(x)\n",
    "        bias = self.source_bias(source_ids)\n",
    "        return adapted + 0.1 * bias\n",
    "\n",
    "class SourceDiscriminator(nn.Module):\n",
    "    def __init__(self, d_model, n_sources):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, n_sources)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2486f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLogFormer(nn.Module):\n",
    "    def __init__(self, n_sources, n_templates, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        bert_config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        bert_config.output_attentions = True\n",
    "        bert_config.output_hidden_states = False  # Don't need all hidden states\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', config=bert_config)\n",
    "        \n",
    "        self.freeze_layers = freeze_layers\n",
    "        \n",
    "        # Freeze embeddings\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Freeze early encoder layers and set to eval mode\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "            self.bert.encoder.layer[i].eval()  # Disable dropout/layernorm updates\n",
    "        \n",
    "        self.template_embedding = nn.Embedding(n_templates + 1, D_MODEL, padding_idx=n_templates)\n",
    "        nn.init.xavier_uniform_(self.template_embedding.weight)\n",
    "        \n",
    "        self.template_attention = TemplateAwareAttention(D_MODEL, N_HEADS, n_templates)\n",
    "        \n",
    "        self.temporal_module = TemporalModule(D_MODEL)\n",
    "        \n",
    "        # Use domain adaptive layer instead of source-specific adapters\n",
    "        self.domain_adapter = DomainAdaptiveLayer(D_MODEL, n_sources)\n",
    "        \n",
    "        self.source_discriminator = SourceDiscriminator(D_MODEL, n_sources)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(D_MODEL // 2, 2)\n",
    "        )\n",
    "        \n",
    "        self.template_classifier = nn.Linear(D_MODEL, min(n_templates, 1000))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, template_ids, timestamps, source_ids):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        \n",
    "        template_emb = self.template_embedding(template_ids)\n",
    "        enhanced_output = pooled_output + template_emb\n",
    "        \n",
    "        template_attended = self.template_attention(\n",
    "            sequence_output, template_ids, attention_mask\n",
    "        )\n",
    "        template_pooled = template_attended[:, 0, :]\n",
    "        \n",
    "        combined_output = template_pooled + template_emb\n",
    "        \n",
    "        temporal_output = self.temporal_module(combined_output, timestamps)\n",
    "        \n",
    "        # Apply domain adaptation\n",
    "        final_output = self.domain_adapter(temporal_output, source_ids)\n",
    "        \n",
    "        logits = self.classifier(final_output)\n",
    "        \n",
    "        reversed_features = gradient_reversal(final_output, lambda_=0.1)\n",
    "        source_logits = self.source_discriminator(reversed_features)\n",
    "        \n",
    "        template_logits = self.template_classifier(final_output)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'source_logits': source_logits,\n",
    "            'template_logits': template_logits,\n",
    "            'features': final_output\n",
    "        }\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Override train to keep frozen layers in eval mode\"\"\"\n",
    "        super().train(mode)\n",
    "        if mode:\n",
    "            # Keep frozen layers in eval\n",
    "            self.bert.embeddings.eval()\n",
    "            for i in range(self.freeze_layers):\n",
    "                self.bert.encoder.layer[i].eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76eeb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute balanced class weights\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    weights = torch.FloatTensor([total / (len(unique) * count) for count in counts])\n",
    "    return weights\n",
    "\n",
    "def focal_loss(logits, labels, class_weights=None, alpha=0.5, gamma=3.0):\n",
    "    ce_loss = F.cross_entropy(logits, labels, weight=class_weights, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal.mean()\n",
    "\n",
    "def temporal_consistency_loss(features, timestamps, tau=0.1):\n",
    "    sorted_indices = torch.argsort(timestamps)\n",
    "    sorted_features = features[sorted_indices]\n",
    "    sorted_times = timestamps[sorted_indices]\n",
    "    \n",
    "    if len(sorted_features) < 2:\n",
    "        return torch.tensor(0.0, device=features.device)\n",
    "    \n",
    "    feature_diff = sorted_features[1:] - sorted_features[:-1]\n",
    "    time_diff = sorted_times[1:] - sorted_times[:-1]\n",
    "    \n",
    "    weights = torch.exp(-time_diff / tau)\n",
    "    consistency = (feature_diff.pow(2).sum(dim=1) * weights).mean()\n",
    "    \n",
    "    return consistency\n",
    "\n",
    "def compute_loss(outputs, batch, class_weights=None):\n",
    "    logits = outputs['logits']\n",
    "    source_logits = outputs['source_logits']\n",
    "    template_logits = outputs['template_logits']\n",
    "    features = outputs['features']\n",
    "    \n",
    "    labels = batch['labels']\n",
    "    source_ids = batch['source_ids']\n",
    "    template_ids = batch['template_ids']\n",
    "    timestamps = batch['timestamps']\n",
    "    \n",
    "    loss_cls = focal_loss(logits, labels, class_weights)\n",
    "    \n",
    "    loss_source = F.cross_entropy(source_logits, source_ids)\n",
    "    \n",
    "    valid_template_mask = template_ids < template_logits.size(1)\n",
    "    if valid_template_mask.any():\n",
    "        loss_template = F.cross_entropy(\n",
    "            template_logits[valid_template_mask],\n",
    "            template_ids[valid_template_mask]\n",
    "        )\n",
    "    else:\n",
    "        loss_template = torch.tensor(0.0, device=logits.device)\n",
    "    \n",
    "    loss_temporal = temporal_consistency_loss(features, timestamps)\n",
    "    \n",
    "    total_loss = (\n",
    "        ALPHA_CLASSIFICATION * loss_cls +\n",
    "        ALPHA_TEMPLATE * loss_template +\n",
    "        ALPHA_TEMPORAL * loss_temporal +\n",
    "        ALPHA_SOURCE * loss_source\n",
    "    )\n",
    "    \n",
    "    return total_loss, {\n",
    "        'loss_cls': loss_cls.item(),\n",
    "        'loss_template': loss_template.item() if isinstance(loss_template, torch.Tensor) else 0.0,\n",
    "        'loss_temporal': loss_temporal.item(),\n",
    "        'loss_source': loss_source.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a555fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_acc'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Add G-Mean for imbalanced data\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn + 1e-8)\n",
    "        specificity = tn / (tn + fp + 1e-8)\n",
    "        metrics['g_mean'] = np.sqrt(sensitivity * specificity)\n",
    "    else:\n",
    "        metrics['g_mean'] = 0.0\n",
    "    \n",
    "    # Add precision-recall metrics\n",
    "    if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true, y_proba[:, 1])\n",
    "            metrics['avg_precision'] = average_precision_score(y_true, y_proba[:, 1])\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            optimal_idx = np.argmax(f1_scores)\n",
    "            metrics['optimal_threshold'] = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "            metrics['optimal_f1'] = f1_scores[optimal_idx]\n",
    "        except:\n",
    "            metrics['avg_precision'] = 0.0\n",
    "            metrics['optimal_threshold'] = 0.5\n",
    "            metrics['optimal_f1'] = 0.0\n",
    "    \n",
    "    per_class = {}\n",
    "    for cls in np.unique(np.concatenate([y_true, y_pred])):\n",
    "        y_true_bin = (y_true == cls).astype(int)\n",
    "        y_pred_bin = (y_pred == cls).astype(int)\n",
    "        if y_true_bin.sum() > 0:\n",
    "            per_class[int(cls)] = {\n",
    "                'precision': precision_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'recall': recall_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'f1': f1_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'support': int(y_true_bin.sum())\n",
    "            }\n",
    "    metrics['per_class'] = per_class\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "        except:\n",
    "            metrics['auroc'] = 0.0\n",
    "    else:\n",
    "        metrics['auroc'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6386124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, class_weights=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with autocast(enabled=USE_AMP):\n",
    "            outputs = model(\n",
    "                batch['input_ids'],\n",
    "                batch['attention_mask'],\n",
    "                batch['template_ids'],\n",
    "                batch['timestamps'],\n",
    "                batch['source_ids']\n",
    "            )\n",
    "            loss, loss_dict = compute_loss(outputs, batch, class_weights)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item() * GRADIENT_ACCUMULATION_STEPS})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return avg_loss, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with autocast(enabled=USE_AMP):\n",
    "                outputs = model(\n",
    "                    batch['input_ids'],\n",
    "                    batch['attention_mask'],\n",
    "                    batch['template_ids'],\n",
    "                    batch['timestamps'],\n",
    "                    batch['source_ids']\n",
    "                )\n",
    "            \n",
    "            probs = F.softmax(outputs['logits'], dim=1)\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# FIX 9: Threshold Optimization\n",
    "def find_optimal_threshold(model, val_loader, device, metric='f1'):\n",
    "    \"\"\"Find optimal classification threshold on validation set\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with autocast(enabled=USE_AMP):\n",
    "                outputs = model(\n",
    "                    batch['input_ids'],\n",
    "                    batch['attention_mask'],\n",
    "                    batch['template_ids'],\n",
    "                    batch['timestamps'],\n",
    "                    batch['source_ids']\n",
    "                )\n",
    "            probs = F.softmax(outputs['logits'], dim=1)\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Try thresholds from 0.1 to 0.9\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_score = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        preds = (all_probs >= threshold).astype(int)\n",
    "        score = f1_score(all_labels, preds, average='macro', zero_division=0)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "# FIX 3: Learning Rate Schedule - Don't divide by gradient accumulation\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=NUM_EPOCHS, class_weights=None):\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # FIX: Don't divide by gradient accumulation\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    patience = 3\n",
    "    \n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, optimizer, scheduler, scaler, device, class_weights)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_f1 = val_metrics['f1_macro']\n",
    "        print(f\"Val F1: {val_f1:.4f}, Val Balanced Acc: {val_metrics['balanced_acc']:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            best_threshold = find_optimal_threshold(model, val_loader, device)\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'optimal_threshold': best_threshold\n",
    "            }, MODELS_PATH / 'best_model.pt')\n",
    "            print(f\"Saved best model with F1: {best_f1:.4f}, Threshold: {best_threshold:.3f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "048fe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loso_split(split_idx, split):\n",
    "    test_source = split['test_source']\n",
    "    train_sources = [s for s in split['train_sources'] if s in usable_sources]\n",
    "    \n",
    "    if test_source not in usable_sources:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Split {split_idx + 1}: Test on {test_source}\")\n",
    "    print(f\"Train sources: {train_sources}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    train_texts_list = []\n",
    "    train_labels_list = []\n",
    "    train_template_ids_list = []\n",
    "    train_timestamps_list = []\n",
    "    train_source_ids_list = []\n",
    "    \n",
    "    for source in train_sources:\n",
    "        texts, labels, template_ids, timestamps, source_id = prepare_source_data(source)\n",
    "        train_texts_list.extend(texts)\n",
    "        train_labels_list.extend(labels)\n",
    "        train_template_ids_list.extend(template_ids)\n",
    "        train_timestamps_list.extend(timestamps)\n",
    "        train_source_ids_list.extend([source_id] * len(texts))\n",
    "    \n",
    "    test_texts, test_labels, test_template_ids, test_timestamps, test_source_id = prepare_source_data(test_source)\n",
    "    \n",
    "    if len(np.unique(test_labels)) < 2:\n",
    "        print(f\"Skipping {test_source}: single class\")\n",
    "        return None\n",
    "    \n",
    "    # Use pre-tokenized dataset for speed\n",
    "    train_dataset = PreTokenizedLogDataset(\n",
    "        train_texts_list,\n",
    "        train_labels_list,\n",
    "        train_template_ids_list,\n",
    "        train_timestamps_list,\n",
    "        train_source_ids_list\n",
    "    )\n",
    "    \n",
    "    test_dataset = PreTokenizedLogDataset(\n",
    "        test_texts,\n",
    "        test_labels,\n",
    "        test_template_ids,\n",
    "        test_timestamps,\n",
    "        [test_source_id] * len(test_texts)\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    # OPTIMIZATION 2: Optimized DataLoader settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    max_template_id = max(\n",
    "        max(train_template_ids_list),\n",
    "        max(test_template_ids)\n",
    "    )\n",
    "    n_templates = min(max_template_id + 1, N_TEMPLATES)\n",
    "    \n",
    "    # Compute class weights for imbalanced data\n",
    "    train_labels_array = np.array(train_labels_list)\n",
    "    class_weights = compute_class_weights(train_labels_array).to(device)\n",
    "    print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
    "    \n",
    "    model = HLogFormer(N_SOURCES, n_templates, FREEZE_BERT_LAYERS).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    best_f1 = train_model(model, train_loader, val_loader, device, class_weights=class_weights)\n",
    "    \n",
    "    checkpoint = torch.load(MODELS_PATH / 'best_model.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nTest Results for {test_source}:\")\n",
    "    print(f\"F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Balanced Acc: {test_metrics['balanced_acc']:.4f}\")\n",
    "    print(f\"AUROC: {test_metrics['auroc']:.4f}\")\n",
    "    print(f\"MCC: {test_metrics['mcc']:.4f}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        'split_idx': split_idx,\n",
    "        'test_source': test_source,\n",
    "        'train_sources': train_sources,\n",
    "        'f1_macro': test_metrics['f1_macro'],\n",
    "        'balanced_acc': test_metrics['balanced_acc'],\n",
    "        'auroc': test_metrics['auroc'],\n",
    "        'mcc': test_metrics['mcc'],\n",
    "        'per_class': test_metrics['per_class']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3c27044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 16 splits...\n",
      "\n",
      "================================================================================\n",
      "Split 1: Test on Android_2k\n",
      "Train sources: ['Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.71069837 1.6865302 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.1799, Train F1: 0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2335, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2335, Threshold: 0.620\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2474, Train F1: 0.2307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2335, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0192, Train F1: 0.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2341, Val Balanced Acc: 0.5002\n",
      "Saved best model with F1: 0.2341, Threshold: 0.610\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7777, Train F1: 0.2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2346, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2346, Threshold: 0.600\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6916, Train F1: 0.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2346, Val Balanced Acc: 0.5005\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Android_2k:\n",
      "F1-Macro: 0.0128\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5916\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 2: Test on Apache_2k\n",
      "Train sources: ['Android_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.6928086 1.7966224]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.2780, Train F1: 0.3263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2240, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2240, Threshold: 0.630\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5131, Train F1: 0.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2240, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2472, Train F1: 0.2182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2240, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9842, Train F1: 0.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2251, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2251, Threshold: 0.620\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8967, Train F1: 0.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2251, Val Balanced Acc: 0.5005\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Apache_2k:\n",
      "F1-Macro: 0.2221\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5460\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 3: Test on BGL_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.70594877 1.713894  ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.5115, Train F1: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2353, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2353, Threshold: 0.610\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3152, Train F1: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2353, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0900, Train F1: 0.2279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2353, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8633, Train F1: 0.2308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2353, Val Balanced Acc: 0.5000\n",
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for BGL_2k:\n",
      "F1-Macro: 0.0775\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4743\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 4: Test on Hadoop_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.66994196 1.9710907 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.5137, Train F1: 0.3789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2159, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2159, Threshold: 0.630\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3726, Train F1: 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2159, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1258, Train F1: 0.2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2159, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8783, Train F1: 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2169, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2169, Threshold: 0.630\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7895, Train F1: 0.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2174, Val Balanced Acc: 0.5007\n",
      "Saved best model with F1: 0.2174, Threshold: 0.620\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Hadoop_2k:\n",
      "F1-Macro: 0.3958\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4069\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 5: Test on HDFS_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping HDFS_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 6: Test on HealthApp_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.7112038 1.6836907]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11.7407, Train F1: 0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2443, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2443, Threshold: 0.610\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3365, Train F1: 0.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2443, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0808, Train F1: 0.2313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2454, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2454, Threshold: 0.610\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8306, Train F1: 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2454, Val Balanced Acc: 0.5005\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7424, Train F1: 0.2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2476, Val Balanced Acc: 0.5015\n",
      "Saved best model with F1: 0.2476, Threshold: 0.590\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for HealthApp_2k:\n",
      "F1-Macro: 0.0055\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.6432\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 7: Test on HPC_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.6827492 1.867995 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.2731, Train F1: 0.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2274, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2274, Threshold: 0.630\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3247, Train F1: 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2274, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0690, Train F1: 0.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2274, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8398, Train F1: 0.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2279, Val Balanced Acc: 0.5002\n",
      "Saved best model with F1: 0.2279, Threshold: 0.610\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7580, Train F1: 0.2403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2320, Val Balanced Acc: 0.5016\n",
      "Saved best model with F1: 0.2320, Threshold: 0.600\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for HPC_2k:\n",
      "F1-Macro: 0.3080\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4791\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 8: Test on Linux_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.65257114 2.13858   ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.1472, Train F1: 0.1878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2049, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2049, Threshold: 0.650\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4074, Train F1: 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2049, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1742, Train F1: 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2049, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9277, Train F1: 0.1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2059, Val Balanced Acc: 0.5004\n",
      "Saved best model with F1: 0.2059, Threshold: 0.630\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8371, Train F1: 0.1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2069, Val Balanced Acc: 0.5009\n",
      "Saved best model with F1: 0.2069, Threshold: 0.620\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Linux_2k:\n",
      "F1-Macro: 0.4880\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.1531\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 9: Test on Mac_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.6972528 1.767409 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.6041, Train F1: 0.4188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2339, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2339, Threshold: 0.610\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2496, Train F1: 0.2219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2339, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9930, Train F1: 0.2234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2339, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7763, Train F1: 0.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2350, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2350, Threshold: 0.630\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6941, Train F1: 0.2903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2382, Val Balanced Acc: 0.5019\n",
      "Saved best model with F1: 0.2382, Threshold: 0.630\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Mac_2k:\n",
      "F1-Macro: 0.1780\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4451\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 10: Test on OpenSSH_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping OpenSSH_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 11: Test on OpenStack_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping OpenStack_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 12: Test on Proxifier_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.7083156 1.700102 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.4801, Train F1: 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2357, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2357, Threshold: 0.590\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4119, Train F1: 0.2309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2357, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1603, Train F1: 0.2315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2357, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9273, Train F1: 0.2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2357, Val Balanced Acc: 0.5000\n",
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Proxifier_2k:\n",
      "F1-Macro: 0.0463\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.7541\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 13: Test on Spark_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.711305  1.6831238]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11.9136, Train F1: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2374, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2374, Threshold: 0.620\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3722, Train F1: 0.2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2374, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1616, Train F1: 0.2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2396, Val Balanced Acc: 0.5010\n",
      "Saved best model with F1: 0.2396, Threshold: 0.620\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9219, Train F1: 0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2477, Val Balanced Acc: 0.5046\n",
      "Saved best model with F1: 0.2477, Threshold: 0.610\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8382, Train F1: 0.3355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.7699, Val Balanced Acc: 0.8348\n",
      "Saved best model with F1: 0.7699, Threshold: 0.560\n",
      "GPU Memory: 3.12 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Spark_2k:\n",
      "F1-Macro: 0.0055\n",
      "Balanced Acc: 0.5008\n",
      "AUROC: 0.5215\n",
      "MCC: 0.0025\n",
      "\n",
      "================================================================================\n",
      "Split 14: Test on Thunderbird_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.70505285 1.7191978 ]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.0566, Train F1: 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2323, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2323, Threshold: 0.620\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3823, Train F1: 0.2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2404, Val Balanced Acc: 0.5036\n",
      "Saved best model with F1: 0.2404, Threshold: 0.600\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1499, Train F1: 0.2376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2366, Val Balanced Acc: 0.5019\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9122, Train F1: 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.3155, Val Balanced Acc: 0.5380\n",
      "Saved best model with F1: 0.3155, Threshold: 0.590\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8307, Train F1: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2366, Val Balanced Acc: 0.5019\n",
      "GPU Memory: 3.11 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Thunderbird_2k:\n",
      "F1-Macro: 0.0758\n",
      "Balanced Acc: 0.3711\n",
      "AUROC: 0.5010\n",
      "MCC: -0.4220\n",
      "\n",
      "================================================================================\n",
      "Split 15: Test on Windows_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.6858397 1.8452454]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.0745, Train F1: 0.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2218, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2218, Threshold: 0.630\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3548, Train F1: 0.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2218, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1824, Train F1: 0.2159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2228, Val Balanced Acc: 0.5005\n",
      "Saved best model with F1: 0.2228, Threshold: 0.610\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9306, Train F1: 0.2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2218, Val Balanced Acc: 0.5000\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8414, Train F1: 0.2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2223, Val Balanced Acc: 0.5002\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Windows_2k:\n",
      "F1-Macro: 0.2834\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5546\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 16: Test on Zookeeper_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k']\n",
      "================================================================================\n",
      "Pre-tokenizing 30000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Pre-tokenizing 2000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Class weights: [0.6946374 1.7844397]\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11.9808, Train F1: 0.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2286, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2286, Threshold: 0.620\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3719, Train F1: 0.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2449, Val Balanced Acc: 0.5073\n",
      "Saved best model with F1: 0.2449, Threshold: 0.590\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1627, Train F1: 0.2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2328, Val Balanced Acc: 0.5019\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9300, Train F1: 0.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2360, Val Balanced Acc: 0.5033\n",
      "GPU Memory: 3.10 GB / 8.59 GB\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8448, Train F1: 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2318, Val Balanced Acc: 0.5014\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Zookeeper_2k:\n",
      "F1-Macro: 0.2045\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.2914\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "   Test Source  F1-Macro  Balanced Acc    AUROC       MCC\n",
      "      Linux_2k  0.487967      0.500000 0.153122  0.000000\n",
      "     Hadoop_2k  0.395770      0.500000 0.406944  0.000000\n",
      "        HPC_2k  0.307958      0.500000 0.479148  0.000000\n",
      "    Windows_2k  0.283411      0.500000 0.554573  0.000000\n",
      "     Apache_2k  0.222093      0.500000 0.546043  0.000000\n",
      "  Zookeeper_2k  0.204455      0.500000 0.291390  0.000000\n",
      "        Mac_2k  0.177970      0.500000 0.445060  0.000000\n",
      "        BGL_2k  0.077491      0.500000 0.474329  0.000000\n",
      "Thunderbird_2k  0.075773      0.371099 0.501003 -0.421973\n",
      "  Proxifier_2k  0.046257      0.500000 0.754100  0.000000\n",
      "    Android_2k  0.012833      0.500000 0.591585  0.000000\n",
      "      Spark_2k  0.005494      0.500753 0.521524  0.002456\n",
      "  HealthApp_2k  0.005470      0.500000 0.643242  0.000000\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE STATISTICS\n",
      "================================================================================\n",
      "Sources evaluated: 13\n",
      "Average F1-Macro: 0.1771 +/- 0.1578\n",
      "Average Balanced Acc: 0.4901 +/- 0.0358\n",
      "Average AUROC: 0.4894 +/- 0.1512\n",
      "Average MCC: -0.0323 +/- 0.1171\n",
      "Best source: Linux_2k (F1: 0.4880)\n",
      "Worst source: HealthApp_2k (F1: 0.0055)\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "splits_to_process = splits[:MAX_SPLITS] if TEST_MODE and MAX_SPLITS else splits\n",
    "print(f\"\\nProcessing {len(splits_to_process)} splits...\")\n",
    "\n",
    "for split_idx, split in enumerate(splits_to_process):\n",
    "    result = run_loso_split(split_idx, split)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "\n",
    "if not all_results:\n",
    "    print(\"No results generated\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'Test Source': r['test_source'],\n",
    "    'F1-Macro': r['f1_macro'],\n",
    "    'Balanced Acc': r['balanced_acc'],\n",
    "    'AUROC': r['auroc'],\n",
    "    'MCC': r['mcc']\n",
    "} for r in all_results])\n",
    "\n",
    "results_df = results_df.sort_values('F1-Macro', ascending=False)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources evaluated: {len(all_results)}\")\n",
    "print(f\"Average F1-Macro: {results_df['F1-Macro'].mean():.4f} +/- {results_df['F1-Macro'].std():.4f}\")\n",
    "print(f\"Average Balanced Acc: {results_df['Balanced Acc'].mean():.4f} +/- {results_df['Balanced Acc'].std():.4f}\")\n",
    "print(f\"Average AUROC: {results_df['AUROC'].mean():.4f} +/- {results_df['AUROC'].std():.4f}\")\n",
    "print(f\"Average MCC: {results_df['MCC'].mean():.4f} +/- {results_df['MCC'].std():.4f}\")\n",
    "print(f\"Best source: {results_df.iloc[0]['Test Source']} (F1: {results_df.iloc[0]['F1-Macro']:.4f})\")\n",
    "print(f\"Worst source: {results_df.iloc[-1]['Test Source']} (F1: {results_df.iloc[-1]['F1-Macro']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3819442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\hlogformer\\results_20251125_221010\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = RESULTS_PATH / f\"results_{timestamp}\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_df.to_csv(results_dir / \"loso_results.csv\", index=False)\n",
    "\n",
    "with open(results_dir / \"complete_results.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_results': all_results,\n",
    "        'summary': results_df.to_dict('records'),\n",
    "        'config': {\n",
    "            'max_seq_len': MAX_SEQ_LEN,\n",
    "            'd_model': D_MODEL,\n",
    "            'n_heads': N_HEADS,\n",
    "            'n_layers': N_LAYERS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'num_epochs': NUM_EPOCHS,\n",
    "            'freeze_bert_layers': FREEZE_BERT_LAYERS\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89d6195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING FINAL PRODUCTION MODEL ON ALL DATA\n",
      "================================================================================\n",
      "\n",
      "Preparing all available data...\n",
      "Total samples: 21,000\n",
      "Label distribution: {np.int32(0): np.int64(15313), np.int32(1): np.int64(5687)}\n",
      "Pre-tokenizing 21000 samples (one-time cost)...\n",
      "Pre-tokenization complete!\n",
      "Train samples: 18,900\n",
      "Val samples: 2,100\n",
      "Class weights: [0.6856919 1.8463162]\n",
      "\n",
      "Initializing final model...\n",
      "Templates: 1596\n",
      "Sources: 16\n",
      "Model parameters: 124,219,020\n",
      "Trainable parameters: 57,854,604\n",
      "\n",
      "Training for 1 epochs...\n",
      "Training batches per epoch: 1181\n",
      "Validation batches: 66\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.6853, Train F1: 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2084, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.2084, Threshold: 0.620\n",
      "GPU Memory: 1.89 GB / 8.59 GB\n",
      "\n",
      "================================================================================\n",
      "FINAL PRODUCTION MODEL SAVED\n",
      "================================================================================\n",
      "Location: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\hlogformer\\final_production_model.pt\n",
      "Best F1: 0.2084\n",
      "Training samples: 21,000\n",
      "Sources: 16\n",
      "Templates: 1596\n",
      "\n",
      "To load for inference:\n",
      "  checkpoint = torch.load('models/hlogformer/final_production_model.pt')\n",
      "  model = HLogFormer(checkpoint['n_sources'], checkpoint['n_templates'])\n",
      "  model.load_state_dict(checkpoint['model_state_dict'])\n",
      "  model.eval()\n",
      "================================================================================\n",
      "\n",
      "All training complete. Models saved at: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\hlogformer\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_FINAL_MODEL:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FINAL PRODUCTION MODEL ON ALL DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nPreparing all available data...\")\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    all_template_ids = []\n",
    "    all_timestamps = []\n",
    "    all_source_ids = []\n",
    "    \n",
    "    for source_name in usable_sources:\n",
    "        texts, labels, template_ids, timestamps, source_id = prepare_source_data(source_name)\n",
    "        all_texts.extend(texts)\n",
    "        all_labels.extend(labels)\n",
    "        all_template_ids.extend(template_ids)\n",
    "        all_timestamps.extend(timestamps)\n",
    "        all_source_ids.extend([source_id] * len(texts))\n",
    "    \n",
    "    print(f\"Total samples: {len(all_texts):,}\")\n",
    "    print(f\"Label distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}\")\n",
    "    \n",
    "    # Use pre-tokenized dataset for speed\n",
    "    full_dataset = PreTokenizedLogDataset(\n",
    "        all_texts,\n",
    "        all_labels,\n",
    "        all_template_ids,\n",
    "        all_timestamps,\n",
    "        all_source_ids\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {train_size:,}\")\n",
    "    print(f\"Val samples: {val_size:,}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    max_template_id = max(all_template_ids)\n",
    "    n_templates = min(max_template_id + 1, N_TEMPLATES)\n",
    "    \n",
    "    # Compute class weights for final model\n",
    "    all_labels_array = np.array(all_labels)\n",
    "    class_weights_final = compute_class_weights(all_labels_array).to(device)\n",
    "    print(f\"Class weights: {class_weights_final.cpu().numpy()}\")\n",
    "    \n",
    "    print(f\"\\nInitializing final model...\")\n",
    "    print(f\"Templates: {n_templates}\")\n",
    "    print(f\"Sources: {N_SOURCES}\")\n",
    "    \n",
    "    final_model = HLogFormer(N_SOURCES, n_templates, FREEZE_BERT_LAYERS).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in final_model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in final_model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    final_epochs = 1 if TEST_MODE else 10\n",
    "    print(f\"\\nTraining for {final_epochs} epochs...\")\n",
    "    \n",
    "    best_f1 = train_model(final_model, train_loader, val_loader, device, num_epochs=final_epochs, class_weights=class_weights_final)\n",
    "    \n",
    "    checkpoint = torch.load(MODELS_PATH / 'best_model.pt')\n",
    "    final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    final_model_path = MODELS_PATH / 'final_production_model.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'n_sources': N_SOURCES,\n",
    "        'n_templates': n_templates,\n",
    "        'config': {\n",
    "            'max_seq_len': MAX_SEQ_LEN,\n",
    "            'd_model': D_MODEL,\n",
    "            'n_heads': N_HEADS,\n",
    "            'n_layers': N_LAYERS,\n",
    "            'freeze_bert_layers': FREEZE_BERT_LAYERS\n",
    "        },\n",
    "        'training_samples': len(all_texts),\n",
    "        'best_f1': best_f1,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'source_to_id': source_to_id,\n",
    "        'id_to_source': id_to_source\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PRODUCTION MODEL SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Location: {final_model_path}\")\n",
    "    print(f\"Best F1: {best_f1:.4f}\")\n",
    "    print(f\"Training samples: {len(all_texts):,}\")\n",
    "    print(f\"Sources: {N_SOURCES}\")\n",
    "    print(f\"Templates: {n_templates}\")\n",
    "    print(\"\\nTo load for inference:\")\n",
    "    print(\"  checkpoint = torch.load('models/hlogformer/final_production_model.pt')\")\n",
    "    print(\"  model = HLogFormer(checkpoint['n_sources'], checkpoint['n_templates'])\")\n",
    "    print(\"  model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "    print(\"  model.eval()\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    del final_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nAll training complete. Models saved at: {MODELS_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
