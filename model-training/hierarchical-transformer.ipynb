{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695494eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0135c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "TEST_MODE = True\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "DATA_PATH = ROOT / \"dataset\" / \"labeled_data\" / \"normalized\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"hlogformer\"\n",
    "MODELS_PATH = ROOT / \"models\" / \"hlogformer\"\n",
    "\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4273f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST MODE ENABLED - Quick Pipeline Test\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST MODE ENABLED - Quick Pipeline Test\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    MAX_SEQ_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    NUM_EPOCHS = 1\n",
    "    MAX_SAMPLES_PER_SOURCE = None\n",
    "    MAX_SPLITS = None\n",
    "else:\n",
    "    MAX_SEQ_LEN = 256\n",
    "    BATCH_SIZE = 16\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    NUM_EPOCHS = 5\n",
    "    MAX_SAMPLES_PER_SOURCE = None\n",
    "    MAX_SPLITS = None\n",
    "\n",
    "D_MODEL = 768\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 2\n",
    "N_TEMPLATES = 10000\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "FREEZE_BERT_LAYERS = 6\n",
    "\n",
    "ALPHA_CLASSIFICATION = 1.0\n",
    "ALPHA_TEMPLATE = 0.3\n",
    "ALPHA_TEMPORAL = 0.2\n",
    "ALPHA_SOURCE = 0.1\n",
    "\n",
    "USE_AMP = True\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac02e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...\n",
      "Loading splits...\n",
      "Usable sources: 16\n",
      "Total sources: 16\n",
      "Total splits: 16\n",
      "GPU available: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "feat_file = FEAT_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "split_file = FEAT_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "\n",
    "if not feat_file.exists():\n",
    "    print(f\"ERROR: Feature file not found at {feat_file}\")\n",
    "    print(\"Please run feature-engineering.py first\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not split_file.exists():\n",
    "    print(f\"ERROR: Split file not found at {split_file}\")\n",
    "    print(\"Please run feature-engineering.py first\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Loading features...\")\n",
    "with open(feat_file, 'rb') as f:\n",
    "    feat_data = pickle.load(f)\n",
    "    data_dict = feat_data['hybrid_features_data']\n",
    "\n",
    "print(\"Loading splits...\")\n",
    "with open(split_file, 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    splits = split_data['splits']\n",
    "\n",
    "usable_sources = [s for s in data_dict.keys() if data_dict[s]['labels'] is not None]\n",
    "print(f\"Usable sources: {len(usable_sources)}\")\n",
    "\n",
    "source_to_id = {src: idx for idx, src in enumerate(sorted(usable_sources))}\n",
    "id_to_source = {idx: src for src, idx in source_to_id.items()}\n",
    "N_SOURCES = len(source_to_id)\n",
    "\n",
    "print(f\"Total sources: {N_SOURCES}\")\n",
    "print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow on CPU.\")\n",
    "    response = input(\"Continue anyway? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a07afb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting templates for all sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template extraction: 100%|██████████| 16/16 [00:00<00:00, 103.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique templates: 1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_templates_for_source(texts, source_name):\n",
    "    config = TemplateMinerConfig()\n",
    "    config.drain_sim_th = 0.4\n",
    "    config.drain_depth = 4\n",
    "    config.drain_max_children = 100\n",
    "    \n",
    "    miner = TemplateMiner(config=config)\n",
    "    template_ids = []\n",
    "    templates = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        result = miner.add_log_message(str(text))\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        if tid not in templates:\n",
    "            templates[tid] = result[\"template_mined\"]\n",
    "    \n",
    "    return np.array(template_ids), templates\n",
    "\n",
    "def extract_all_templates():\n",
    "    print(\"\\nExtracting templates for all sources...\")\n",
    "    template_data = {}\n",
    "    global_template_map = {}\n",
    "    global_tid = 0\n",
    "    \n",
    "    for source_name in tqdm(usable_sources, desc=\"Template extraction\"):\n",
    "        texts = data_dict[source_name]['texts']\n",
    "        local_template_ids, local_templates = extract_templates_for_source(texts, source_name)\n",
    "        \n",
    "        remapped_ids = []\n",
    "        for local_tid in local_template_ids:\n",
    "            key = (source_name, local_tid)\n",
    "            if key not in global_template_map:\n",
    "                global_template_map[key] = global_tid\n",
    "                global_tid += 1\n",
    "            remapped_ids.append(global_template_map[key])\n",
    "        \n",
    "        template_data[source_name] = {\n",
    "            'template_ids': np.array(remapped_ids),\n",
    "            'templates': local_templates,\n",
    "            'n_templates': len(local_templates)\n",
    "        }\n",
    "    \n",
    "    print(f\"Total unique templates: {global_tid}\")\n",
    "    return template_data\n",
    "\n",
    "template_data = extract_all_templates()\n",
    "\n",
    "\n",
    "def normalize_timestamps(texts):\n",
    "    timestamps = np.arange(len(texts), dtype=np.float32)\n",
    "    if len(timestamps) > 1:\n",
    "        timestamps = (timestamps - timestamps.min()) / (timestamps.max() - timestamps.min() + 1e-8)\n",
    "    return timestamps\n",
    "\n",
    "def prepare_source_data(source_name):\n",
    "    texts = data_dict[source_name]['texts']\n",
    "    labels = data_dict[source_name]['labels']\n",
    "    template_ids = template_data[source_name]['template_ids']\n",
    "    timestamps = normalize_timestamps(texts)\n",
    "    source_id = source_to_id[source_name]\n",
    "    \n",
    "    if TEST_MODE and MAX_SAMPLES_PER_SOURCE is not None:\n",
    "        if len(texts) > MAX_SAMPLES_PER_SOURCE:\n",
    "            indices = np.random.choice(len(texts), MAX_SAMPLES_PER_SOURCE, replace=False)\n",
    "            texts = [texts[i] for i in indices]\n",
    "            labels = labels[indices]\n",
    "            template_ids = template_ids[indices]\n",
    "            timestamps = timestamps[indices]\n",
    "    \n",
    "    return texts, labels, template_ids, timestamps, source_id\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ecc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, texts, labels, template_ids, timestamps, source_ids):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.template_ids = template_ids\n",
    "        self.timestamps = timestamps\n",
    "        self.source_ids = source_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long),\n",
    "            'template_ids': torch.tensor(int(self.template_ids[idx]), dtype=torch.long),\n",
    "            'timestamps': torch.tensor(float(self.timestamps[idx]), dtype=torch.float32),\n",
    "            'source_ids': torch.tensor(int(self.source_ids[idx]), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "def gradient_reversal(x, lambda_=1.0):\n",
    "    return GradientReversalFunction.apply(x, lambda_)\n",
    "\n",
    "class TemplateAwareAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.template_alpha = nn.Parameter(torch.tensor(0.1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, template_ids, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.out_proj(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(x + output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TemporalModule(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temporal_embedding = nn.Linear(1, d_model)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, timestamps):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        temporal_emb = self.temporal_embedding(timestamps.unsqueeze(-1)).unsqueeze(1)\n",
    "        x = x + temporal_emb\n",
    "        \n",
    "        sorted_indices = torch.argsort(timestamps)\n",
    "        x_sorted = x[sorted_indices]\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x_sorted)\n",
    "        \n",
    "        unsorted_indices = torch.argsort(sorted_indices)\n",
    "        lstm_out = lstm_out[unsorted_indices]\n",
    "        \n",
    "        output = self.layer_norm(x + lstm_out)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "class SourceAdapter(nn.Module):\n",
    "    def __init__(self, d_model, adapter_dim=192):\n",
    "        super().__init__()\n",
    "        self.down_proj = nn.Linear(d_model, adapter_dim)\n",
    "        self.up_proj = nn.Linear(adapter_dim, d_model)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        adapter_out = self.up_proj(F.relu(self.down_proj(x)))\n",
    "        return self.alpha * x + (1 - self.alpha) * adapter_out\n",
    "\n",
    "class SourceDiscriminator(nn.Module):\n",
    "    def __init__(self, d_model, n_sources):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, n_sources)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class HLogFormer(nn.Module):\n",
    "    def __init__(self, n_sources, n_templates, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        bert_config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        bert_config.output_attentions = True\n",
    "        bert_config.output_hidden_states = True\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', config=bert_config)\n",
    "        \n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if 'embeddings' in name:\n",
    "                param.requires_grad = False\n",
    "            elif 'encoder.layer' in name:\n",
    "                layer_num = int(name.split('.')[2])\n",
    "                if layer_num < freeze_layers:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.template_embedding = nn.Embedding(n_templates + 1, D_MODEL, padding_idx=n_templates)\n",
    "        nn.init.xavier_uniform_(self.template_embedding.weight)\n",
    "        \n",
    "        self.template_attention = TemplateAwareAttention(D_MODEL, N_HEADS)\n",
    "        \n",
    "        self.temporal_module = TemporalModule(D_MODEL)\n",
    "        \n",
    "        self.source_adapters = nn.ModuleList([\n",
    "            SourceAdapter(D_MODEL) for _ in range(n_sources)\n",
    "        ])\n",
    "        \n",
    "        self.source_discriminator = SourceDiscriminator(D_MODEL, n_sources)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(D_MODEL // 2, 2)\n",
    "        )\n",
    "        \n",
    "        self.template_classifier = nn.Linear(D_MODEL, min(n_templates, 1000))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, template_ids, timestamps, source_ids):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        \n",
    "        template_emb = self.template_embedding(template_ids)\n",
    "        enhanced_output = pooled_output + template_emb\n",
    "        \n",
    "        template_attended = self.template_attention(\n",
    "            sequence_output, template_ids, attention_mask\n",
    "        )\n",
    "        template_pooled = template_attended[:, 0, :]\n",
    "        \n",
    "        combined_output = template_pooled + template_emb\n",
    "        \n",
    "        temporal_output = self.temporal_module(combined_output, timestamps)\n",
    "        \n",
    "        adapted_outputs = []\n",
    "        for i, adapter in enumerate(self.source_adapters):\n",
    "            mask = (source_ids == i)\n",
    "            if mask.any():\n",
    "                adapted = adapter(temporal_output[mask])\n",
    "                adapted_outputs.append((mask, adapted))\n",
    "        \n",
    "        final_output = temporal_output.clone()\n",
    "        for mask, adapted in adapted_outputs:\n",
    "            final_output[mask] = adapted\n",
    "        \n",
    "        logits = self.classifier(final_output)\n",
    "        \n",
    "        reversed_features = gradient_reversal(final_output, lambda_=0.1)\n",
    "        source_logits = self.source_discriminator(reversed_features)\n",
    "        \n",
    "        template_logits = self.template_classifier(final_output)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'source_logits': source_logits,\n",
    "            'template_logits': template_logits,\n",
    "            'features': final_output\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76eeb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal.mean()\n",
    "\n",
    "def temporal_consistency_loss(features, timestamps, tau=0.1):\n",
    "    sorted_indices = torch.argsort(timestamps)\n",
    "    sorted_features = features[sorted_indices]\n",
    "    sorted_times = timestamps[sorted_indices]\n",
    "    \n",
    "    if len(sorted_features) < 2:\n",
    "        return torch.tensor(0.0, device=features.device)\n",
    "    \n",
    "    feature_diff = sorted_features[1:] - sorted_features[:-1]\n",
    "    time_diff = sorted_times[1:] - sorted_times[:-1]\n",
    "    \n",
    "    weights = torch.exp(-time_diff / tau)\n",
    "    consistency = (feature_diff.pow(2).sum(dim=1) * weights).mean()\n",
    "    \n",
    "    return consistency\n",
    "\n",
    "def compute_loss(outputs, batch):\n",
    "    logits = outputs['logits']\n",
    "    source_logits = outputs['source_logits']\n",
    "    template_logits = outputs['template_logits']\n",
    "    features = outputs['features']\n",
    "    \n",
    "    labels = batch['labels']\n",
    "    source_ids = batch['source_ids']\n",
    "    template_ids = batch['template_ids']\n",
    "    timestamps = batch['timestamps']\n",
    "    \n",
    "    loss_cls = focal_loss(logits, labels)\n",
    "    \n",
    "    loss_source = F.cross_entropy(source_logits, source_ids)\n",
    "    \n",
    "    valid_template_mask = template_ids < template_logits.size(1)\n",
    "    if valid_template_mask.any():\n",
    "        loss_template = F.cross_entropy(\n",
    "            template_logits[valid_template_mask],\n",
    "            template_ids[valid_template_mask]\n",
    "        )\n",
    "    else:\n",
    "        loss_template = torch.tensor(0.0, device=logits.device)\n",
    "    \n",
    "    loss_temporal = temporal_consistency_loss(features, timestamps)\n",
    "    \n",
    "    total_loss = (\n",
    "        ALPHA_CLASSIFICATION * loss_cls +\n",
    "        ALPHA_TEMPLATE * loss_template +\n",
    "        ALPHA_TEMPORAL * loss_temporal +\n",
    "        ALPHA_SOURCE * loss_source\n",
    "    )\n",
    "    \n",
    "    return total_loss, {\n",
    "        'loss_cls': loss_cls.item(),\n",
    "        'loss_template': loss_template.item() if isinstance(loss_template, torch.Tensor) else 0.0,\n",
    "        'loss_temporal': loss_temporal.item(),\n",
    "        'loss_source': loss_source.item()\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_acc'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    per_class = {}\n",
    "    for cls in np.unique(np.concatenate([y_true, y_pred])):\n",
    "        y_true_bin = (y_true == cls).astype(int)\n",
    "        y_pred_bin = (y_pred == cls).astype(int)\n",
    "        if y_true_bin.sum() > 0:\n",
    "            per_class[int(cls)] = {\n",
    "                'precision': precision_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'recall': recall_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'f1': f1_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'support': int(y_true_bin.sum())\n",
    "            }\n",
    "    metrics['per_class'] = per_class\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "        except:\n",
    "            metrics['auroc'] = 0.0\n",
    "    else:\n",
    "        metrics['auroc'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6386124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with autocast(enabled=USE_AMP):\n",
    "            outputs = model(\n",
    "                batch['input_ids'],\n",
    "                batch['attention_mask'],\n",
    "                batch['template_ids'],\n",
    "                batch['timestamps'],\n",
    "                batch['source_ids']\n",
    "            )\n",
    "            loss, loss_dict = compute_loss(outputs, batch)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item() * GRADIENT_ACCUMULATION_STEPS})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return avg_loss, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with autocast(enabled=USE_AMP):\n",
    "                outputs = model(\n",
    "                    batch['input_ids'],\n",
    "                    batch['attention_mask'],\n",
    "                    batch['template_ids'],\n",
    "                    batch['timestamps'],\n",
    "                    batch['source_ids']\n",
    "                )\n",
    "            \n",
    "            probs = F.softmax(outputs['logits'], dim=1)\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc351a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=NUM_EPOCHS):\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * num_epochs // GRADIENT_ACCUMULATION_STEPS\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    patience = 3\n",
    "    \n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, optimizer, scheduler, scaler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_f1 = val_metrics['f1_macro']\n",
    "        print(f\"Val F1: {val_f1:.4f}, Val Balanced Acc: {val_metrics['balanced_acc']:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_f1': best_f1\n",
    "            }, MODELS_PATH / 'best_model.pt')\n",
    "            print(f\"Saved best model with F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "048fe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loso_split(split_idx, split):\n",
    "    test_source = split['test_source']\n",
    "    train_sources = [s for s in split['train_sources'] if s in usable_sources]\n",
    "    \n",
    "    if test_source not in usable_sources:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Split {split_idx + 1}: Test on {test_source}\")\n",
    "    print(f\"Train sources: {train_sources}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    train_texts_list = []\n",
    "    train_labels_list = []\n",
    "    train_template_ids_list = []\n",
    "    train_timestamps_list = []\n",
    "    train_source_ids_list = []\n",
    "    \n",
    "    for source in train_sources:\n",
    "        texts, labels, template_ids, timestamps, source_id = prepare_source_data(source)\n",
    "        train_texts_list.extend(texts)\n",
    "        train_labels_list.extend(labels)\n",
    "        train_template_ids_list.extend(template_ids)\n",
    "        train_timestamps_list.extend(timestamps)\n",
    "        train_source_ids_list.extend([source_id] * len(texts))\n",
    "    \n",
    "    test_texts, test_labels, test_template_ids, test_timestamps, test_source_id = prepare_source_data(test_source)\n",
    "    \n",
    "    if len(np.unique(test_labels)) < 2:\n",
    "        print(f\"Skipping {test_source}: single class\")\n",
    "        return None\n",
    "    \n",
    "    train_dataset = LogDataset(\n",
    "        train_texts_list,\n",
    "        train_labels_list,\n",
    "        train_template_ids_list,\n",
    "        train_timestamps_list,\n",
    "        train_source_ids_list\n",
    "    )\n",
    "    \n",
    "    test_dataset = LogDataset(\n",
    "        test_texts,\n",
    "        test_labels,\n",
    "        test_template_ids,\n",
    "        test_timestamps,\n",
    "        [test_source_id] * len(test_texts)\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    max_template_id = max(\n",
    "        max(train_template_ids_list),\n",
    "        max(test_template_ids)\n",
    "    )\n",
    "    n_templates = min(max_template_id + 1, N_TEMPLATES)\n",
    "    \n",
    "    model = HLogFormer(N_SOURCES, n_templates, FREEZE_BERT_LAYERS).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    best_f1 = train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "    checkpoint = torch.load(MODELS_PATH / 'best_model.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nTest Results for {test_source}:\")\n",
    "    print(f\"F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Balanced Acc: {test_metrics['balanced_acc']:.4f}\")\n",
    "    print(f\"AUROC: {test_metrics['auroc']:.4f}\")\n",
    "    print(f\"MCC: {test_metrics['mcc']:.4f}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        'split_idx': split_idx,\n",
    "        'test_source': test_source,\n",
    "        'train_sources': train_sources,\n",
    "        'f1_macro': test_metrics['f1_macro'],\n",
    "        'balanced_acc': test_metrics['balanced_acc'],\n",
    "        'auroc': test_metrics['auroc'],\n",
    "        'mcc': test_metrics['mcc'],\n",
    "        'per_class': test_metrics['per_class']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c27044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 16 splits...\n",
      "\n",
      "================================================================================\n",
      "Split 1: Test on Android_2k\n",
      "Train sources: ['Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4612, Train F1: 0.4571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4101, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4101\n",
      "GPU Memory: 1.31 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Android_2k:\n",
      "F1-Macro: 0.4967\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5884\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 2: Test on Apache_2k\n",
      "Train sources: ['Android_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4951, Train F1: 0.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4157, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4157\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Apache_2k:\n",
      "F1-Macro: 0.4167\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.2279\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 3: Test on BGL_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.5570, Train F1: 0.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4091, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4091\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for BGL_2k:\n",
      "F1-Macro: 0.4781\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.3860\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 4: Test on Hadoop_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3354, Train F1: 0.4368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4202, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4202\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Hadoop_2k:\n",
      "F1-Macro: 0.2565\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.6640\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 5: Test on HDFS_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping HDFS_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 6: Test on HealthApp_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4078, Train F1: 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4036, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4036\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for HealthApp_2k:\n",
      "F1-Macro: 0.4986\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.3180\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 7: Test on HPC_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.2749, Train F1: 0.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4137, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4137\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for HPC_2k:\n",
      "F1-Macro: 0.3569\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5120\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 8: Test on Linux_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.2639, Train F1: 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4261, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4261\n",
      "GPU Memory: 1.32 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Linux_2k:\n",
      "F1-Macro: 0.0449\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.7978\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 9: Test on Mac_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3972, Train F1: 0.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4099, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4099\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Mac_2k:\n",
      "F1-Macro: 0.4393\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4775\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 10: Test on OpenSSH_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping OpenSSH_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 11: Test on OpenStack_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Skipping OpenStack_2k: single class\n",
      "\n",
      "================================================================================\n",
      "Split 12: Test on Proxifier_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4791, Train F1: 0.4145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4089, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4089\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Proxifier_2k:\n",
      "F1-Macro: 0.4876\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5050\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 13: Test on Spark_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4308, Train F1: 0.4261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4078, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4078\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Spark_2k:\n",
      "F1-Macro: 0.4990\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.9651\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 14: Test on Thunderbird_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Windows_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3277, Train F1: 0.4371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4108, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4108\n",
      "GPU Memory: 1.33 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Thunderbird_2k:\n",
      "F1-Macro: 0.4744\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5017\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 15: Test on Windows_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Zookeeper_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3205, Train F1: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4169, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4169\n",
      "GPU Memory: 1.32 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Windows_2k:\n",
      "F1-Macro: 0.3768\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.4784\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Split 16: Test on Zookeeper_2k\n",
      "Train sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HDFS_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'OpenSSH_2k', 'OpenStack_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k']\n",
      "================================================================================\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "Training batches per epoch: 843\n",
      "Validation batches: 47\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.5105, Train F1: 0.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4130, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4130\n",
      "GPU Memory: 1.32 GB / 8.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results for Zookeeper_2k:\n",
      "F1-Macro: 0.4263\n",
      "Balanced Acc: 0.5000\n",
      "AUROC: 0.5167\n",
      "MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "   Test Source  F1-Macro  Balanced Acc    AUROC  MCC\n",
      "      Spark_2k  0.498998           0.5 0.965079  0.0\n",
      "  HealthApp_2k  0.498621           0.5 0.318022  0.0\n",
      "    Android_2k  0.496729           0.5 0.588438  0.0\n",
      "  Proxifier_2k  0.487574           0.5 0.504968  0.0\n",
      "        BGL_2k  0.478079           0.5 0.385972  0.0\n",
      "Thunderbird_2k  0.474376           0.5 0.501700  0.0\n",
      "        Mac_2k  0.439305           0.5 0.477462  0.0\n",
      "  Zookeeper_2k  0.426277           0.5 0.516688  0.0\n",
      "     Apache_2k  0.416740           0.5 0.227855  0.0\n",
      "    Windows_2k  0.376753           0.5 0.478395  0.0\n",
      "        HPC_2k  0.356913           0.5 0.512049  0.0\n",
      "     Hadoop_2k  0.256506           0.5 0.663973  0.0\n",
      "      Linux_2k  0.044890           0.5 0.797758  0.0\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "splits_to_process = splits[:MAX_SPLITS] if TEST_MODE and MAX_SPLITS else splits\n",
    "print(f\"\\nProcessing {len(splits_to_process)} splits...\")\n",
    "\n",
    "for split_idx, split in enumerate(splits_to_process):\n",
    "    result = run_loso_split(split_idx, split)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "\n",
    "if not all_results:\n",
    "    print(\"No results generated\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'Test Source': r['test_source'],\n",
    "    'F1-Macro': r['f1_macro'],\n",
    "    'Balanced Acc': r['balanced_acc'],\n",
    "    'AUROC': r['auroc'],\n",
    "    'MCC': r['mcc']\n",
    "} for r in all_results])\n",
    "\n",
    "results_df = results_df.sort_values('F1-Macro', ascending=False)\n",
    "print(\"\\n\" + results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3819442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\hlogformer\\results_20251123_233729\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = RESULTS_PATH / f\"results_{timestamp}\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_df.to_csv(results_dir / \"loso_results.csv\", index=False)\n",
    "\n",
    "with open(results_dir / \"complete_results.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_results': all_results,\n",
    "        'summary': results_df.to_dict('records'),\n",
    "        'config': {\n",
    "            'max_seq_len': MAX_SEQ_LEN,\n",
    "            'd_model': D_MODEL,\n",
    "            'n_heads': N_HEADS,\n",
    "            'n_layers': N_LAYERS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'num_epochs': NUM_EPOCHS,\n",
    "            'freeze_bert_layers': FREEZE_BERT_LAYERS\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b282bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING FINAL PRODUCTION MODEL ON ALL DATA\n",
      "================================================================================\n",
      "\n",
      "Preparing all available data...\n",
      "Total samples: 32,000\n",
      "Label distribution: {np.int32(0): np.int64(23080), np.int32(1): np.int64(8920)}\n",
      "Train samples: 28,800\n",
      "Val samples: 3,200\n",
      "\n",
      "Initializing final model...\n",
      "Templates: 1596\n",
      "Sources: 16\n",
      "Model parameters: 128,625,675\n",
      "Trainable parameters: 62,261,259\n",
      "\n",
      "Training for 1 epochs...\n",
      "Training batches per epoch: 900\n",
      "Validation batches: 50\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4232, Train F1: 0.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4208, Val Balanced Acc: 0.5000\n",
      "Saved best model with F1: 0.4208\n",
      "GPU Memory: 1.07 GB / 8.59 GB\n",
      "\n",
      "================================================================================\n",
      "FINAL PRODUCTION MODEL SAVED\n",
      "================================================================================\n",
      "Location: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\hlogformer\\final_production_model.pt\n",
      "Best F1: 0.4208\n",
      "Training samples: 32,000\n",
      "Sources: 16\n",
      "Templates: 1596\n",
      "\n",
      "To load for inference:\n",
      "  checkpoint = torch.load('models/hlogformer/final_production_model.pt')\n",
      "  model = HLogFormer(checkpoint['n_sources'], checkpoint['n_templates'])\n",
      "  model.load_state_dict(checkpoint['model_state_dict'])\n",
      "  model.eval()\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FINAL_MODEL = True\n",
    "if TRAIN_FINAL_MODEL:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FINAL PRODUCTION MODEL ON ALL DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nPreparing all available data...\")\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    all_template_ids = []\n",
    "    all_timestamps = []\n",
    "    all_source_ids = []\n",
    "    \n",
    "    for source_name in usable_sources:\n",
    "        texts, labels, template_ids, timestamps, source_id = prepare_source_data(source_name)\n",
    "        all_texts.extend(texts)\n",
    "        all_labels.extend(labels)\n",
    "        all_template_ids.extend(template_ids)\n",
    "        all_timestamps.extend(timestamps)\n",
    "        all_source_ids.extend([source_id] * len(texts))\n",
    "    \n",
    "    print(f\"Total samples: {len(all_texts):,}\")\n",
    "    print(f\"Label distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}\")\n",
    "    \n",
    "    full_dataset = LogDataset(\n",
    "        all_texts,\n",
    "        all_labels,\n",
    "        all_template_ids,\n",
    "        all_timestamps,\n",
    "        all_source_ids\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {train_size:,}\")\n",
    "    print(f\"Val samples: {val_size:,}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    max_template_id = max(all_template_ids)\n",
    "    n_templates = min(max_template_id + 1, N_TEMPLATES)\n",
    "    \n",
    "    print(f\"\\nInitializing final model...\")\n",
    "    print(f\"Templates: {n_templates}\")\n",
    "    print(f\"Sources: {N_SOURCES}\")\n",
    "    \n",
    "    final_model = HLogFormer(N_SOURCES, n_templates, FREEZE_BERT_LAYERS).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in final_model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in final_model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    final_epochs = 1 if TEST_MODE else 10\n",
    "    print(f\"\\nTraining for {final_epochs} epochs...\")\n",
    "    \n",
    "    best_f1 = train_model(final_model, train_loader, val_loader, device, num_epochs=final_epochs)\n",
    "    \n",
    "    checkpoint = torch.load(MODELS_PATH / 'best_model.pt')\n",
    "    final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    final_model_path = MODELS_PATH / 'final_production_model.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'n_sources': N_SOURCES,\n",
    "        'n_templates': n_templates,\n",
    "        'config': {\n",
    "            'max_seq_len': MAX_SEQ_LEN,\n",
    "            'd_model': D_MODEL,\n",
    "            'n_heads': N_HEADS,\n",
    "            'n_layers': N_LAYERS,\n",
    "            'freeze_bert_layers': FREEZE_BERT_LAYERS\n",
    "        },\n",
    "        'training_samples': len(all_texts),\n",
    "        'best_f1': best_f1,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'source_to_id': source_to_id,\n",
    "        'id_to_source': id_to_source\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PRODUCTION MODEL SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Location: {final_model_path}\")\n",
    "    print(f\"Best F1: {best_f1:.4f}\")\n",
    "    print(f\"Training samples: {len(all_texts):,}\")\n",
    "    print(f\"Sources: {N_SOURCES}\")\n",
    "    print(f\"Templates: {n_templates}\")\n",
    "    print(\"\\nTo load for inference:\")\n",
    "    print(\"  checkpoint = torch.load('models/hlogformer/final_production_model.pt')\")\n",
    "    print(\"  model = HLogFormer(checkpoint['n_sources'], checkpoint['n_templates'])\")\n",
    "    print(\"  model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "    print(\"  model.eval()\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    del final_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
