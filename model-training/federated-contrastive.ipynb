{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e051316",
   "metadata": {},
   "source": [
    "Federated Contrastive Learning for Privacy-Preserving Cross-Source Log Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bae0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15852496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory: 8.59 GB\n",
      "\n",
      "================================================================================\n",
      "TEST MODE ENABLED\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - 2 splits only (Apache, Linux)\n",
      "  - 2 rounds per split\n",
      "  - 2 clients per split\n",
      "  - Reduced batch size: 16\n",
      "  - Max 500 pairs per client\n",
      "Set TEST_MODE = False for full training\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE = True\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST MODE ENABLED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Configuration:\")\n",
    "    print(\"  - 2 splits only (Apache, Linux)\")\n",
    "    print(\"  - 2 rounds per split\")\n",
    "    print(\"  - 2 clients per split\")\n",
    "    print(\"  - Reduced batch size: 16\")\n",
    "    print(\"  - Max 500 pairs per client\")\n",
    "    print(\"Set TEST_MODE = False for full training\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87283e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "DATA_PATH = ROOT / \"dataset\" / \"labeled_data\" / \"normalized\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"federated_contrastive\"\n",
    "MODELS_PATH = ROOT / \"models\" / \"federated_contrastive\"\n",
    "\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCLUDED_SOURCES = ['HDFS_2k', 'OpenSSH_2k', 'OpenStack_2k']\n",
    "\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "MAX_LENGTH = 64\n",
    "PROJECTION_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "NUM_ROUNDS = 2 if TEST_MODE else 10\n",
    "LOCAL_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "LR_ENCODER = 2e-5\n",
    "LR_HEAD = 1e-3\n",
    "ACCUMULATION_STEPS = 2\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "LAMBDA_CONTRASTIVE = 0.5\n",
    "LAMBDA_FOCAL = 0.3\n",
    "LAMBDA_TEMPLATE = 0.2\n",
    "\n",
    "ALPHA_SAMPLES = 0.3\n",
    "BETA_TEMPLATES = 0.4\n",
    "GAMMA_IMBALANCE = 0.3\n",
    "\n",
    "TEMPERATURE = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ec0ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...\n",
      "Loading splits...\n",
      "Usable sources: 13\n",
      "Sources: ['Android_2k', 'Apache_2k', 'BGL_2k', 'Hadoop_2k', 'HealthApp_2k', 'HPC_2k', 'Linux_2k', 'Mac_2k', 'Proxifier_2k', 'Spark_2k', 'Thunderbird_2k', 'Windows_2k', 'Zookeeper_2k']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading features...\")\n",
    "feat_file = FEAT_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "with open(feat_file, 'rb') as f:\n",
    "    feat_data = pickle.load(f)\n",
    "    data_dict = feat_data['hybrid_features_data']\n",
    "\n",
    "print(\"Loading splits...\")\n",
    "split_file = FEAT_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "with open(split_file, 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    splits = split_data['splits']\n",
    "\n",
    "usable_sources = [s for s in data_dict.keys() if s not in EXCLUDED_SOURCES and data_dict[s]['labels'] is not None]\n",
    "print(f\"Usable sources: {len(usable_sources)}\")\n",
    "print(f\"Sources: {usable_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_templates(texts, source_name):\n",
    "    config = TemplateMinerConfig()\n",
    "    config.drain_sim_th = 0.4\n",
    "    config.drain_depth = 4\n",
    "    config.drain_max_children = 100\n",
    "    \n",
    "    miner = TemplateMiner(config=config)\n",
    "    template_ids = []\n",
    "    templates = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        result = miner.add_log_message(str(text))\n",
    "        tid = result[\"cluster_id\"]\n",
    "        template_ids.append(tid)\n",
    "        if tid not in templates:\n",
    "            templates[tid] = result[\"template_mined\"]\n",
    "    \n",
    "    return np.array(template_ids), templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8f68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrastive_pairs(texts, labels, template_ids, source_name, augment=False):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    label_to_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "    \n",
    "    for idx, (text, label, tid) in enumerate(zip(texts, labels, template_ids)):\n",
    "        same_label_indices = label_to_indices[label]\n",
    "        same_label_indices = same_label_indices[same_label_indices != idx]\n",
    "        \n",
    "        if len(same_label_indices) > 0:\n",
    "            pos_idx = np.random.choice(same_label_indices)\n",
    "            pairs.append((idx, pos_idx, 1))\n",
    "            pair_labels.append(label)\n",
    "        \n",
    "        diff_labels = [l for l in unique_labels if l != label]\n",
    "        if len(diff_labels) > 0:\n",
    "            neg_label = np.random.choice(diff_labels)\n",
    "            neg_indices = label_to_indices[neg_label]\n",
    "            if len(neg_indices) > 0:\n",
    "                neg_idx = np.random.choice(neg_indices)\n",
    "                pairs.append((idx, neg_idx, 0))\n",
    "                pair_labels.append(label)\n",
    "    \n",
    "    same_template_indices = defaultdict(list)\n",
    "    for idx, tid in enumerate(template_ids):\n",
    "        same_template_indices[tid].append(idx)\n",
    "    \n",
    "    for tid, indices in same_template_indices.items():\n",
    "        if len(indices) > 1:\n",
    "            for i in range(len(indices) - 1):\n",
    "                idx1 = indices[i]\n",
    "                idx2 = indices[i + 1]\n",
    "                if labels[idx1] == labels[idx2]:\n",
    "                    pairs.append((idx1, idx2, 1))\n",
    "                    pair_labels.append(labels[idx1])\n",
    "    \n",
    "    if augment:\n",
    "        minority_label = np.argmin([len(label_to_indices[l]) for l in unique_labels])\n",
    "        minority_indices = label_to_indices[minority_label]\n",
    "        \n",
    "        for _ in range(min(len(minority_indices) * 3, 1500)):\n",
    "            if len(minority_indices) > 1:\n",
    "                idx1, idx2 = np.random.choice(minority_indices, 2, replace=False)\n",
    "                pairs.append((idx1, idx2, 1))\n",
    "                pair_labels.append(minority_label)\n",
    "    \n",
    "    return pairs, pair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1e5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePairDataset(Dataset):\n",
    "    def __init__(self, texts, labels, template_ids, pairs, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.template_ids = template_ids\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, is_similar = self.pairs[idx]\n",
    "        \n",
    "        text1 = str(self.texts[idx1])\n",
    "        text2 = str(self.texts[idx2])\n",
    "        label1 = int(self.labels[idx1])\n",
    "        label2 = int(self.labels[idx2])\n",
    "        tid1 = int(self.template_ids[idx1])\n",
    "        tid2 = int(self.template_ids[idx2])\n",
    "        \n",
    "        enc1 = self.tokenizer(text1, max_length=self.max_length, padding='max_length', \n",
    "                             truncation=True, return_tensors='pt')\n",
    "        enc2 = self.tokenizer(text2, max_length=self.max_length, padding='max_length', \n",
    "                             truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids1': enc1['input_ids'].squeeze(0),\n",
    "            'attention_mask1': enc1['attention_mask'].squeeze(0),\n",
    "            'input_ids2': enc2['input_ids'].squeeze(0),\n",
    "            'attention_mask2': enc2['attention_mask'].squeeze(0),\n",
    "            'label1': torch.tensor(label1, dtype=torch.long),\n",
    "            'label2': torch.tensor(label2, dtype=torch.long),\n",
    "            'template_id1': torch.tensor(tid1, dtype=torch.long),\n",
    "            'template_id2': torch.tensor(tid2, dtype=torch.long),\n",
    "            'is_similar': torch.tensor(is_similar, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class TemplateAwareAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_templates):\n",
    "        super().__init__()\n",
    "        self.template_embeddings = nn.Embedding(num_templates + 1, hidden_dim)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, template_ids):\n",
    "        template_emb = self.template_embeddings(template_ids).unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x.unsqueeze(1), template_emb, template_emb)\n",
    "        return self.norm(x + attn_out.squeeze(1))\n",
    "\n",
    "class FedLogCLModel(nn.Module):\n",
    "    def __init__(self, model_name, projection_dim, hidden_dim, num_templates, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.template_attention = TemplateAwareAttention(projection_dim, num_templates)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(projection_dim, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 4, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, template_ids=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output\n",
    "        projected = self.projection_head(pooled)\n",
    "        \n",
    "        if template_ids is not None:\n",
    "            projected = self.template_attention(projected, template_ids)\n",
    "        \n",
    "        logits = self.classifier(projected)\n",
    "        return projected, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd7a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z1, z2, is_similar, temperature=0.07):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    \n",
    "    similarity = torch.matmul(z1, z2.T) / temperature\n",
    "    \n",
    "    batch_size = z1.size(0)\n",
    "    labels = torch.arange(batch_size).to(z1.device)\n",
    "    \n",
    "    loss_12 = F.cross_entropy(similarity, labels)\n",
    "    loss_21 = F.cross_entropy(similarity.T, labels)\n",
    "    \n",
    "    contrastive = (loss_12 + loss_21) / 2\n",
    "    \n",
    "    cosine_sim = F.cosine_similarity(z1, z2)\n",
    "    alignment = (1 - cosine_sim * is_similar - (1 - cosine_sim) * (1 - is_similar)).mean()\n",
    "    \n",
    "    return contrastive + 0.5 * alignment\n",
    "\n",
    "def focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal.mean()\n",
    "\n",
    "def template_alignment_loss(z1, z2, tid1, tid2):\n",
    "    same_template = (tid1 == tid2).float()\n",
    "    similarity = F.cosine_similarity(z1, z2)\n",
    "    loss = F.binary_cross_entropy_with_logits(similarity, same_template)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b2c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client(model, dataloader, optimizer, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        label1 = batch['label1'].to(device)\n",
    "        label2 = batch['label2'].to(device)\n",
    "        tid1 = batch['template_id1'].to(device)\n",
    "        tid2 = batch['template_id2'].to(device)\n",
    "        is_similar = batch['is_similar'].to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            z1, logits1 = model(input_ids1, attention_mask1, tid1)\n",
    "            z2, logits2 = model(input_ids2, attention_mask2, tid2)\n",
    "            \n",
    "            loss_contrastive = contrastive_loss(z1, z2, is_similar, TEMPERATURE)\n",
    "            loss_focal = (focal_loss(logits1, label1) + focal_loss(logits2, label2)) / 2\n",
    "            loss_template = template_alignment_loss(z1, z2, tid1, tid2)\n",
    "            \n",
    "            loss = (LAMBDA_CONTRASTIVE * loss_contrastive + \n",
    "                   LAMBDA_FOCAL * loss_focal + \n",
    "                   LAMBDA_TEMPLATE * loss_template)\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_client(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids1'].to(device)\n",
    "            attention_mask = batch['attention_mask1'].to(device)\n",
    "            labels = batch['label1'].to(device)\n",
    "            tids = batch['template_id1'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                _, logits = model(input_ids, attention_mask, tids)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    auroc = 0.0\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return f1, bal_acc, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c74451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging(global_model, client_models, client_weights):\n",
    "    global_dict = global_model.state_dict()\n",
    "    \n",
    "    for key in global_dict.keys():\n",
    "        if 'template_embeddings' in key:\n",
    "            continue\n",
    "        \n",
    "        global_dict[key] = torch.zeros_like(global_dict[key], dtype=torch.float32)\n",
    "        \n",
    "        for client_model, weight in zip(client_models, client_weights):\n",
    "            client_dict = client_model.state_dict()\n",
    "            if key in client_dict and client_dict[key].shape == global_dict[key].shape:\n",
    "                global_dict[key] += weight * client_dict[key].float()\n",
    "    \n",
    "    global_model.load_state_dict(global_dict, strict=False)\n",
    "\n",
    "def calculate_client_weights(client_data_sizes, client_template_counts, client_imbalance_ratios):\n",
    "    weights = []\n",
    "    \n",
    "    for size, templates, imbalance in zip(client_data_sizes, client_template_counts, client_imbalance_ratios):\n",
    "        w_samples = size\n",
    "        w_templates = templates\n",
    "        w_imbalance = 1.0 / (imbalance + 1e-6)\n",
    "        \n",
    "        weight = ALPHA_SAMPLES * w_samples + BETA_TEMPLATES * w_templates + GAMMA_IMBALANCE * w_imbalance\n",
    "        weights.append(weight)\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1627b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_federated_round(global_model, client_data, tokenizer, round_num, device):\n",
    "    client_models = []\n",
    "    client_weights_data = []\n",
    "    \n",
    "    for client_name, data in client_data.items():\n",
    "        print(f\"  Training client: {client_name}\")\n",
    "        \n",
    "        client_model = FedLogCLModel(\n",
    "            BERT_MODEL, PROJECTION_DIM, HIDDEN_DIM, \n",
    "            data['num_templates'], num_classes=2\n",
    "        ).to(device)\n",
    "        \n",
    "        global_state = global_model.state_dict()\n",
    "        client_state = client_model.state_dict()\n",
    "        \n",
    "        for key in global_state.keys():\n",
    "            if 'template_embeddings' not in key:\n",
    "                client_state[key] = global_state[key]\n",
    "        \n",
    "        client_model.load_state_dict(client_state)\n",
    "        \n",
    "        train_dataset = ContrastivePairDataset(\n",
    "            data['train_texts'], data['train_labels'], data['train_template_ids'],\n",
    "            data['train_pairs'], tokenizer, MAX_LENGTH\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        \n",
    "        val_dataset = ContrastivePairDataset(\n",
    "            data['val_texts'], data['val_labels'], data['val_template_ids'],\n",
    "            data['val_pairs'], tokenizer, MAX_LENGTH\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0)\n",
    "        \n",
    "        encoder_params = list(client_model.encoder.parameters())\n",
    "        head_params = list(client_model.projection_head.parameters()) + \\\n",
    "                     list(client_model.template_attention.parameters()) + \\\n",
    "                     list(client_model.classifier.parameters())\n",
    "        \n",
    "        optimizer = AdamW([\n",
    "            {'params': encoder_params, 'lr': LR_ENCODER},\n",
    "            {'params': head_params, 'lr': LR_HEAD}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        total_steps = len(train_loader) * LOCAL_EPOCHS\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        for epoch in range(LOCAL_EPOCHS):\n",
    "            train_loss = train_client(client_model, train_loader, optimizer, scheduler, scaler, device)\n",
    "        \n",
    "        val_f1, val_bal_acc, val_auroc = evaluate_client(client_model, val_loader, device)\n",
    "        print(f\"    Val F1: {val_f1:.4f}, Bal Acc: {val_bal_acc:.4f}, AUROC: {val_auroc:.4f}\")\n",
    "        \n",
    "        client_models.append(client_model)\n",
    "        \n",
    "        unique, counts = np.unique(data['train_labels'], return_counts=True)\n",
    "        imbalance = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "        \n",
    "        client_weights_data.append({\n",
    "            'size': len(data['train_labels']),\n",
    "            'templates': data['num_templates'],\n",
    "            'imbalance': imbalance\n",
    "        })\n",
    "        \n",
    "        del train_loader, val_loader, optimizer, scheduler, scaler\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    sizes = [w['size'] for w in client_weights_data]\n",
    "    templates = [w['templates'] for w in client_weights_data]\n",
    "    imbalances = [w['imbalance'] for w in client_weights_data]\n",
    "    \n",
    "    weights = calculate_client_weights(sizes, templates, imbalances)\n",
    "    \n",
    "    print(f\"  Aggregating {len(client_models)} clients...\")\n",
    "    federated_averaging(global_model, client_models, weights)\n",
    "    \n",
    "    for client_model in client_models:\n",
    "        del client_model\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fc2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_client_data(source_name, data_dict, tokenizer, augment=False):\n",
    "    texts = data_dict[source_name]['texts']\n",
    "    labels = data_dict[source_name]['labels']\n",
    "    \n",
    "    template_ids, templates = extract_templates(texts, source_name)\n",
    "    num_templates = len(templates)\n",
    "    \n",
    "    train_texts, val_texts, train_labels, val_labels, train_tids, val_tids = train_test_split(\n",
    "        texts, labels, template_ids, test_size=0.2, random_state=SEED, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_pairs, _ = create_contrastive_pairs(train_texts, train_labels, train_tids, source_name, augment)\n",
    "    val_pairs, _ = create_contrastive_pairs(val_texts, val_labels, val_tids, source_name, False)\n",
    "    \n",
    "    return {\n",
    "        'train_texts': train_texts,\n",
    "        'train_labels': train_labels,\n",
    "        'train_template_ids': train_tids,\n",
    "        'train_pairs': train_pairs,\n",
    "        'val_texts': val_texts,\n",
    "        'val_labels': val_labels,\n",
    "        'val_template_ids': val_tids,\n",
    "        'val_pairs': val_pairs,\n",
    "        'num_templates': num_templates,\n",
    "        'templates': templates\n",
    "    }\n",
    "\n",
    "def evaluate_global_model(global_model, test_texts, test_labels, test_template_ids, tokenizer, device):\n",
    "    global_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_embeddings = []\n",
    "    \n",
    "    test_pairs = [(i, i, 1) for i in range(len(test_texts))]\n",
    "    test_dataset = ContrastivePairDataset(\n",
    "        test_texts, test_labels, test_template_ids, test_pairs, tokenizer, MAX_LENGTH\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids1'].to(device)\n",
    "            attention_mask = batch['attention_mask1'].to(device)\n",
    "            labels = batch['label1'].to(device)\n",
    "            tids = batch['template_id1'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                embeddings, logits = global_model(input_ids, attention_mask, tids)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_embeddings.extend(embeddings.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    auroc = 0.0\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return f1, bal_acc, auroc, all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b0084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loso_split(split_idx, split, data_dict, tokenizer, device):\n",
    "    test_source = split['test_source']\n",
    "    train_sources = [s for s in split['train_sources'] if s in usable_sources]\n",
    "    \n",
    "    if test_source not in usable_sources:\n",
    "        return None\n",
    "    \n",
    "    if TEST_MODE:\n",
    "        train_sources = train_sources[:2]\n",
    "    \n",
    "    print(f\"\\nSplit {split_idx + 1}: Test on {test_source}\")\n",
    "    print(f\"Train sources: {train_sources}\")\n",
    "    \n",
    "    test_texts = data_dict[test_source]['texts']\n",
    "    test_labels = data_dict[test_source]['labels']\n",
    "    test_template_ids, _ = extract_templates(test_texts, test_source)\n",
    "    \n",
    "    if len(np.unique(test_labels)) < 2:\n",
    "        print(f\"Skipping {test_source}: single class\")\n",
    "        return None\n",
    "    \n",
    "    augment_sources = ['HealthApp_2k', 'Spark_2k']\n",
    "    \n",
    "    client_data = {}\n",
    "    max_templates = 0\n",
    "    \n",
    "    for source in train_sources:\n",
    "        augment = source in augment_sources\n",
    "        client_data[source] = prepare_client_data(source, data_dict, tokenizer, augment)\n",
    "        max_templates = max(max_templates, client_data[source]['num_templates'])\n",
    "    \n",
    "    global_model = FedLogCLModel(\n",
    "        BERT_MODEL, PROJECTION_DIM, HIDDEN_DIM, \n",
    "        max_templates, num_classes=2\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        global_model.encoder.gradient_checkpointing_enable()\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    history = {'train_loss': [], 'val_f1': [], 'test_f1': []}\n",
    "    \n",
    "    for round_num in range(NUM_ROUNDS):\n",
    "        print(f\"\\nRound {round_num + 1}/{NUM_ROUNDS}\")\n",
    "        \n",
    "        global_model = run_federated_round(global_model, client_data, tokenizer, round_num, device)\n",
    "        \n",
    "        test_f1, test_bal_acc, test_auroc, test_embeddings = evaluate_global_model(\n",
    "            global_model, test_texts, test_labels, test_template_ids, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        print(f\"  Test F1: {test_f1:.4f}, Bal Acc: {test_bal_acc:.4f}, AUROC: {test_auroc:.4f}\")\n",
    "        \n",
    "        history['test_f1'].append(test_f1)\n",
    "        \n",
    "        if round_num == 0:\n",
    "            print(f\"  Round 1 validation check: F1={test_f1:.4f}\")\n",
    "        if round_num == 4:\n",
    "            print(f\"  Round 5 validation check: F1={test_f1:.4f}\")\n",
    "        \n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = global_model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"  Early stopping at round {round_num + 1}\")\n",
    "                break\n",
    "        \n",
    "        checkpoint_path = MODELS_PATH / f\"split_{split_idx}_round_{round_num}.pt\"\n",
    "        torch.save({\n",
    "            'round': round_num,\n",
    "            'model_state': global_model.state_dict(),\n",
    "            'test_f1': test_f1,\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        global_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    final_f1, final_bal_acc, final_auroc, final_embeddings = evaluate_global_model(\n",
    "        global_model, test_texts, test_labels, test_template_ids, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    embeddings_path = MODELS_PATH / f\"split_{split_idx}_embeddings.npy\"\n",
    "    np.save(embeddings_path, final_embeddings)\n",
    "    \n",
    "    del global_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'split_idx': split_idx,\n",
    "        'test_source': test_source,\n",
    "        'train_sources': train_sources,\n",
    "        'best_f1': best_f1,\n",
    "        'final_f1': final_f1,\n",
    "        'final_bal_acc': final_bal_acc,\n",
    "        'final_auroc': final_auroc,\n",
    "        'history': history,\n",
    "        'embeddings_path': str(embeddings_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82945566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing tokenizer...\n",
      "\n",
      "Starting LOSO evaluation...\n",
      "TEST MODE: Processing 2 splits (Apache, Linux)\n",
      "\n",
      "Split 1: Test on Apache_2k\n",
      "Train sources: ['Android_2k', 'BGL_2k']\n",
      "\n",
      "Round 1/2\n",
      "  Training client: Android_2k\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "print(\"\\nStarting LOSO evaluation...\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "usable_splits = [s for s in splits if s['test_source'] in usable_sources]\n",
    "\n",
    "if TEST_MODE:\n",
    "    test_sources_to_use = ['Apache_2k', 'Linux_2k']\n",
    "    usable_splits = [s for s in usable_splits if s['test_source'] in test_sources_to_use]\n",
    "    print(f\"TEST MODE: Processing {len(usable_splits)} splits (Apache, Linux)\")\n",
    "else:\n",
    "    print(f\"Processing {len(usable_splits)} splits\")\n",
    "\n",
    "for split_idx, split in enumerate(usable_splits):\n",
    "    result = run_loso_split(split_idx, split, data_dict, tokenizer, device)\n",
    "    \n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(f\"\\nCompleted {split_idx + 1}/{len(usable_splits)}\")\n",
    "        print(f\"Best F1: {result['best_f1']:.4f}\")\n",
    "        print(f\"Final F1: {result['final_f1']:.4f}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59843ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING FINAL BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find the split with the best F1 score\n",
    "best_result = max(all_results, key=lambda x: x['final_f1'])\n",
    "best_split_idx = best_result['split_idx']\n",
    "\n",
    "print(f\"Best performing model from Split {best_split_idx + 1}\")\n",
    "print(f\"  Test Source: {best_result['test_source']}\")\n",
    "print(f\"  Final F1: {best_result['final_f1']:.4f}\")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "best_checkpoint_files = list(MODELS_PATH.glob(f\"split_{best_split_idx}_round_*.pt\"))\n",
    "if best_checkpoint_files:\n",
    "    # Find the checkpoint with the best F1\n",
    "    best_checkpoint = None\n",
    "    best_checkpoint_f1 = 0\n",
    "    \n",
    "    for checkpoint_file in best_checkpoint_files:\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        if checkpoint['test_f1'] > best_checkpoint_f1:\n",
    "            best_checkpoint_f1 = checkpoint['test_f1']\n",
    "            best_checkpoint = checkpoint\n",
    "    \n",
    "    # Save the final production model\n",
    "    final_model_path = MODELS_PATH / \"final_best_model.pt\"\n",
    "    torch.save({\n",
    "        'model_state': best_checkpoint['model_state'],\n",
    "        'test_source': best_result['test_source'],\n",
    "        'f1_score': best_result['final_f1'],\n",
    "        'balanced_acc': best_result['final_bal_acc'],\n",
    "        'auroc': best_result['final_auroc'],\n",
    "        'config': {\n",
    "            'bert_model': BERT_MODEL,\n",
    "            'projection_dim': PROJECTION_DIM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'num_classes': 2\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"\\nFinal model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEDERATED CONTRASTIVE LEARNING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Test Source  Best F1  Final F1  Balanced Acc    AUROC\n",
      "  Apache_2k  0.41674   0.41674           0.5 0.041789\n",
      "   Linux_2k  0.04489   0.04489           0.5 0.808938\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEDERATED CONTRASTIVE LEARNING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not all_results:\n",
    "    print(\"No results generated\")\n",
    "    sys.exit(1)\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'Test Source': r['test_source'],\n",
    "    'Best F1': r['best_f1'],\n",
    "    'Final F1': r['final_f1'],\n",
    "    'Balanced Acc': r['final_bal_acc'],\n",
    "    'AUROC': r['final_auroc']\n",
    "} for r in all_results])\n",
    "\n",
    "results_df = results_df.sort_values('Final F1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ace15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AGGREGATE STATISTICS\n",
      "============================================================\n",
      "Sources evaluated: 2\n",
      "Average F1-Macro: 0.2308 ± 0.2629\n",
      "Average Balanced Acc: 0.5000 ± 0.0000\n",
      "Average AUROC: 0.4254 ± 0.5425\n",
      "Best source: Apache_2k (F1: 0.4167)\n",
      "Worst source: Linux_2k (F1: 0.0449)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sources evaluated: {len(all_results)}\")\n",
    "print(f\"Average F1-Macro: {results_df['Final F1'].mean():.4f} ± {results_df['Final F1'].std():.4f}\")\n",
    "print(f\"Average Balanced Acc: {results_df['Balanced Acc'].mean():.4f} ± {results_df['Balanced Acc'].std():.4f}\")\n",
    "print(f\"Average AUROC: {results_df['AUROC'].mean():.4f} ± {results_df['AUROC'].std():.4f}\")\n",
    "print(f\"Best source: {results_df.iloc[0]['Test Source']} (F1: {results_df.iloc[0]['Final F1']:.4f})\")\n",
    "print(f\"Worst source: {results_df.iloc[-1]['Test Source']} (F1: {results_df.iloc[-1]['Final F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71200688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\federated_contrastive\\results_20251122_154439\n",
      "  - loso_results.csv\n",
      "  - complete_results.pkl\n",
      "  - training_history.json\n",
      "  - embeddings saved per split\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = RESULTS_PATH / f\"results_{timestamp}\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_df.to_csv(results_dir / \"loso_results.csv\", index=False)\n",
    "\n",
    "with open(results_dir / \"complete_results.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_results': all_results,\n",
    "        'summary': results_df.to_dict('records'),\n",
    "        'config': {\n",
    "            'num_rounds': NUM_ROUNDS,\n",
    "            'local_epochs': LOCAL_EPOCHS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'lr_encoder': LR_ENCODER,\n",
    "            'lr_head': LR_HEAD,\n",
    "            'lambda_contrastive': LAMBDA_CONTRASTIVE,\n",
    "            'lambda_focal': LAMBDA_FOCAL,\n",
    "            'lambda_template': LAMBDA_TEMPLATE\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }, f)\n",
    "\n",
    "training_history = {r['test_source']: r['history'] for r in all_results}\n",
    "with open(results_dir / \"training_history.json\", 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_dir}\")\n",
    "print(f\"  - loso_results.csv\")\n",
    "print(f\"  - complete_results.pkl\")\n",
    "print(f\"  - training_history.json\")\n",
    "print(f\"  - embeddings saved per split\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
