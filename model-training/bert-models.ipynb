{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23065fa4",
   "metadata": {},
   "source": [
    "IMPORTS AND DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccd5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    BertTokenizer, BertModel, BertConfig,\n",
    "    DebertaV2Tokenizer, DebertaV2Model, DebertaV2Config,\n",
    "    MPNetTokenizer, MPNetModel, MPNetConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, balanced_accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323e9e3",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52cebb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEVICE CONFIGURATION\n",
      "================================================================================\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "Available GPU Memory: 8.59 GB\n",
      "CUDA Device Count: 1\n",
      "✓ GPU test successful\n",
      "================================================================================\n",
      "\n",
      "Models will be saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\bert_models\n",
      "Results will be saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\bert_results\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = False  # False for better performance\n",
    "    torch.backends.cudnn.benchmark = True       # True for better performance\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DEVICE CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Test GPU\n",
    "    try:\n",
    "        test_tensor = torch.randn(100, 100).to(device)\n",
    "        result = test_tensor @ test_tensor\n",
    "        print(f\"✓ GPU test successful\")\n",
    "        del test_tensor, result\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPU test failed: {e}\")\n",
    "        print(f\"⚠️  Falling back to CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(f\"⚠️  CUDA not available. Using CPU (will be very slow!)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "MODELS_PATH = ROOT / \"models\" / \"bert_models\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"bert_results\"\n",
    "CACHE_PATH = RESULTS_PATH / \"split_cache\"\n",
    "\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Models will be saved to: {MODELS_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72050072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 log sources\n",
      "Loaded 16 cross-source splits\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "feat_file = FEAT_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "if not feat_file.exists():\n",
    "    print(f\"Error: {feat_file} not found\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(feat_file, 'rb') as f:\n",
    "    feat_data = pickle.load(f)\n",
    "    dat = feat_data['hybrid_features_data']\n",
    "    num_classes = feat_data['config'].get('num_classes', 2)\n",
    "\n",
    "split_file = FEAT_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "if not split_file.exists():\n",
    "    print(f\"Error: {split_file} not found\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(split_file, 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    splts = split_data['splits']\n",
    "\n",
    "print(f\"Loaded {len(dat)} log sources\")\n",
    "print(f\"Loaded {len(splts)} cross-source splits\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22655b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BERT MODELS CONFIGURATION\n",
      "================================================================================\n",
      "Max sequence length: 256\n",
      "Batch size: 32\n",
      "Learning rate: 2e-05\n",
      "Epochs: 1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "BERT_CONFIG = {\n",
    "    'max_length': 256,              # FIXED: Increased from 32\n",
    "    'batch_size': 32,               # FIXED: Reduced from 256 for stability\n",
    "    'learning_rate': 2e-5,          # FIXED: More conservative\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 1,               # FIXED: Increased from 1\n",
    "    'warmup_ratio': 0.1,            # FIXED: Increased from 0.05\n",
    "    'gradient_clip': 1.0,\n",
    "    'dropout': 0.1,\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 2.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    'early_stopping_patience': 3,\n",
    "    'accumulation_steps': 2,        # FIXED: Increased for effective batch size\n",
    "    'use_amp': True,\n",
    "    'num_workers': 0,               # 0 for Windows compatibility\n",
    "    'pin_memory': False,\n",
    "    'compile_model': False,\n",
    "}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'logbert': {\n",
    "        'model_name': 'bert-base-uncased',\n",
    "        'hidden_size': 768,\n",
    "        'num_attention_heads': 12,\n",
    "        'num_hidden_layers': 12,\n",
    "        'use_mlm_pretraining': True,\n",
    "        'mlm_probability': 0.15,\n",
    "    },\n",
    "    'dapt_bert': {\n",
    "        'model_name': 'bert-base-uncased',\n",
    "        'hidden_size': 768,\n",
    "        'domain_adapt_epochs': 3,\n",
    "        'use_domain_adaptation': True,\n",
    "    },\n",
    "    'deberta_v3': {\n",
    "        'model_name': 'microsoft/deberta-v3-base',\n",
    "        'hidden_size': 768,\n",
    "        'use_disentangled_attention': True,\n",
    "    },\n",
    "    'mpnet': {\n",
    "        'model_name': 'microsoft/mpnet-base',\n",
    "        'hidden_size': 768,\n",
    "        'use_mean_pooling': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BERT MODELS CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Max sequence length: {BERT_CONFIG['max_length']}\")\n",
    "print(f\"Batch size: {BERT_CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {BERT_CONFIG['learning_rate']}\")\n",
    "print(f\"Epochs: {BERT_CONFIG['num_epochs']}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c2439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log(text):\n",
    "    \"\"\"Preprocess log text to normalize patterns\"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Replace common patterns\n",
    "    text = re.sub(r'[0-9a-f]{8,}', '<HEX>', text)  # Hex IDs\n",
    "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '<IP>', text)  # IP addresses\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # Dates\n",
    "    text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', '<TIME>', text)  # Times\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Numbers\n",
    "    text = re.sub(r'[^\\w\\s<>]', ' ', text)  # Remove special chars except <>\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda185f",
   "metadata": {},
   "source": [
    "DATASET CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ba82936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    \"\"\"Dataset for log anomaly detection with text and optional features\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 additional_features=None, augment=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.additional_features = additional_features\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = preprocess_log(self.texts[idx])  # FIXED: Added preprocessing\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # FIXED: Better augmentation logic\n",
    "        if self.augment and label == 1 and np.random.random() < 0.3:\n",
    "            text = self._augment_text(text)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.additional_features is not None:\n",
    "            item['additional_features'] = torch.tensor(\n",
    "                self.additional_features[idx], dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Improved text augmentation\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        aug_type = np.random.choice(['drop', 'swap', 'mask'])\n",
    "        \n",
    "        if aug_type == 'drop' and len(words) > 5:\n",
    "            # Random word dropout\n",
    "            num_drop = max(1, int(len(words) * 0.1))\n",
    "            drop_indices = np.random.choice(len(words), num_drop, replace=False)\n",
    "            words = [w for i, w in enumerate(words) if i not in drop_indices]\n",
    "        \n",
    "        elif aug_type == 'swap' and len(words) > 3:\n",
    "            # Random word swap\n",
    "            idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        \n",
    "        elif aug_type == 'mask':\n",
    "            # Random word masking\n",
    "            mask_idx = np.random.choice(len(words))\n",
    "            words[mask_idx] = '<MASK>'\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "# =============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross entropy with label smoothing\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = F.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        loss = -log_preds.sum(dim=-1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, reduction='mean')\n",
    "        \n",
    "        return self.smoothing * loss / n_classes + (1 - self.smoothing) * nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b4901",
   "metadata": {},
   "source": [
    "BERT MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cdfe0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogBERT(nn.Module):\n",
    "    \"\"\"LogBERT: BERT with log-specific adaptations and MLM pretraining\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, \n",
    "                 dropout=0.1, use_additional_features=False, \n",
    "                 additional_feature_dim=0):\n",
    "        super(LogBERT, self).__init__()\n",
    "        \n",
    "        self.config = BertConfig.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=self.config, use_safetensors=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.use_additional_features = use_additional_features\n",
    "        \n",
    "        if use_additional_features and additional_feature_dim > 0:\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(additional_feature_dim, hidden_size // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size // 4, hidden_size // 4)\n",
    "            )\n",
    "            classifier_input_dim = hidden_size + hidden_size // 4\n",
    "        else:\n",
    "            classifier_input_dim = hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.LayerNorm(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.mlm_head = nn.Linear(hidden_size, self.config.vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, additional_features=None, \n",
    "                return_mlm_logits=False):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if self.use_additional_features and additional_features is not None:\n",
    "            additional_features = self.feature_fusion(additional_features)\n",
    "            pooled_output = torch.cat([pooled_output, additional_features], dim=1)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if return_mlm_logits:\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "            mlm_logits = self.mlm_head(sequence_output)\n",
    "            return logits, mlm_logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class DomainAdaptedBERT(nn.Module):\n",
    "    \"\"\"BERT with Domain-Adaptive Pretraining (DAPT) for log data\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, \n",
    "                 dropout=0.1, use_additional_features=False, \n",
    "                 additional_feature_dim=0):\n",
    "        super(DomainAdaptedBERT, self).__init__()\n",
    "        \n",
    "        self.config = BertConfig.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=self.config, use_safetensors=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.use_additional_features = use_additional_features\n",
    "        \n",
    "        if use_additional_features and additional_feature_dim > 0:\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(additional_feature_dim, hidden_size // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            classifier_input_dim = hidden_size + hidden_size // 4\n",
    "        else:\n",
    "            classifier_input_dim = hidden_size\n",
    "        \n",
    "        self.domain_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 16)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, additional_features=None, \n",
    "                return_domain_logits=False):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        attended_output, _ = self.domain_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        \n",
    "        pooled_output = attended_output[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if self.use_additional_features and additional_features is not None:\n",
    "            additional_features = self.feature_fusion(additional_features)\n",
    "            pooled_output = torch.cat([pooled_output, additional_features], dim=1)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if return_domain_logits:\n",
    "            domain_logits = self.domain_classifier(outputs.pooler_output)\n",
    "            return logits, domain_logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class DeBERTaV3Classifier(nn.Module):\n",
    "    \"\"\"DeBERTa-v3 with disentangled attention for log classification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='microsoft/deberta-v3-base', num_classes=2, \n",
    "                 dropout=0.1, use_additional_features=False, \n",
    "                 additional_feature_dim=0):\n",
    "        super(DeBERTaV3Classifier, self).__init__()\n",
    "        \n",
    "        self.config = DebertaV2Config.from_pretrained(model_name)\n",
    "        self.deberta = DebertaV2Model.from_pretrained(model_name, config=self.config, use_safetensors=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.use_additional_features = use_additional_features\n",
    "        \n",
    "        if use_additional_features and additional_feature_dim > 0:\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(additional_feature_dim, hidden_size // 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            classifier_input_dim = hidden_size + hidden_size // 4\n",
    "        else:\n",
    "            classifier_input_dim = hidden_size\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(classifier_input_dim, hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, additional_features=None):\n",
    "        outputs = self.deberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(sequence_output.size()).float()\n",
    "        sum_embeddings = torch.sum(sequence_output * attention_mask_expanded, 1)\n",
    "        sum_mask = attention_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if self.use_additional_features and additional_features is not None:\n",
    "            additional_features = self.feature_fusion(additional_features)\n",
    "            pooled_output = torch.cat([pooled_output, additional_features], dim=1)\n",
    "        \n",
    "        pre_logits = self.pre_classifier(pooled_output)\n",
    "        logits = self.classifier(pre_logits + pooled_output[:, :self.config.hidden_size])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class MPNetClassifier(nn.Module):\n",
    "    \"\"\"MPNet with mean pooling for log classification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='microsoft/mpnet-base', num_classes=2, \n",
    "                 dropout=0.1, use_additional_features=False, \n",
    "                 additional_feature_dim=0):\n",
    "        super(MPNetClassifier, self).__init__()\n",
    "        \n",
    "        self.config = MPNetConfig.from_pretrained(model_name)\n",
    "        self.mpnet = MPNetModel.from_pretrained(model_name, config=self.config, use_safetensors=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.use_additional_features = use_additional_features\n",
    "        \n",
    "        if use_additional_features and additional_feature_dim > 0:\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(additional_feature_dim, hidden_size // 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            classifier_input_dim = hidden_size + hidden_size // 4\n",
    "        else:\n",
    "            classifier_input_dim = hidden_size\n",
    "        \n",
    "        self.attention_pooling = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.LayerNorm(hidden_size // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, additional_features=None):\n",
    "        outputs = self.mpnet(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        attention_weights = self.attention_pooling(sequence_output)\n",
    "        pooled_output = torch.sum(sequence_output * attention_weights, dim=1)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if self.use_additional_features and additional_features is not None:\n",
    "            additional_features = self.feature_fusion(additional_features)\n",
    "            pooled_output = torch.cat([pooled_output, additional_features], dim=1)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f490c",
   "metadata": {},
   "source": [
    "FUNCTIONS FOR DATA HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6b7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(filepath):\n",
    "    \"\"\"Compute MD5 hash of file for reproducibility\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    \"\"\"Calculate class weights for imbalanced data\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    weights = {int(cls): total / (len(unique) * count) for cls, count in zip(unique, counts)}\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_imbalance_tier(imbalance_ratio):\n",
    "    \"\"\"Categorize imbalance severity\"\"\"\n",
    "    if imbalance_ratio > 100:\n",
    "        return 'Extreme (>100:1)'\n",
    "    elif imbalance_ratio > 10:\n",
    "        return 'High (10-100:1)'\n",
    "    elif imbalance_ratio > 5:\n",
    "        return 'Moderate (5-10:1)'\n",
    "    else:\n",
    "        return 'Balanced (≤5:1)'\n",
    "\n",
    "\n",
    "def calculate_geometric_mean(y_true, y_pred):\n",
    "    \"\"\"Calculate geometric mean of per-class recalls\"\"\"\n",
    "    unique_classes = np.unique(y_true)\n",
    "    recalls = []\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        y_true_binary = (y_true == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    if len(recalls) > 0 and all(r >= 0 for r in recalls):\n",
    "        recalls = [max(r, 1e-10) for r in recalls]\n",
    "        return np.prod(recalls) ** (1/len(recalls))\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_iba(y_true, y_pred, alpha=0.1):\n",
    "    \"\"\"Calculate Index of Balanced Accuracy\"\"\"\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    geometric_mean = calculate_geometric_mean(y_true, y_pred)\n",
    "    iba = (1 + alpha * geometric_mean) * balanced_acc\n",
    "    return iba\n",
    "\n",
    "\n",
    "def calc_enhanced_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['acc'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['bal_acc'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics['geometric_mean'] = calculate_geometric_mean(y_true, y_pred)\n",
    "    metrics['iba'] = calculate_iba(y_true, y_pred)\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            metrics['auprc'] = average_precision_score(y_true, y_proba[:, 1])\n",
    "        except:\n",
    "            metrics['auroc'] = 0.0\n",
    "            metrics['auprc'] = 0.0\n",
    "    else:\n",
    "        metrics['auroc'] = 0.0\n",
    "        metrics['auprc'] = 0.0\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_metrics = {}\n",
    "    unique_classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        y_true_binary = (y_true == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        \n",
    "        if y_true_binary.sum() > 0:\n",
    "            per_class_metrics[int(class_id)] = {\n",
    "                'precision': precision_score(y_true_binary, y_pred_binary, zero_division=0),\n",
    "                'recall': recall_score(y_true_binary, y_pred_binary, zero_division=0),\n",
    "                'f1': f1_score(y_true_binary, y_pred_binary, zero_division=0),\n",
    "                'support': int(y_true_binary.sum())\n",
    "            }\n",
    "    \n",
    "    metrics['per_class'] = per_class_metrics\n",
    "    \n",
    "    # Confusion matrix\n",
    "    labels = sorted(np.unique(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_weighted_sampler(labels, imbalance_ratio):\n",
    "    \"\"\"Create weighted sampler for imbalanced data\"\"\"\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = class_weights[labels]\n",
    "    \n",
    "    if imbalance_ratio > 100:\n",
    "        minority_class = np.argmin(class_counts)\n",
    "        sample_weights[labels == minority_class] *= 2.0\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    return sampler\n",
    "\n",
    "\n",
    "# FIXED: Proper SMOTE implementation\n",
    "def apply_smote_if_needed(texts, labels, imbalance_ratio):\n",
    "    \"\"\"Apply SMOTE to balance classes - FIXED version\"\"\"\n",
    "    if imbalance_ratio < 5:\n",
    "        print(f\"  ℹ️  Imbalance ratio {imbalance_ratio:.2f} < 5. Skipping SMOTE.\")\n",
    "        return texts, labels, np.arange(len(texts))\n",
    "    \n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "    \n",
    "    if min_count <= 1:\n",
    "        print(f\"  ⚠️  Minority class has only {min_count} sample(s). Skipping SMOTE.\")\n",
    "        return texts, labels, np.arange(len(texts))\n",
    "    \n",
    "    # SMOTE needs numeric features - use indices as proxy\n",
    "    indices = np.arange(len(texts)).reshape(-1, 1)\n",
    "    \n",
    "    k_neighbors = min(5, min_count - 1)\n",
    "    k_neighbors = max(1, k_neighbors)\n",
    "    \n",
    "    try:\n",
    "        if imbalance_ratio > 100:\n",
    "            smote = ADASYN(random_state=SEED, n_neighbors=k_neighbors)\n",
    "            print(f\"  Using ADASYN (imbalance: {imbalance_ratio:.2f}:1)\")\n",
    "        elif imbalance_ratio > 10:\n",
    "            smote = BorderlineSMOTE(random_state=SEED, k_neighbors=k_neighbors)\n",
    "            print(f\"  Using BorderlineSMOTE (imbalance: {imbalance_ratio:.2f}:1)\")\n",
    "        else:\n",
    "            smote = SMOTE(random_state=SEED, k_neighbors=k_neighbors)\n",
    "            print(f\"  Using SMOTE (imbalance: {imbalance_ratio:.2f}:1)\")\n",
    "        \n",
    "        indices_resampled, labels_resampled = smote.fit_resample(indices, labels)\n",
    "        indices_resampled = indices_resampled.flatten().astype(int)\n",
    "        \n",
    "        # FIXED: Create array of texts using resampled indices\n",
    "        texts_resampled = np.array([texts[i] for i in indices_resampled])\n",
    "        \n",
    "        print(f\"  ✓ SMOTE applied: {len(labels)} → {len(labels_resampled)} samples\")\n",
    "        print(f\"    Original distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "        print(f\"    New distribution: {dict(zip(*np.unique(labels_resampled, return_counts=True)))}\")\n",
    "        \n",
    "        return texts_resampled, labels_resampled, indices_resampled\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  SMOTE failed: {e}. Using original data.\")\n",
    "        return texts, labels, np.arange(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c95a9",
   "metadata": {},
   "source": [
    "TRAINING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ef6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, \n",
    "                accumulation_steps=1, gradient_clip=1.0, use_amp=True, scaler=None):\n",
    "    \"\"\"Train for one epoch with mixed precision support - FIXED version\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False, \n",
    "                disable=False, dynamic_ncols=True, ascii=True)\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "        \n",
    "        additional_features = None\n",
    "        if 'additional_features' in batch:\n",
    "            additional_features = batch['additional_features'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if use_amp and scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if additional_features is not None:\n",
    "                    logits = model(input_ids, attention_mask, additional_features)\n",
    "                else:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # FIXED: Handle last batch properly\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            if additional_features is not None:\n",
    "                logits = model(input_ids, attention_mask, additional_features)\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # FIXED: Handle last batch properly\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        # FIXED: Collect all predictions\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # FIXED: Ensure predictions and labels are same length\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    if len(predictions) != len(true_labels):\n",
    "        print(f\"  ⚠️  Warning: predictions ({len(predictions)}) != labels ({len(true_labels)})\")\n",
    "        min_len = min(len(predictions), len(true_labels))\n",
    "        predictions = predictions[:min_len]\n",
    "        true_labels = true_labels[:min_len]\n",
    "    \n",
    "    f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return avg_loss, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, use_amp=True):\n",
    "    \"\"\"Evaluate model with mixed precision support\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False, \n",
    "                         disable=False, dynamic_ncols=True, ascii=True):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "            \n",
    "            additional_features = None\n",
    "            if 'additional_features' in batch:\n",
    "                additional_features = batch['additional_features'].to(device, non_blocking=True)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if additional_features is not None:\n",
    "                        logits = model(input_ids, attention_mask, additional_features)\n",
    "                    else:\n",
    "                        logits = model(input_ids, attention_mask)\n",
    "                    loss = criterion(logits, labels)\n",
    "            else:\n",
    "                if additional_features is not None:\n",
    "                    logits = model(input_ids, attention_mask, additional_features)\n",
    "                else:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(probs)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    probabilities = np.array(probabilities)\n",
    "    \n",
    "    metrics = calc_enhanced_metrics(true_labels, predictions, probabilities)\n",
    "    \n",
    "    return avg_loss, metrics, predictions, probabilities\n",
    "\n",
    "\n",
    "def tune_threshold(model, dataloader, device, use_amp=True):\n",
    "    \"\"\"Find optimal classification threshold\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            additional_features = None\n",
    "            if 'additional_features' in batch:\n",
    "                additional_features = batch['additional_features'].to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if additional_features is not None:\n",
    "                        logits = model(input_ids, attention_mask, additional_features)\n",
    "                    else:\n",
    "                        logits = model(input_ids, attention_mask)\n",
    "            else:\n",
    "                if additional_features is not None:\n",
    "                    logits = model(input_ids, attention_mask, additional_features)\n",
    "                else:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            all_probs.extend(probs[:, 1])\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_probs >= threshold).astype(int)\n",
    "        f1 = f1_score(all_labels, preds, average='macro', zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def train_model(model_name, model, tokenizer, train_texts, train_labels, \n",
    "                val_texts, val_labels, test_texts, test_labels,\n",
    "                additional_features_train=None, additional_features_val=None,\n",
    "                additional_features_test=None, imbalance_ratio=1.0,\n",
    "                config=BERT_CONFIG, split_idx=None):\n",
    "    \"\"\"Train a BERT-based model with all enhancements - FIXED version\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    if split_idx is not None:\n",
    "        print(f\"Split: {split_idx}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # FIXED: Apply SMOTE and get indices\n",
    "    train_texts_aug, train_labels_aug, indices_aug = apply_smote_if_needed(\n",
    "        train_texts, train_labels, imbalance_ratio\n",
    "    )\n",
    "    \n",
    "    # FIXED: Adjust additional features using resampled indices\n",
    "    if additional_features_train is not None:\n",
    "        additional_features_train_aug = additional_features_train[indices_aug]\n",
    "    else:\n",
    "        additional_features_train_aug = None\n",
    "    \n",
    "    # FIXED: Augmentation should be used for imbalanced data\n",
    "    augment_flag = (imbalance_ratio > 5)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = LogDataset(\n",
    "        train_texts_aug, train_labels_aug, tokenizer, \n",
    "        config['max_length'], additional_features_train_aug, augment=augment_flag\n",
    "    )\n",
    "    val_dataset = LogDataset(\n",
    "        val_texts, val_labels, tokenizer, \n",
    "        config['max_length'], additional_features_val, augment=False\n",
    "    )\n",
    "    test_dataset = LogDataset(\n",
    "        test_texts, test_labels, tokenizer, \n",
    "        config['max_length'], additional_features_test, augment=False\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with weighted sampling\n",
    "    train_sampler = create_weighted_sampler(train_labels_aug, imbalance_ratio)\n",
    "    \n",
    "    num_workers = config.get('num_workers', 0)\n",
    "    pin_memory = config.get('pin_memory', False)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'] * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * config['num_epochs'] // config['accumulation_steps']\n",
    "    warmup_steps = int(total_steps * config['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    if imbalance_ratio > 10:\n",
    "        criterion = FocalLoss(\n",
    "            alpha=config['focal_alpha'],\n",
    "            gamma=config['focal_gamma']\n",
    "        )\n",
    "        print(f\"Using Focal Loss (imbalance: {imbalance_ratio:.2f}:1)\")\n",
    "    else:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=config['label_smoothing'])\n",
    "        print(f\"Using Label Smoothing CE (imbalance: {imbalance_ratio:.2f}:1)\")\n",
    "    \n",
    "    # Initialize mixed precision scaler\n",
    "    use_amp = config.get('use_amp', True) and torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    if use_amp:\n",
    "        print(\"✓ Using Automatic Mixed Precision (AMP)\")\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining for up to {config['num_epochs']} epochs...\")\n",
    "    print(f\"Train samples: {len(train_labels_aug):,}\")\n",
    "    print(f\"Val samples: {len(val_labels):,}\")\n",
    "    print(f\"Test samples: {len(test_labels):,}\")\n",
    "    print(f\"Batch size: {config['batch_size']} (effective: {config['batch_size'] * config['accumulation_steps']})\")\n",
    "    print(f\"Max length: {config['max_length']} tokens\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, device,\n",
    "            config['accumulation_steps'], config['gradient_clip'], use_amp, scaler\n",
    "        )\n",
    "        \n",
    "        val_loss, val_metrics, _, _ = evaluate(model, val_loader, criterion, device, use_amp)\n",
    "        val_f1 = val_metrics['f1_macro']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping with model checkpointing\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"✓ New best model (F1: {best_val_f1:.4f})\")\n",
    "            \n",
    "            # FIXED: Save checkpoint during training\n",
    "            if split_idx is not None:\n",
    "                checkpoint_path = MODELS_PATH / f\"{model_name}_split_{split_idx}_best.pt\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': best_model_state,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_f1': best_val_f1,\n",
    "                    'val_metrics': val_metrics\n",
    "                }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience: {patience_counter}/{config['early_stopping_patience']}\")\n",
    "            if patience_counter >= config['early_stopping_patience']:\n",
    "                print(f\"\\n⚡ Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\n✓ Loaded best model (Val F1: {best_val_f1:.4f})\")\n",
    "    \n",
    "    # Threshold tuning\n",
    "    print(\"\\n--- Threshold Tuning ---\")\n",
    "    optimal_threshold, tuned_f1 = tune_threshold(model, val_loader, device, use_amp)\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.3f} (Val F1: {tuned_f1:.4f})\")\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    print(\"\\n--- Test Set Evaluation ---\")\n",
    "    test_loss, test_metrics, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device, use_amp\n",
    "    )\n",
    "    \n",
    "    # Apply tuned threshold\n",
    "    test_preds_tuned = (test_probs[:, 1] >= optimal_threshold).astype(int)\n",
    "    test_metrics_tuned = calc_enhanced_metrics(test_labels, test_preds_tuned, test_probs)\n",
    "    \n",
    "    print(f\"\\nTest Results (default threshold 0.5):\")\n",
    "    print(f\"  F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {test_metrics['bal_acc']:.4f}\")\n",
    "    print(f\"  AUROC: {test_metrics['auroc']:.4f}\")\n",
    "    print(f\"  MCC: {test_metrics['mcc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Results (tuned threshold {optimal_threshold:.3f}):\")\n",
    "    print(f\"  F1-Macro: {test_metrics_tuned['f1_macro']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {test_metrics_tuned['bal_acc']:.4f}\")\n",
    "    print(f\"  AUROC: {test_metrics_tuned['auroc']:.4f}\")\n",
    "    print(f\"  MCC: {test_metrics_tuned['mcc']:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n--- Per-Class Performance ---\")\n",
    "    for class_id, metrics in test_metrics_tuned['per_class'].items():\n",
    "        class_name = LABEL_MAP.get(class_id, f'Class_{class_id}')\n",
    "        print(f\"{class_name} (ID {class_id}):\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    # FIXED: Synchronize GPU before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_metrics': test_metrics,\n",
    "        'test_metrics_tuned': test_metrics_tuned,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'test_predictions': test_preds_tuned,\n",
    "        'test_probabilities': test_probs\n",
    "    }\n",
    "\n",
    "\n",
    "def train_single_model_for_split(model_key, model_config, train_texts_split, train_labels_split,\n",
    "                                 val_texts, val_labels, test_texts, test_labels, imb_ratio, split_idx):\n",
    "    \"\"\"Train a single model for a split with proper memory management - FIXED\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_key.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"GPU Memory before loading: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        print(f\"Loading tokenizer: {model_config['model_name']}...\")\n",
    "        if model_key == 'deberta_v3':\n",
    "            tokenizer = DebertaV2Tokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        elif model_key == 'mpnet':\n",
    "            tokenizer = MPNetTokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        else:\n",
    "            tokenizer = BertTokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        print(f\"✓ Tokenizer loaded\")\n",
    "        \n",
    "        # Initialize model\n",
    "        print(f\"Loading model: {model_config['model_name']}...\")\n",
    "        if model_key == 'logbert':\n",
    "            model = LogBERT(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            )\n",
    "        elif model_key == 'dapt_bert':\n",
    "            model = DomainAdaptedBERT(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            )\n",
    "        elif model_key == 'deberta_v3':\n",
    "            model = DeBERTaV3Classifier(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            )\n",
    "        elif model_key == 'mpnet':\n",
    "            model = MPNetClassifier(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            )\n",
    "        \n",
    "        print(f\"✓ Model loaded\")\n",
    "        print(f\"Moving model to {device}...\")\n",
    "        model = model.to(device)\n",
    "        print(f\"✓ Model on {device}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Train model\n",
    "        result = train_model(\n",
    "            model_key, model, tokenizer,\n",
    "            train_texts_split, train_labels_split,\n",
    "            val_texts, val_labels,\n",
    "            test_texts, test_labels,\n",
    "            imbalance_ratio=imb_ratio,\n",
    "            config=BERT_CONFIG,\n",
    "            split_idx=split_idx\n",
    "        )\n",
    "        \n",
    "        # Extract summary\n",
    "        result_summary = {\n",
    "            'test_metrics': result['test_metrics'],\n",
    "            'test_metrics_tuned': result['test_metrics_tuned'],\n",
    "            'optimal_threshold': result['optimal_threshold'],\n",
    "            'history': result['history']\n",
    "        }\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        del tokenizer\n",
    "        del result\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✓ {model_key.upper()} training complete and memory cleared\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        \n",
    "        return result_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training {model_key}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Cleanup on error\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'tokenizer' in locals():\n",
    "            del tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "def process_single_split(split_idx, split, model_configs_to_test):\n",
    "    \"\"\"Process a single cross-source split for all BERT models\"\"\"\n",
    "    \n",
    "    test_src = split['test_source']\n",
    "    train_srcs = split['train_sources']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SPLIT {split_idx+1}/{len(splts)}: Testing on {test_src}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train sources: {', '.join(train_srcs)}\\n\")\n",
    "    \n",
    "    # Validate test source\n",
    "    if test_src not in dat or dat[test_src]['labels'] is None:\n",
    "        print(f\"⚠️  Skipping {test_src}: No labels available\")\n",
    "        return None\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = dat[test_src]\n",
    "    test_texts = test_data['texts']\n",
    "    test_labels = test_data['labels']\n",
    "    \n",
    "    # Check for single-class test set\n",
    "    if len(np.unique(test_labels)) < 2:\n",
    "        print(f\"⚠️  Single-class test set detected for {test_src}. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # Load training data\n",
    "    train_texts_list, train_labels_list = [], []\n",
    "    for src in train_srcs:\n",
    "        if src in dat and dat[src]['labels'] is not None:\n",
    "            train_texts_list.extend(dat[src]['texts'])\n",
    "            train_labels_list.extend(dat[src]['labels'])\n",
    "    \n",
    "    if not train_texts_list:\n",
    "        print(f\"⚠️  Skipping {test_src}: No training data available\")\n",
    "        return None\n",
    "    \n",
    "    train_texts = np.array(train_texts_list)\n",
    "    train_labels = np.array(train_labels_list)\n",
    "    \n",
    "    # Validate classes\n",
    "    train_classes = np.unique(train_labels)\n",
    "    test_classes = np.unique(test_labels)\n",
    "    \n",
    "    if len(train_classes) < 2:\n",
    "        print(f\"⚠️  Training data has only {len(train_classes)} class(es). Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Train classes: {sorted(train_classes)}\")\n",
    "    print(f\"Test classes: {sorted(test_classes)}\")\n",
    "    \n",
    "    # Calculate imbalance\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    imb_ratio = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "    imb_tier = get_imbalance_tier(imb_ratio)\n",
    "    \n",
    "    print(f\"\\nTrain samples: {len(train_labels):,}\")\n",
    "    print(f\"Test samples: {len(test_labels):,}\")\n",
    "    print(f\"Train imbalance ratio: {imb_ratio:.2f}:1 ({imb_tier})\")\n",
    "    \n",
    "    # Split training into train/val (80/20)\n",
    "    train_texts_split, val_texts, train_labels_split, val_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.2, random_state=SEED, \n",
    "        stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Train split: {len(train_labels_split):,}\")\n",
    "    print(f\"Val split: {len(val_labels):,}\")\n",
    "    \n",
    "    # Train models ONE AT A TIME\n",
    "    results = {}\n",
    "    \n",
    "    for model_key, model_config in model_configs_to_test.items():\n",
    "        result = train_single_model_for_split(\n",
    "            model_key, model_config,\n",
    "            train_texts_split, train_labels_split,\n",
    "            val_texts, val_labels,\n",
    "            test_texts, test_labels,\n",
    "            imb_ratio, split_idx\n",
    "        )\n",
    "        results[model_key] = result\n",
    "        \n",
    "        # Extra cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compare models\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL COMPARISON FOR {test_src}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    for model_key, result in results.items():\n",
    "        if 'error' not in result:\n",
    "            metrics = result['test_metrics_tuned']\n",
    "            comparison_data.append({\n",
    "                'Model': model_key.upper(),\n",
    "                'F1-Macro': metrics['f1_macro'],\n",
    "                'Balanced Acc': metrics['bal_acc'],\n",
    "                'AUROC': metrics['auroc'],\n",
    "                'AUPRC': metrics['auprc'],\n",
    "                'MCC': metrics['mcc'],\n",
    "                'Threshold': result['optimal_threshold']\n",
    "            })\n",
    "    \n",
    "    if comparison_data:\n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        df_comparison = df_comparison.sort_values('F1-Macro', ascending=False)\n",
    "        print(df_comparison.to_string(index=False))\n",
    "        \n",
    "        best_model = df_comparison.iloc[0]['Model'].lower()\n",
    "        print(f\"\\n✓ Best Model: {best_model.upper()}\")\n",
    "        print(f\"  F1-Macro: {df_comparison.iloc[0]['F1-Macro']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'split_idx': split_idx,\n",
    "        'test_source': test_src,    \n",
    "        'train_sources': train_srcs,\n",
    "        'results': results,\n",
    "        'comparison': comparison_data if comparison_data else None,\n",
    "        'imbalance_ratio': float(imb_ratio),\n",
    "        'train_samples': int(len(train_labels)),\n",
    "        'test_samples': int(len(test_labels))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77644f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING BERT MODELS PIPELINE - CROSS-SOURCE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Total splits to process: 16\n",
      "Models to train: 4\n",
      "Device: cuda\n",
      "\n",
      "================================================================================\n",
      "SPLIT 1/16: Testing on Android_2k\n",
      "================================================================================\n",
      "Train sources: Apache_2k, BGL_2k, Hadoop_2k, HDFS_2k, HealthApp_2k, HPC_2k, Linux_2k, Mac_2k, OpenSSH_2k, OpenStack_2k, Proxifier_2k, Spark_2k, Thunderbird_2k, Windows_2k, Zookeeper_2k\n",
      "\n",
      "Train classes: [np.int32(0), np.int32(1)]\n",
      "Test classes: [np.int32(0), np.int32(1)]\n",
      "\n",
      "Train samples: 30,000\n",
      "Test samples: 2,000\n",
      "Train imbalance ratio: 2.37:1 (Balanced (≤5:1))\n",
      "Train split: 24,000\n",
      "Val split: 6,000\n",
      "\n",
      "============================================================\n",
      "Training LOGBERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 133,324,412\n",
      "GPU Memory allocated: 2.32 GB\n",
      "\n",
      "================================================================================\n",
      "Training LOGBERT\n",
      "Split: 0\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.37 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.37:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2880 | Train F1: 0.9389\n",
      "Val Loss: 0.2377 | Val F1: 0.9813\n",
      "✓ New best model (F1: 0.9813)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9813)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.660 (Val F1: 0.9828)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.4545\n",
      "  Balanced Acc: 0.5295\n",
      "  AUROC: 0.7731\n",
      "  MCC: 0.0164\n",
      "\n",
      "Test Results (tuned threshold 0.660):\n",
      "  F1-Macro: 0.4763\n",
      "  Balanced Acc: 0.5571\n",
      "  AUROC: 0.7731\n",
      "  MCC: 0.0356\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9887\n",
      "  Recall: 0.8450\n",
      "  F1: 0.9112\n",
      "  Support: 1974\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.0224\n",
      "  Recall: 0.2692\n",
      "  F1: 0.0413\n",
      "  Support: 26\n",
      "✓ LOGBERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DAPT_BERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 112,442,898\n",
      "GPU Memory allocated: 2.24 GB\n",
      "\n",
      "================================================================================\n",
      "Training DAPT_BERT\n",
      "Split: 0\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.37 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.37:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2657 | Train F1: 0.9535\n",
      "Val Loss: 0.2218 | Val F1: 0.9848\n",
      "✓ New best model (F1: 0.9848)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9848)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.270 (Val F1: 0.9860)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.4681\n",
      "  Balanced Acc: 0.5470\n",
      "  AUROC: 0.6771\n",
      "  MCC: 0.0279\n",
      "\n",
      "Test Results (tuned threshold 0.270):\n",
      "  F1-Macro: 0.4446\n",
      "  Balanced Acc: 0.5998\n",
      "  AUROC: 0.6771\n",
      "  MCC: 0.0513\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9905\n",
      "  Recall: 0.7381\n",
      "  F1: 0.8459\n",
      "  Support: 1974\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.0227\n",
      "  Recall: 0.4615\n",
      "  F1: 0.0432\n",
      "  Support: 26\n",
      "✓ DAPT_BERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DEBERTA_V3\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: microsoft/deberta-v3-base...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: microsoft/deberta-v3-base...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 184,720,514\n",
      "GPU Memory allocated: 2.53 GB\n",
      "\n",
      "================================================================================\n",
      "Training DEBERTA_V3\n",
      "Split: 0\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.37 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.37:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2880 | Train F1: 0.9397\n",
      "Val Loss: 0.2285 | Val F1: 0.9840\n",
      "✓ New best model (F1: 0.9840)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9840)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.830 (Val F1: 0.9853)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.5213\n",
      "  Balanced Acc: 0.5701\n",
      "  AUROC: 0.7436\n",
      "  MCC: 0.0702\n",
      "\n",
      "Test Results (tuned threshold 0.830):\n",
      "  F1-Macro: 0.5408\n",
      "  Balanced Acc: 0.5643\n",
      "  AUROC: 0.7436\n",
      "  MCC: 0.0898\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9887\n",
      "  Recall: 0.9747\n",
      "  F1: 0.9816\n",
      "  Support: 1974\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.0741\n",
      "  Recall: 0.1538\n",
      "  F1: 0.1000\n",
      "  Support: 26\n",
      "✓ DEBERTA_V3 training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training MPNET\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: microsoft/mpnet-base...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: microsoft/mpnet-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 109,857,987\n",
      "GPU Memory allocated: 2.23 GB\n",
      "\n",
      "================================================================================\n",
      "Training MPNET\n",
      "Split: 0\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.37 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.37:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3098 | Train F1: 0.9242\n",
      "Val Loss: 0.2376 | Val F1: 0.9742\n",
      "✓ New best model (F1: 0.9742)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9742)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.830 (Val F1: 0.9806)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.4877\n",
      "  Balanced Acc: 0.6335\n",
      "  AUROC: 0.6819\n",
      "  MCC: 0.0826\n",
      "\n",
      "Test Results (tuned threshold 0.830):\n",
      "  F1-Macro: 0.6872\n",
      "  Balanced Acc: 0.6518\n",
      "  AUROC: 0.6819\n",
      "  MCC: 0.3861\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9909\n",
      "  Recall: 0.9959\n",
      "  F1: 0.9934\n",
      "  Support: 1974\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.5000\n",
      "  Recall: 0.3077\n",
      "  F1: 0.3810\n",
      "  Support: 26\n",
      "✓ MPNET training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON FOR Android_2k\n",
      "================================================================================\n",
      "\n",
      "     Model  F1-Macro  Balanced Acc    AUROC    AUPRC      MCC  Threshold\n",
      "     MPNET  0.687192      0.651820 0.681864 0.192394 0.386089       0.83\n",
      "DEBERTA_V3  0.540816      0.564258 0.743551 0.040168 0.089816       0.83\n",
      "   LOGBERT  0.476262      0.557108 0.773108 0.048295 0.035609       0.66\n",
      " DAPT_BERT  0.444553      0.599817 0.677110 0.022604 0.051270       0.27\n",
      "\n",
      "✓ Best Model: MPNET\n",
      "  F1-Macro: 0.6872\n",
      "\n",
      "================================================================================\n",
      "SPLIT 2/16: Testing on Apache_2k\n",
      "================================================================================\n",
      "Train sources: Android_2k, BGL_2k, Hadoop_2k, HDFS_2k, HealthApp_2k, HPC_2k, Linux_2k, Mac_2k, OpenSSH_2k, OpenStack_2k, Proxifier_2k, Spark_2k, Thunderbird_2k, Windows_2k, Zookeeper_2k\n",
      "\n",
      "Train classes: [np.int32(0), np.int32(1)]\n",
      "Test classes: [np.int32(0), np.int32(1)]\n",
      "\n",
      "Train samples: 30,000\n",
      "Test samples: 2,000\n",
      "Train imbalance ratio: 2.59:1 (Balanced (≤5:1))\n",
      "Train split: 24,000\n",
      "Val split: 6,000\n",
      "\n",
      "============================================================\n",
      "Training LOGBERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 133,324,412\n",
      "GPU Memory allocated: 2.32 GB\n",
      "\n",
      "================================================================================\n",
      "Training LOGBERT\n",
      "Split: 1\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.59 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.59:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2850 | Train F1: 0.9442\n",
      "Val Loss: 0.2259 | Val F1: 0.9851\n",
      "✓ New best model (F1: 0.9851)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9851)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.670 (Val F1: 0.9865)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.4706\n",
      "  Balanced Acc: 0.5238\n",
      "  AUROC: 0.9916\n",
      "  MCC: 0.1467\n",
      "\n",
      "Test Results (tuned threshold 0.670):\n",
      "  F1-Macro: 0.4706\n",
      "  Balanced Acc: 0.5238\n",
      "  AUROC: 0.9916\n",
      "  MCC: 0.1467\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.7244\n",
      "  Recall: 0.9916\n",
      "  F1: 0.8372\n",
      "  Support: 1429\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.7273\n",
      "  Recall: 0.0560\n",
      "  F1: 0.1041\n",
      "  Support: 571\n",
      "✓ LOGBERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DAPT_BERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 112,442,898\n",
      "GPU Memory allocated: 2.24 GB\n",
      "\n",
      "================================================================================\n",
      "Training DAPT_BERT\n",
      "Split: 1\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.59 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.59:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2689 | Train F1: 0.9540\n",
      "Val Loss: 0.2185 | Val F1: 0.9867\n",
      "✓ New best model (F1: 0.9867)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9867)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.810 (Val F1: 0.9892)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.9927\n",
      "  Balanced Acc: 0.9958\n",
      "  AUROC: 0.9916\n",
      "  MCC: 0.9855\n",
      "\n",
      "Test Results (tuned threshold 0.810):\n",
      "  F1-Macro: 0.9727\n",
      "  Balanced Acc: 0.9678\n",
      "  AUROC: 0.9916\n",
      "  MCC: 0.9458\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9779\n",
      "  Recall: 0.9916\n",
      "  F1: 0.9847\n",
      "  Support: 1429\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.9782\n",
      "  Recall: 0.9440\n",
      "  F1: 0.9608\n",
      "  Support: 571\n",
      "✓ DAPT_BERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DEBERTA_V3\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: microsoft/deberta-v3-base...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: microsoft/deberta-v3-base...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 184,720,514\n",
      "GPU Memory allocated: 2.53 GB\n",
      "\n",
      "================================================================================\n",
      "Training DEBERTA_V3\n",
      "Split: 1\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.59 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.59:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2869 | Train F1: 0.9409\n",
      "Val Loss: 0.2322 | Val F1: 0.9817\n",
      "✓ New best model (F1: 0.9817)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9817)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.850 (Val F1: 0.9857)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.7094\n",
      "  Balanced Acc: 0.8009\n",
      "  AUROC: 0.6241\n",
      "  MCC: 0.5490\n",
      "\n",
      "Test Results (tuned threshold 0.850):\n",
      "  F1-Macro: 0.4167\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.6241\n",
      "  MCC: 0.0000\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.7145\n",
      "  Recall: 1.0000\n",
      "  F1: 0.8335\n",
      "  Support: 1429\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1: 0.0000\n",
      "  Support: 571\n",
      "✓ DEBERTA_V3 training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training MPNET\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: microsoft/mpnet-base...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: microsoft/mpnet-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 109,857,987\n",
      "GPU Memory allocated: 2.23 GB\n",
      "\n",
      "================================================================================\n",
      "Training MPNET\n",
      "Split: 1\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.59 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.59:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3161 | Train F1: 0.9219\n",
      "Val Loss: 0.2379 | Val F1: 0.9703\n",
      "✓ New best model (F1: 0.9703)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9703)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.740 (Val F1: 0.9774)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.4737\n",
      "  Balanced Acc: 0.5280\n",
      "  AUROC: 0.6083\n",
      "  MCC: 0.2017\n",
      "\n",
      "Test Results (tuned threshold 0.740):\n",
      "  F1-Macro: 0.4167\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.6083\n",
      "  MCC: 0.0000\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.7145\n",
      "  Recall: 1.0000\n",
      "  F1: 0.8335\n",
      "  Support: 1429\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1: 0.0000\n",
      "  Support: 571\n",
      "✓ MPNET training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON FOR Apache_2k\n",
      "================================================================================\n",
      "\n",
      "     Model  F1-Macro  Balanced Acc    AUROC    AUPRC      MCC  Threshold\n",
      " DAPT_BERT  0.972748      0.967780 0.991603 0.978284 0.945793       0.81\n",
      "   LOGBERT  0.470644      0.523822 0.991603 0.962997 0.146702       0.67\n",
      "DEBERTA_V3  0.416740      0.500000 0.624134 0.528849 0.000000       0.85\n",
      "     MPNET  0.416740      0.500000 0.608281 0.517454 0.000000       0.74\n",
      "\n",
      "✓ Best Model: DAPT_BERT\n",
      "  F1-Macro: 0.9727\n",
      "\n",
      "================================================================================\n",
      "SPLIT 3/16: Testing on BGL_2k\n",
      "================================================================================\n",
      "Train sources: Android_2k, Apache_2k, Hadoop_2k, HDFS_2k, HealthApp_2k, HPC_2k, Linux_2k, Mac_2k, OpenSSH_2k, OpenStack_2k, Proxifier_2k, Spark_2k, Thunderbird_2k, Windows_2k, Zookeeper_2k\n",
      "\n",
      "Train classes: [np.int32(0), np.int32(1)]\n",
      "Test classes: [np.int32(0), np.int32(1)]\n",
      "\n",
      "Train samples: 30,000\n",
      "Test samples: 2,000\n",
      "Train imbalance ratio: 2.43:1 (Balanced (≤5:1))\n",
      "Train split: 24,000\n",
      "Val split: 6,000\n",
      "\n",
      "============================================================\n",
      "Training LOGBERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 133,324,412\n",
      "GPU Memory allocated: 2.32 GB\n",
      "\n",
      "================================================================================\n",
      "Training LOGBERT\n",
      "Split: 2\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.43 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.43:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2819 | Train F1: 0.9465\n",
      "Val Loss: 0.2207 | Val F1: 0.9893\n",
      "✓ New best model (F1: 0.9893)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9893)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.800 (Val F1: 0.9905)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.6866\n",
      "  Balanced Acc: 0.6563\n",
      "  AUROC: 0.8634\n",
      "  MCC: 0.3870\n",
      "\n",
      "Test Results (tuned threshold 0.800):\n",
      "  F1-Macro: 0.6198\n",
      "  Balanced Acc: 0.5849\n",
      "  AUROC: 0.8634\n",
      "  MCC: 0.3143\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9294\n",
      "  Recall: 0.9913\n",
      "  F1: 0.9593\n",
      "  Support: 1832\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.6522\n",
      "  Recall: 0.1786\n",
      "  F1: 0.2804\n",
      "  Support: 168\n",
      "✓ LOGBERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DAPT_BERT\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: bert-base-uncased...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: bert-base-uncased...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 112,442,898\n",
      "GPU Memory allocated: 2.24 GB\n",
      "\n",
      "================================================================================\n",
      "Training DAPT_BERT\n",
      "Split: 2\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.43 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.43:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2623 | Train F1: 0.9575\n",
      "Val Loss: 0.2168 | Val F1: 0.9887\n",
      "✓ New best model (F1: 0.9887)\n",
      "\n",
      "✓ Loaded best model (Val F1: 0.9887)\n",
      "\n",
      "--- Threshold Tuning ---\n",
      "Optimal threshold: 0.830 (Val F1: 0.9901)\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results (default threshold 0.5):\n",
      "  F1-Macro: 0.7643\n",
      "  Balanced Acc: 0.7844\n",
      "  AUROC: 0.9010\n",
      "  MCC: 0.5307\n",
      "\n",
      "Test Results (tuned threshold 0.830):\n",
      "  F1-Macro: 0.7220\n",
      "  Balanced Acc: 0.6834\n",
      "  AUROC: 0.9010\n",
      "  MCC: 0.4624\n",
      "\n",
      "--- Per-Class Performance ---\n",
      "normal (ID 0):\n",
      "  Precision: 0.9457\n",
      "  Recall: 0.9798\n",
      "  F1: 0.9625\n",
      "  Support: 1832\n",
      "anomaly (ID 1):\n",
      "  Precision: 0.6373\n",
      "  Recall: 0.3869\n",
      "  F1: 0.4815\n",
      "  Support: 168\n",
      "✓ DAPT_BERT training complete and memory cleared\n",
      "GPU Memory after cleanup: 1.78 GB\n",
      "\n",
      "============================================================\n",
      "Training DEBERTA_V3\n",
      "============================================================\n",
      "GPU Memory before loading: 1.78 GB\n",
      "Loading tokenizer: microsoft/deberta-v3-base...\n",
      "✓ Tokenizer loaded\n",
      "Loading model: microsoft/deberta-v3-base...\n",
      "✓ Model loaded\n",
      "Moving model to cuda...\n",
      "✓ Model on cuda\n",
      "Parameters: 184,720,514\n",
      "GPU Memory allocated: 2.53 GB\n",
      "\n",
      "================================================================================\n",
      "Training DEBERTA_V3\n",
      "Split: 2\n",
      "================================================================================\n",
      "  ℹ️  Imbalance ratio 2.43 < 5. Skipping SMOTE.\n",
      "Using Label Smoothing CE (imbalance: 2.43:1)\n",
      "✓ Using Automatic Mixed Precision (AMP)\n",
      "\n",
      "Training for up to 1 epochs...\n",
      "Train samples: 24,000\n",
      "Val samples: 6,000\n",
      "Test samples: 2,000\n",
      "Batch size: 32 (effective: 64)\n",
      "Max length: 256 tokens\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                           \n",
      "  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_11020\\2848599704.py\", line 470, in train_single_model_for_split\n",
      "    result = train_model(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_11020\\2848599704.py\", line 311, in train_model\n",
      "    train_loss, train_f1 = train_epoch(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_11020\\2848599704.py\", line 39, in train_epoch\n",
      "    scaler.step(optimizer)\n",
      "  File \"c:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 457, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 351, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 351, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "               ^^^^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error training deberta_v3: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 470\u001b[39m, in \u001b[36mtrain_single_model_for_split\u001b[39m\u001b[34m(model_key, model_config, train_texts_split, train_labels_split, val_texts, val_labels, test_texts, test_labels, imb_ratio, split_idx)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m result = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_texts_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimbalance_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimb_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBERT_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_idx\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Extract summary\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, model, tokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, additional_features_train, additional_features_val, additional_features_test, imbalance_ratio, config, split_idx)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m train_loss, train_f1 = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccumulation_steps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgradient_clip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m val_loss, val_metrics, _, _ = evaluate(model, val_loader, criterion, device, use_amp)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, criterion, device, accumulation_steps, gradient_clip, use_amp, scaler)\u001b[39m\n\u001b[32m     38\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m     retval = optimizer.step(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    350\u001b[39m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m     retval = optimizer.step(*args, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     splts_to_process = splts\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_idx, split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splts_to_process):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     result = \u001b[43mprocess_single_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_CONFIGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     22\u001b[39m         all_split_results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 595\u001b[39m, in \u001b[36mprocess_single_split\u001b[39m\u001b[34m(split_idx, split, model_configs_to_test)\u001b[39m\n\u001b[32m    592\u001b[39m results = {}\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_key, model_config \u001b[38;5;129;01min\u001b[39;00m model_configs_to_test.items():\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     result = \u001b[43mtrain_single_model_for_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_texts_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimb_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     results[model_key] = result\n\u001b[32m    604\u001b[39m     \u001b[38;5;66;03m# Extra cleanup\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 515\u001b[39m, in \u001b[36mtrain_single_model_for_split\u001b[39m\u001b[34m(model_key, model_config, train_texts_split, train_labels_split, val_texts, val_labels, test_texts, test_labels, imb_ratio, split_idx)\u001b[39m\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m    517\u001b[39m gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\AIMLDL\\log-anomaly-detection\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:954\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    952\u001b[39m _lazy_init()\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING BERT MODELS PIPELINE - CROSS-SOURCE EVALUATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"Total splits to process: {len(splts)}\")\n",
    "print(f\"Models to train: {len(MODEL_CONFIGS)}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "all_split_results = []\n",
    "\n",
    "# FIXED: Add option to limit splits for testing\n",
    "TESTING_MODE = False  # Set to True for quick test\n",
    "if TESTING_MODE:\n",
    "    splts_to_process = splts[:2]\n",
    "    print(f\"\\n⚠️  TESTING MODE: Processing only {len(splts_to_process)} splits\\n\")\n",
    "else:\n",
    "    splts_to_process = splts\n",
    "\n",
    "for split_idx, split in enumerate(splts_to_process):\n",
    "    result = process_single_split(split_idx, split, MODEL_CONFIGS)\n",
    "    if result is not None:\n",
    "        all_split_results.append(result)\n",
    "    \n",
    "    # Save intermediate results\n",
    "    if (split_idx + 1) % 3 == 0 or (split_idx + 1) == len(splts_to_process):\n",
    "        intermediate_file = RESULTS_PATH / f\"intermediate_results_split_{split_idx+1}.pkl\"\n",
    "        with open(intermediate_file, 'wb') as f:\n",
    "            pickle.dump(all_split_results, f)\n",
    "        print(f\"\\n✓ Intermediate results saved: {intermediate_file}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE RESULTS - ALL SPLITS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if not all_split_results:\n",
    "    print(\"⚠️  No splits processed successfully!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create summary for each model\n",
    "model_summaries = {key: [] for key in MODEL_CONFIGS.keys()}\n",
    "\n",
    "for split_result in all_split_results:\n",
    "    for model_key, result in split_result['results'].items():\n",
    "        if 'error' not in result:\n",
    "            metrics = result['test_metrics_tuned']\n",
    "            model_summaries[model_key].append({\n",
    "                'test_source': split_result['test_source'],\n",
    "                'f1_macro': metrics['f1_macro'],\n",
    "                'bal_acc': metrics['bal_acc'],\n",
    "                'auroc': metrics['auroc'],\n",
    "                'mcc': metrics['mcc'],\n",
    "                'threshold': result['optimal_threshold']\n",
    "            })\n",
    "\n",
    "# Print summary for each model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "overall_summary = []\n",
    "\n",
    "for model_key, results_list in model_summaries.items():\n",
    "    if results_list:\n",
    "        f1_scores = [r['f1_macro'] for r in results_list]\n",
    "        bal_acc_scores = [r['bal_acc'] for r in results_list]\n",
    "        auroc_scores = [r['auroc'] for r in results_list]\n",
    "        mcc_scores = [r['mcc'] for r in results_list]\n",
    "        \n",
    "        print(f\"\\n{model_key.upper()}:\")\n",
    "        print(f\"  Evaluated on: {len(results_list)} sources\")\n",
    "        print(f\"  F1-Macro: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "        print(f\"  Balanced Acc: {np.mean(bal_acc_scores):.4f} ± {np.std(bal_acc_scores):.4f}\")\n",
    "        print(f\"  AUROC: {np.mean(auroc_scores):.4f} ± {np.std(auroc_scores):.4f}\")\n",
    "        print(f\"  MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "        \n",
    "        overall_summary.append({\n",
    "            'Model': model_key.upper(),\n",
    "            'Avg F1-Macro': np.mean(f1_scores),\n",
    "            'Std F1-Macro': np.std(f1_scores),\n",
    "            'Avg Balanced Acc': np.mean(bal_acc_scores),\n",
    "            'Avg AUROC': np.mean(auroc_scores),\n",
    "            'Avg MCC': np.mean(mcc_scores),\n",
    "            'Sources': len(results_list)\n",
    "        })\n",
    "\n",
    "# Create overall comparison dataframe\n",
    "df_overall = pd.DataFrame(overall_summary)\n",
    "df_overall = df_overall.sort_values('Avg F1-Macro', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL MODEL RANKING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_overall.to_string(index=False))\n",
    "\n",
    "best_overall_model = df_overall.iloc[0]['Model']\n",
    "print(f\"\\n🏆 Best Overall Model: {best_overall_model}\")\n",
    "print(f\"   Average F1-Macro: {df_overall.iloc[0]['Avg F1-Macro']:.4f}\")\n",
    "print(f\"   Average Balanced Acc: {df_overall.iloc[0]['Avg Balanced Acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc992855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = RESULTS_PATH / f\"bert_results_{timestamp}\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save overall summary\n",
    "df_overall.to_csv(results_dir / \"overall_model_ranking.csv\", index=False)\n",
    "print(f\"✓ Saved: overall_model_ranking.csv\")\n",
    "\n",
    "# Save per-source results for each model\n",
    "for model_key, results_list in model_summaries.items():\n",
    "    if results_list:\n",
    "        df_model = pd.DataFrame(results_list)\n",
    "        df_model.to_csv(results_dir / f\"{model_key}_per_source_results.csv\", index=False)\n",
    "        print(f\"✓ Saved: {model_key}_per_source_results.csv\")\n",
    "\n",
    "# Save complete results as pickle\n",
    "results_file = results_dir / \"complete_results.pkl\"\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_split_results': all_split_results,\n",
    "        'model_summaries': model_summaries,\n",
    "        'overall_summary': overall_summary,\n",
    "        'config': {\n",
    "            'bert_config': BERT_CONFIG,\n",
    "            'model_configs': MODEL_CONFIGS,\n",
    "            'num_classes': num_classes,\n",
    "            'label_map': LABEL_MAP\n",
    "        },\n",
    "        'timestamp': timestamp,\n",
    "        'device': str(device)\n",
    "    }, f)\n",
    "print(f\"✓ Saved: complete_results.pkl\")\n",
    "\n",
    "# Save configuration\n",
    "config_file = results_dir / \"experiment_config.json\"\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'bert_config': BERT_CONFIG,\n",
    "        'model_configs': {k: {**v, 'model_name': v['model_name']} \n",
    "                         for k, v in MODEL_CONFIGS.items()},\n",
    "        'num_classes': num_classes,\n",
    "        'label_map': LABEL_MAP,\n",
    "        'device': str(device),\n",
    "        'timestamp': timestamp,\n",
    "        'num_splits_processed': len(all_split_results)\n",
    "    }, f, indent=2)\n",
    "print(f\"✓ Saved: experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL DEPLOYMENT MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "deployment_dir = MODELS_PATH / \"deployment\"\n",
    "deployment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Collect all training data\n",
    "all_train_texts = []\n",
    "all_train_labels = []\n",
    "\n",
    "for source, source_data in dat.items():\n",
    "    if source_data['labels'] is not None and len(np.unique(source_data['labels'])) >= 2:\n",
    "        all_train_texts.extend(source_data['texts'])\n",
    "        all_train_labels.extend(source_data['labels'])\n",
    "\n",
    "all_train_texts = np.array(all_train_texts)\n",
    "all_train_labels = np.array(all_train_labels)\n",
    "\n",
    "print(f\"Total training samples: {len(all_train_labels):,}\")\n",
    "\n",
    "# Calculate overall imbalance\n",
    "unique, counts = np.unique(all_train_labels, return_counts=True)\n",
    "overall_imb_ratio = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "print(f\"Overall imbalance ratio: {overall_imb_ratio:.2f}:1\")\n",
    "\n",
    "# Split into train/val\n",
    "final_train_texts, final_val_texts, final_train_labels, final_val_labels = train_test_split(\n",
    "    all_train_texts, all_train_labels, test_size=0.1, random_state=SEED, \n",
    "    stratify=all_train_labels\n",
    ")\n",
    "\n",
    "print(f\"Final train: {len(final_train_labels):,}\")\n",
    "print(f\"Final val: {len(final_val_labels):,}\")\n",
    "\n",
    "# Train each model - ONE AT A TIME\n",
    "deployment_models = {}\n",
    "\n",
    "for model_key, model_config in MODEL_CONFIGS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Final {model_key.upper()} for Deployment\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        if model_key == 'deberta_v3':\n",
    "            tokenizer = DebertaV2Tokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        elif model_key == 'mpnet':\n",
    "            tokenizer = MPNetTokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        else:\n",
    "            tokenizer = BertTokenizer.from_pretrained(model_config['model_name'], use_safetensors=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        if model_key == 'logbert':\n",
    "            model = LogBERT(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            ).to(device)\n",
    "        elif model_key == 'dapt_bert':\n",
    "            model = DomainAdaptedBERT(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            ).to(device)\n",
    "        elif model_key == 'deberta_v3':\n",
    "            model = DeBERTaV3Classifier(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            ).to(device)\n",
    "        elif model_key == 'mpnet':\n",
    "            model = MPNetClassifier(\n",
    "                model_name=model_config['model_name'],\n",
    "                num_classes=num_classes,\n",
    "                dropout=BERT_CONFIG['dropout']\n",
    "            ).to(device)\n",
    "        \n",
    "        print(f\"Model initialized: {model_config['model_name']}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Train model\n",
    "        result = train_model(\n",
    "            model_key, model, tokenizer,\n",
    "            final_train_texts, final_train_labels,\n",
    "            final_val_texts, final_val_labels,\n",
    "            final_val_texts, final_val_labels,\n",
    "            imbalance_ratio=overall_imb_ratio,\n",
    "            config=BERT_CONFIG\n",
    "        )\n",
    "        \n",
    "        deployment_models[model_key] = {\n",
    "            'model': result['model'],\n",
    "            'tokenizer': tokenizer,\n",
    "            'optimal_threshold': result['optimal_threshold'],\n",
    "            'metrics': result['test_metrics_tuned'],\n",
    "            'model_config': model_config\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {model_key.upper()} trained successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training {model_key}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'tokenizer' in locals():\n",
    "            del tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING DEPLOYMENT MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "feature_hash = compute_file_hash(feat_file)\n",
    "split_hash = compute_file_hash(split_file)\n",
    "\n",
    "# Save each model\n",
    "for model_key, deployment_data in deployment_models.items():\n",
    "    print(f\"\\nSaving {model_key.upper()}...\")\n",
    "    \n",
    "    model_save_dir = deployment_dir / model_key\n",
    "    model_save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save PyTorch model state dict\n",
    "    model_state_file = model_save_dir / \"model_state.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': deployment_data['model'].state_dict(),\n",
    "        'model_config': deployment_data['model_config'],\n",
    "        'bert_config': BERT_CONFIG,\n",
    "        'num_classes': num_classes,\n",
    "        'label_map': LABEL_MAP,\n",
    "        'optimal_threshold': deployment_data['optimal_threshold'],\n",
    "        'training_samples': len(all_train_labels),\n",
    "        'imbalance_ratio': float(overall_imb_ratio),\n",
    "        'timestamp': timestamp,\n",
    "        'feature_hash': feature_hash,\n",
    "        'split_hash': split_hash,\n",
    "        'version': '1.0.0'\n",
    "    }, model_state_file)\n",
    "    print(f\"✓ Saved: {model_key}/model_state.pt\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_dir = model_save_dir / \"tokenizer\"\n",
    "    deployment_data['tokenizer'].save_pretrained(tokenizer_dir)\n",
    "    print(f\"✓ Saved: {model_key}/tokenizer/\")\n",
    "    \n",
    "    # Save complete model\n",
    "    complete_model_file = model_save_dir / \"complete_model.pkl\"\n",
    "    with open(complete_model_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': deployment_data['model'].cpu(),\n",
    "            'tokenizer': deployment_data['tokenizer'],\n",
    "            'optimal_threshold': deployment_data['optimal_threshold'],\n",
    "            'model_config': deployment_data['model_config'],\n",
    "            'bert_config': BERT_CONFIG,\n",
    "            'num_classes': num_classes,\n",
    "            'label_map': LABEL_MAP,\n",
    "            'metrics': deployment_data['metrics'],\n",
    "            'training_info': {\n",
    "                'training_samples': len(all_train_labels),\n",
    "                'imbalance_ratio': float(overall_imb_ratio),\n",
    "                'timestamp': timestamp,\n",
    "                'feature_hash': feature_hash,\n",
    "                'split_hash': split_hash,\n",
    "                'version': '1.0.0'\n",
    "            }\n",
    "        }, f)\n",
    "    print(f\"✓ Saved: {model_key}/complete_model.pkl\")\n",
    "    \n",
    "    deployment_data['model'].to(device)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = model_save_dir / \"deployment_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'model_name': model_key,\n",
    "            'model_type': deployment_data['model_config']['model_name'],\n",
    "            'num_classes': num_classes,\n",
    "            'label_map': LABEL_MAP,\n",
    "            'optimal_threshold': float(deployment_data['optimal_threshold']),\n",
    "            'metrics': {\n",
    "                'f1_macro': float(deployment_data['metrics']['f1_macro']),\n",
    "                'balanced_acc': float(deployment_data['metrics']['bal_acc']),\n",
    "                'auroc': float(deployment_data['metrics']['auroc']),\n",
    "                'mcc': float(deployment_data['metrics']['mcc'])\n",
    "            },\n",
    "            'training_info': {\n",
    "                'training_samples': int(len(all_train_labels)),\n",
    "                'imbalance_ratio': float(overall_imb_ratio),\n",
    "                'timestamp': timestamp,\n",
    "                'feature_hash': feature_hash[:16],\n",
    "                'split_hash': split_hash[:16]\n",
    "            },\n",
    "            'config': {\n",
    "                'max_length': BERT_CONFIG['max_length'],\n",
    "                'batch_size': BERT_CONFIG['batch_size'],\n",
    "                'dropout': BERT_CONFIG['dropout']\n",
    "            },\n",
    "            'version': '1.0.0',\n",
    "            'framework': 'pytorch',\n",
    "            'device_trained': str(device)\n",
    "        }, f, indent=2)\n",
    "    print(f\"✓ Saved: {model_key}/deployment_metadata.json\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del deployment_data['model']\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save best model separately\n",
    "best_model_key = best_overall_model.lower()\n",
    "if best_model_key in deployment_models:\n",
    "    best_model_dir = deployment_dir / \"best_model\"\n",
    "    best_model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    import shutil\n",
    "    src_dir = deployment_dir / best_model_key\n",
    "    for file in src_dir.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            shutil.copy2(file, best_model_dir / file.name)\n",
    "        elif file.is_dir():\n",
    "            shutil.copytree(file, best_model_dir / file.name, dirs_exist_ok=True)\n",
    "    \n",
    "    best_model_info = best_model_dir / \"BEST_MODEL_INFO.txt\"\n",
    "    with open(best_model_info, 'w') as f:\n",
    "        f.write(f\"Best Overall Model: {best_overall_model}\\n\")\n",
    "        f.write(f\"Average F1-Macro: {df_overall.iloc[0]['Avg F1-Macro']:.4f}\\n\")\n",
    "        f.write(f\"Average Balanced Acc: {df_overall.iloc[0]['Avg Balanced Acc']:.4f}\\n\")\n",
    "        f.write(f\"Average AUROC: {df_overall.iloc[0]['Avg AUROC']:.4f}\\n\")\n",
    "        f.write(f\"Evaluated on: {df_overall.iloc[0]['Sources']} sources\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    \n",
    "    print(f\"\\n✓ Best model ({best_overall_model}) copied to: best_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('BERT Models - Cross-Source Evaluation Results', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Overall F1-Macro comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = df_overall['Model'].values\n",
    "f1_scores = df_overall['Avg F1-Macro'].values\n",
    "colors = plt.cm.RdYlGn(f1_scores / f1_scores.max())\n",
    "\n",
    "bars = ax1.barh(models, f1_scores, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('Average F1-Macro', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    ax1.text(score + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "            f'{score:.3f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Metrics comparison\n",
    "ax2 = axes[0, 1]\n",
    "metrics_data = []\n",
    "for model_key, results_list in model_summaries.items():\n",
    "    if results_list:\n",
    "        for metric_name in ['f1_macro', 'bal_acc', 'auroc', 'mcc']:\n",
    "            for result in results_list:\n",
    "                if metric_name in result:\n",
    "                    metrics_data.append({\n",
    "                        'Model': model_key.upper(),\n",
    "                        'Metric': metric_name.upper().replace('_', ' '),\n",
    "                        'Score': result[metric_name]\n",
    "                    })\n",
    "\n",
    "if metrics_data:\n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    df_pivot = df_metrics.pivot_table(\n",
    "        index='Model', columns='Metric', values='Score', aggfunc='mean'\n",
    "    )\n",
    "    df_pivot.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Average Metrics by Model', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(loc='lower right', fontsize=9)\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: F1 distribution\n",
    "ax3 = axes[0, 2]\n",
    "f1_distributions = []\n",
    "model_labels = []\n",
    "for model_key, results_list in model_summaries.items():\n",
    "    if results_list:\n",
    "        f1_scores = [r['f1_macro'] for r in results_list]\n",
    "        f1_distributions.append(f1_scores)\n",
    "        model_labels.append(model_key.upper())\n",
    "\n",
    "if f1_distributions:\n",
    "    bp = ax3.boxplot(f1_distributions, labels=model_labels, patch_artist=True,\n",
    "                     showmeans=True, meanline=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], sns.color_palette(\"husl\", len(model_labels))):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax3.set_ylabel('F1-Macro', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('F1 Distribution Across Sources', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: Per-source heatmap\n",
    "ax4 = axes[1, 0]\n",
    "best_model_key = best_overall_model.lower()\n",
    "if best_model_key in model_summaries and model_summaries[best_model_key]:\n",
    "    results_list = model_summaries[best_model_key]\n",
    "    sources = [r['test_source'] for r in results_list]\n",
    "    metrics_matrix = np.array([\n",
    "        [r['f1_macro'], r['bal_acc'], r['auroc'], r['mcc']]\n",
    "        for r in results_list\n",
    "    ])\n",
    "    \n",
    "    im = ax4.imshow(metrics_matrix.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    ax4.set_xticks(np.arange(len(sources)))\n",
    "    ax4.set_yticks(np.arange(4))\n",
    "    ax4.set_xticklabels(sources, rotation=45, ha='right', fontsize=8)\n",
    "    ax4.set_yticklabels(['F1-Macro', 'Bal Acc', 'AUROC', 'MCC'])\n",
    "    ax4.set_title(f'{best_overall_model} - Per-Source Metrics', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        for j in range(4):\n",
    "            text = ax4.text(i, j, f'{metrics_matrix[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax4)\n",
    "\n",
    "# Plot 5: Learning curves (if available)\n",
    "ax5 = axes[1, 1]\n",
    "if all_split_results and 'results' in all_split_results[0]:\n",
    "    first_result = all_split_results[0]['results']\n",
    "    for model_key in MODEL_CONFIGS.keys():\n",
    "        if model_key in first_result and 'error' not in first_result[model_key]:\n",
    "            history = first_result[model_key].get('history', {})\n",
    "            if 'val_f1' in history and len(history['val_f1']) > 0:\n",
    "                ax5.plot(history['val_f1'], label=model_key.upper(), marker='o')\n",
    "    \n",
    "    ax5.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Validation F1-Macro', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Learning Curves (First Split)', fontsize=13, fontweight='bold')\n",
    "    ax5.legend(fontsize=9)\n",
    "    ax5.grid(alpha=0.3, linestyle='--')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Learning curves\\n(no data available)', \n",
    "            ha='center', va='center', fontsize=12, transform=ax5.transAxes)\n",
    "    ax5.axis('off')\n",
    "\n",
    "# Plot 6: Threshold distribution\n",
    "ax6 = axes[1, 2]\n",
    "threshold_data = []\n",
    "for model_key, results_list in model_summaries.items():\n",
    "    if results_list:\n",
    "        for result in results_list:\n",
    "            threshold_data.append({\n",
    "                'Model': model_key.upper(),\n",
    "                'Threshold': result['threshold']\n",
    "            })\n",
    "\n",
    "if threshold_data:\n",
    "    df_thresholds = pd.DataFrame(threshold_data)\n",
    "    df_thresholds.boxplot(column='Threshold', by='Model', ax=ax6, patch_artist=True)\n",
    "    ax6.set_ylabel('Optimal Threshold', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('Threshold Distribution by Model', fontsize=13, fontweight='bold')\n",
    "    ax6.set_xlabel('')\n",
    "    plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    ax6.get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "viz_file = results_dir / \"aggregate_visualization.png\"\n",
    "plt.savefig(viz_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"✓ Saved: aggregate_visualization.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
