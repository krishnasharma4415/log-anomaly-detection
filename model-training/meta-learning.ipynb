{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bcb1eb",
   "metadata": {},
   "source": [
    "Meta-Learning for Few-Shot Log Anomaly Detection in Extreme Imbalance Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35d7c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Import GradScaler with backward compatibility\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP_API = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP_API = False\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee125145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "ROOT = Path(r\"C:\\Computer Science\\AIMLDL\\log-anomaly-detection\")\n",
    "FEAT_PATH = ROOT / \"features\"\n",
    "RESULTS_PATH = ROOT / \"results\" / \"meta_learning\"\n",
    "MODELS_PATH = ROOT / \"models\" / \"meta_learning\"\n",
    "\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21301e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 sources\n",
      "Classes: 2\n"
     ]
    }
   ],
   "source": [
    "feat_file = FEAT_PATH / \"enhanced_imbalanced_features.pkl\"\n",
    "with open(feat_file, 'rb') as f:\n",
    "    feat_data = pickle.load(f)\n",
    "    data_dict = feat_data['hybrid_features_data']\n",
    "    num_classes = feat_data['config'].get('num_classes', 2)\n",
    "\n",
    "split_file = FEAT_PATH / \"enhanced_cross_source_splits.pkl\"\n",
    "with open(split_file, 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    splits = split_data['splits']\n",
    "\n",
    "print(f\"Loaded {len(data_dict)} sources\")\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "LABEL_MAP = {0: 'normal', 1: 'anomaly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25b44e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_acc'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    per_class = {}\n",
    "    for cls in np.unique(np.concatenate([y_true, y_pred])):\n",
    "        y_true_bin = (y_true == cls).astype(int)\n",
    "        y_pred_bin = (y_pred == cls).astype(int)\n",
    "        if y_true_bin.sum() > 0:\n",
    "            per_class[int(cls)] = {\n",
    "                'precision': precision_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'recall': recall_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'f1': f1_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "                'support': int(y_true_bin.sum())\n",
    "            }\n",
    "    metrics['per_class'] = per_class\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            metrics['auprc'] = average_precision_score(y_true, y_proba[:, 1])\n",
    "        except:\n",
    "            metrics['auroc'] = 0.0\n",
    "            metrics['auprc'] = 0.0\n",
    "    else:\n",
    "        metrics['auroc'] = 0.0\n",
    "        metrics['auprc'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d624bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_episode(X, y, n_way, k_shot, q_query, balance=True):\n",
    "    classes = np.unique(y)\n",
    "    if len(classes) < n_way:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    selected_classes = np.random.choice(classes, n_way, replace=False)\n",
    "    \n",
    "    support_X, support_y = [], []\n",
    "    query_X, query_y = [], []\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        \n",
    "        if len(cls_indices) < k_shot + q_query:\n",
    "            if balance:\n",
    "                return None, None, None, None\n",
    "            else:\n",
    "                available = len(cls_indices)\n",
    "                k_use = min(k_shot, available // 2)\n",
    "                q_use = available - k_use\n",
    "                if k_use == 0 or q_use == 0:\n",
    "                    return None, None, None, None\n",
    "        else:\n",
    "            k_use = k_shot\n",
    "            q_use = q_query\n",
    "        \n",
    "        selected = np.random.choice(cls_indices, k_use + q_use, replace=False)\n",
    "        support_indices = selected[:k_use]\n",
    "        query_indices = selected[k_use:k_use + q_use]\n",
    "        \n",
    "        support_X.append(X[support_indices])\n",
    "        support_y.append(y[support_indices])\n",
    "        query_X.append(X[query_indices])\n",
    "        query_y.append(y[query_indices])\n",
    "    \n",
    "    support_X = np.vstack(support_X)\n",
    "    support_y = np.concatenate(support_y)\n",
    "    query_X = np.vstack(query_X)\n",
    "    query_y = np.concatenate(query_y)\n",
    "    \n",
    "    shuffle_support = np.random.permutation(len(support_y))\n",
    "    support_X = support_X[shuffle_support]\n",
    "    support_y = support_y[shuffle_support]\n",
    "    \n",
    "    shuffle_query = np.random.permutation(len(query_y))\n",
    "    query_X = query_X[shuffle_query]\n",
    "    query_y = query_y[shuffle_query]\n",
    "    \n",
    "    return support_X, support_y, query_X, query_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "538bdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_imbalanced_episode(X, y, k_shot_range=(3, 10), q_query=15):\n",
    "    \"\"\"Create episodes with controlled imbalance and stratification\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    if len(classes) != 2:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    class_counts = [np.sum(y == cls) for cls in classes]\n",
    "    minority_cls = classes[np.argmin(class_counts)]\n",
    "    majority_cls = classes[np.argmax(class_counts)]\n",
    "    \n",
    "    minority_indices = np.where(y == minority_cls)[0]\n",
    "    majority_indices = np.where(y == majority_cls)[0]\n",
    "    \n",
    "    k_minority = min(k_shot_range[1], len(minority_indices) // 3)\n",
    "    k_majority = min(k_minority * 3, len(majority_indices) // 3)\n",
    "\n",
    "    if len(minority_indices) < k_minority + q_query:\n",
    "        # Adjust k_minority and q_query to fit available samples\n",
    "        total_minority = len(minority_indices)\n",
    "        if total_minority < k_shot_range[0] + 1:  # Need at least k_shot_range[0] + 1 sample\n",
    "            return None, None, None, None\n",
    "        k_minority = max(k_shot_range[0], total_minority // 2)\n",
    "        q_query_minority = total_minority - k_minority\n",
    "        if q_query_minority < 1:\n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        q_query_minority = q_query\n",
    "    \n",
    "    if len(majority_indices) < k_majority + q_query:\n",
    "        # Adjust k_majority and q_query to fit available samples\n",
    "        total_majority = len(majority_indices)\n",
    "        if total_majority < k_shot_range[0] + 1:\n",
    "            return None, None, None, None\n",
    "        k_majority = max(k_shot_range[0], total_majority // 2)\n",
    "        q_query_majority = total_majority - k_majority\n",
    "        if q_query_majority < 1:\n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        q_query_majority = q_query\n",
    "    \n",
    "    if k_minority < k_shot_range[0] or k_majority < k_shot_range[0]:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    minority_selected = np.random.choice(minority_indices, k_minority + q_query_minority, replace=False)\n",
    "    majority_selected = np.random.choice(majority_indices, k_majority + q_query_majority, replace=False)\n",
    "    \n",
    "    support_X = np.vstack([X[minority_selected[:k_minority]], X[majority_selected[:k_majority]]])\n",
    "    support_y = np.concatenate([y[minority_selected[:k_minority]], y[majority_selected[:k_majority]]])\n",
    "    \n",
    "    query_X = np.vstack([X[minority_selected[k_minority:]], X[majority_selected[k_majority:]]])\n",
    "    query_y = np.concatenate([y[minority_selected[k_minority:]], y[majority_selected[k_majority:]]])\n",
    "    \n",
    "    shuffle_support = np.random.permutation(len(support_y))\n",
    "    support_X = support_X[shuffle_support]\n",
    "    support_y = support_y[shuffle_support]\n",
    "    \n",
    "    shuffle_query = np.random.permutation(len(query_y))\n",
    "    query_X = query_X[shuffle_query]\n",
    "    query_y = query_y[shuffle_query]\n",
    "    \n",
    "    return support_X, support_y, query_X, query_y\n",
    "\n",
    "def create_imbalanced_episode(X, y, minority_k_shot, majority_k_shot, q_query_per_class):\n",
    "    classes = np.unique(y)\n",
    "    if len(classes) != 2:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    class_counts = [np.sum(y == cls) for cls in classes]\n",
    "    minority_cls = classes[np.argmin(class_counts)]\n",
    "    majority_cls = classes[np.argmax(class_counts)]\n",
    "    \n",
    "    minority_indices = np.where(y == minority_cls)[0]\n",
    "    majority_indices = np.where(y == majority_cls)[0]\n",
    "    \n",
    "    if len(minority_indices) < minority_k_shot + q_query_per_class:\n",
    "        return None, None, None, None\n",
    "    if len(majority_indices) < majority_k_shot + q_query_per_class:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    minority_selected = np.random.choice(minority_indices, minority_k_shot + q_query_per_class, replace=False)\n",
    "    majority_selected = np.random.choice(majority_indices, majority_k_shot + q_query_per_class, replace=False)\n",
    "    \n",
    "    support_X = np.vstack([X[minority_selected[:minority_k_shot]], X[majority_selected[:majority_k_shot]]])\n",
    "    support_y = np.concatenate([y[minority_selected[:minority_k_shot]], y[majority_selected[:majority_k_shot]]])\n",
    "    \n",
    "    query_X = np.vstack([X[minority_selected[minority_k_shot:]], X[majority_selected[majority_k_shot:]]])\n",
    "    query_y = np.concatenate([y[minority_selected[minority_k_shot:]], y[majority_selected[majority_k_shot:]]])\n",
    "    \n",
    "    shuffle_support = np.random.permutation(len(support_y))\n",
    "    support_X = support_X[shuffle_support]\n",
    "    support_y = support_y[shuffle_support]\n",
    "    \n",
    "    shuffle_query = np.random.permutation(len(query_y))\n",
    "    query_X = query_X[shuffle_query]\n",
    "    query_y = query_y[shuffle_query]\n",
    "    \n",
    "    return support_X, support_y, query_X, query_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80d06365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototypical_loss(embeddings, labels, n_way):\n",
    "    classes = torch.unique(labels)\n",
    "    prototypes = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_mask = labels == cls\n",
    "        cls_embeddings = embeddings[cls_mask]\n",
    "        prototype = cls_embeddings.mean(dim=0)\n",
    "        prototypes.append(prototype)\n",
    "    \n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    distances = torch.cdist(embeddings, prototypes, p=2)\n",
    "    log_probs = F.log_softmax(-distances, dim=1)\n",
    "    \n",
    "    loss = F.nll_loss(log_probs, labels)\n",
    "    return loss\n",
    "\n",
    "def focal_loss(logits, labels, alpha=0.75, gamma=3.0):\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "def contrastive_loss(embeddings, labels, temperature=0.5):\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "    similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "    \n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "    \n",
    "    logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)\n",
    "    logits = similarity_matrix - logits_max.detach()\n",
    "    \n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "    \n",
    "    mask_sum = mask.sum(1)\n",
    "    mask_sum = torch.clamp(mask_sum, min=1.0)\n",
    "    \n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask_sum\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def combined_meta_loss(embeddings, logits, labels, prototypes=None, \n",
    "                       alpha_focal=0.4, alpha_proto=0.3, alpha_contrastive=0.3):\n",
    "    \"\"\"Multi-task loss for better representation learning\"\"\"\n",
    "    focal = focal_loss(logits, labels)\n",
    "    \n",
    "    if prototypes is not None:\n",
    "        proto = prototypical_loss(embeddings, labels, len(prototypes))\n",
    "    else:\n",
    "        proto = torch.tensor(0.0).to(embeddings.device)\n",
    "    \n",
    "    contrastive = contrastive_loss(embeddings, labels, temperature=0.5)\n",
    "    \n",
    "    return alpha_focal * focal + alpha_proto * proto + alpha_contrastive * contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c64f3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeCache:\n",
    "    def __init__(self, cache_size=500):\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get_key(self, source_name, k_min, k_maj, q_query, seed):\n",
    "        return f\"{source_name}_{k_min}_{k_maj}_{q_query}_{seed}\"\n",
    "    \n",
    "    def get_episode(self, source_name, X, y, k_min, k_maj, q_query):\n",
    "        cache_seed = np.random.randint(0, self.cache_size)\n",
    "        key = self.get_key(source_name, k_min, k_maj, q_query, cache_seed)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        episode = create_balanced_imbalanced_episode(X, y, q_query=q_query)\n",
    "        \n",
    "        if episode[0] is not None and len(self.cache) < self.cache_size:\n",
    "            self.cache[key] = episode\n",
    "        \n",
    "        return episode\n",
    "    \n",
    "    def get_stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return f\"Cache hit rate: {hit_rate:.2%} ({self.hits}/{total})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b515e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_support_set(support_X, support_y, augment_factor=2):\n",
    "    \"\"\"Augment minority class samples with noise and mixup\"\"\"\n",
    "    minority_cls = np.argmin(np.bincount(support_y))\n",
    "    minority_mask = support_y == minority_cls\n",
    "    minority_X = support_X[minority_mask]\n",
    "    \n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    \n",
    "    for _ in range(augment_factor):\n",
    "        noise = np.random.normal(0, 0.01, minority_X.shape)\n",
    "        augmented_X.append(minority_X + noise)\n",
    "        augmented_y.append(np.full(len(minority_X), minority_cls))\n",
    "    \n",
    "    for i in range(len(minority_X)):\n",
    "        j = np.random.randint(len(minority_X))\n",
    "        lam = np.random.beta(0.2, 0.2)\n",
    "        mixed = lam * minority_X[i] + (1 - lam) * minority_X[j]\n",
    "        augmented_X.append(mixed.reshape(1, -1))\n",
    "        augmented_y.append([minority_cls])\n",
    "    \n",
    "    support_X_aug = np.vstack([support_X] + augmented_X)\n",
    "    support_y_aug = np.concatenate([support_y] + augmented_y)\n",
    "    \n",
    "    return support_X_aug, support_y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d81454c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_inner_steps(imbalance_ratio, base_steps=5):\n",
    "    \"\"\"Adjust inner loop steps based on task difficulty\"\"\"\n",
    "    if imbalance_ratio > 100:\n",
    "        return base_steps * 3\n",
    "    elif imbalance_ratio > 20:\n",
    "        return base_steps * 2\n",
    "    return base_steps\n",
    "\n",
    "class TaskAdaptiveLR:\n",
    "    def __init__(self, base_lr=1e-2):\n",
    "        self.base_lr = base_lr\n",
    "    \n",
    "    def get_lr(self, imbalance_ratio):\n",
    "        \"\"\"Adjust LR based on task characteristics\"\"\"\n",
    "        if imbalance_ratio > 100:\n",
    "            return self.base_lr * 0.5\n",
    "        elif imbalance_ratio < 5:\n",
    "            return self.base_lr * 1.5\n",
    "        return self.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fc39cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.bn2 = nn.BatchNorm1d(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.fc2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weights = F.softmax(self.attention(x), dim=0)\n",
    "        return (x * weights).sum(dim=0, keepdim=True)\n",
    "\n",
    "def meta_network(input_dim, hidden_dims=[512, 256, 128], output_dim=128, dropout=0.4):\n",
    "    layers = []\n",
    "    prev_dim = input_dim\n",
    "    for hidden_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        prev_dim = hidden_dim\n",
    "    layers.append(nn.Linear(prev_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def classifier_head(input_dim, num_classes=2, dropout=0.4):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, input_dim // 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(input_dim // 2, num_classes)\n",
    "    )\n",
    "\n",
    "def attention_pooling(input_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, 1),\n",
    "        nn.Softmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e240452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml_inner_loop(model, support_X, support_y, inner_lr, inner_steps, loss_fn, use_amp=True):\n",
    "    model_copy = ImprovedMetaLearner(model.input_dim, model.hidden_dims, model.embedding_dim, \n",
    "                                     model.dropout, model.num_classes).to(device)\n",
    "    \n",
    "    # Handle torch.compile state dict\n",
    "    state_dict = model.state_dict()\n",
    "    if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
    "        state_dict = {key.replace('_orig_mod.', ''): value for key, value in state_dict.items()}\n",
    "    \n",
    "    model_copy.load_state_dict(state_dict)\n",
    "    \n",
    "    optimizer = SGD(model_copy.parameters(), lr=inner_lr)\n",
    "    \n",
    "    support_X_tensor = torch.FloatTensor(support_X).to(device, non_blocking=True)\n",
    "    support_y_tensor = torch.LongTensor(support_y).to(device, non_blocking=True)\n",
    "    \n",
    "    prev_loss = float('inf')\n",
    "    tolerance = 1e-4\n",
    "    \n",
    "    for step in range(inner_steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp and torch.cuda.is_available():\n",
    "            device_type = 'cuda' if USE_NEW_AMP_API else None\n",
    "            with autocast(device_type=device_type) if USE_NEW_AMP_API else autocast():\n",
    "                logits = model_copy.predict(support_X_tensor)\n",
    "                loss = loss_fn(logits, support_y_tensor)\n",
    "        else:\n",
    "            logits = model_copy.predict(support_X_tensor)\n",
    "            loss = loss_fn(logits, support_y_tensor)\n",
    "        \n",
    "        # Early stopping if converged\n",
    "        if step > 2 and abs(prev_loss - loss.item()) < tolerance:\n",
    "            break\n",
    "        prev_loss = loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_copy.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model_copy\n",
    "\n",
    "def batched_maml_inner_loop(model, support_X, support_y, inner_lr, inner_steps, loss_fn, batch_size=32):\n",
    "    \"\"\"Batched inner loop for faster training\"\"\"\n",
    "    model_copy = ImprovedMetaLearner(model.input_dim, model.hidden_dims, model.embedding_dim,\n",
    "                                     model.dropout, model.num_classes).to(device)\n",
    "    \n",
    "    # Handle torch.compile state dict\n",
    "    state_dict = model.state_dict()\n",
    "    if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
    "        state_dict = {key.replace('_orig_mod.', ''): value for key, value in state_dict.items()}\n",
    "    \n",
    "    model_copy.load_state_dict(state_dict)\n",
    "    \n",
    "    optimizer = SGD(model_copy.parameters(), lr=inner_lr)\n",
    "    \n",
    "    support_X_tensor = torch.FloatTensor(support_X).to(device, non_blocking=True)\n",
    "    support_y_tensor = torch.LongTensor(support_y).to(device, non_blocking=True)\n",
    "    \n",
    "    dataset_size = len(support_y)\n",
    "    \n",
    "    for step in range(inner_steps):\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        \n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = support_X_tensor[batch_indices]\n",
    "            y_batch = support_y_tensor[batch_indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            device_type = 'cuda' if USE_NEW_AMP_API else None\n",
    "            with autocast(device_type=device_type) if USE_NEW_AMP_API else autocast():\n",
    "                logits = model_copy.predict(X_batch)\n",
    "                loss = loss_fn(logits, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_copy.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model_copy\n",
    "\n",
    "def memory_efficient_maml(model, support_X, support_y, inner_lr, inner_steps, loss_fn):\n",
    "    params = {name: param.clone() for name, param in model.named_parameters()}\n",
    "    \n",
    "    support_X_tensor = torch.FloatTensor(support_X).to(device)\n",
    "    support_y_tensor = torch.LongTensor(support_y).to(device)\n",
    "    \n",
    "    for step in range(inner_steps):\n",
    "        embeddings = model(support_X_tensor)\n",
    "        logits = model.classifier(embeddings)\n",
    "        loss = loss_fn(logits, support_y_tensor)\n",
    "        \n",
    "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "        \n",
    "        for (name, param), grad in zip(model.named_parameters(), grads):\n",
    "            param.data = param.data - inner_lr * grad\n",
    "    \n",
    "    result_params = {name: param.clone() for name, param in model.named_parameters()}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = params[name].data\n",
    "    \n",
    "    return result_params\n",
    "\n",
    "def process_single_task(encoder_state, classifier_state, source_name, source_features, source_labels, \n",
    "                       source_k_shots, inner_lr, inner_steps, q_query, device_id, input_dim, hidden_dims, \n",
    "                       embedding_dim, dropout, num_classes):\n",
    "    local_device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    local_encoder = meta_network(input_dim, hidden_dims, embedding_dim, dropout).to(local_device)\n",
    "    local_encoder.load_state_dict(encoder_state)\n",
    "    local_encoder.input_dim = input_dim\n",
    "    local_encoder.hidden_dims = hidden_dims\n",
    "    local_encoder.output_dim = embedding_dim\n",
    "    local_encoder.dropout = dropout\n",
    "    \n",
    "    local_classifier = classifier_head(embedding_dim, num_classes, dropout).to(local_device)\n",
    "    local_classifier.load_state_dict(classifier_state)\n",
    "    local_encoder.classifier = local_classifier\n",
    "    \n",
    "    X_source = source_features[source_name]\n",
    "    y_source = source_labels[source_name]\n",
    "    k_shots = source_k_shots[source_name]\n",
    "    \n",
    "    episode = create_imbalanced_episode(\n",
    "        X_source, y_source, \n",
    "        k_shots['minority'], k_shots['majority'], q_query\n",
    "    )\n",
    "    \n",
    "    if episode[0] is None:\n",
    "        return None\n",
    "    \n",
    "    support_X, support_y, query_X, query_y = episode\n",
    "    \n",
    "    adapted_model = maml_inner_loop(\n",
    "        local_encoder, support_X, support_y, \n",
    "        inner_lr, inner_steps, focal_loss\n",
    "    )\n",
    "    \n",
    "    query_X_tensor = torch.FloatTensor(query_X).to(local_device)\n",
    "    query_y_tensor = torch.LongTensor(query_y).to(local_device)\n",
    "    \n",
    "    query_embeddings = adapted_model(query_X_tensor)\n",
    "    query_logits = adapted_model.classifier(query_embeddings)\n",
    "    \n",
    "    task_loss = focal_loss(query_logits, query_y_tensor)\n",
    "    \n",
    "    return task_loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92b84af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prototypes(embeddings, labels):\n",
    "    classes = torch.unique(labels)\n",
    "    prototypes = []\n",
    "    for cls in classes:\n",
    "        cls_mask = labels == cls\n",
    "        cls_embeddings = embeddings[cls_mask]\n",
    "        prototype = cls_embeddings.mean(dim=0)\n",
    "        prototypes.append(prototype)\n",
    "    return torch.stack(prototypes), classes\n",
    "\n",
    "def refined_prototypes(embeddings, labels, momentum=0.9):\n",
    "    \"\"\"Exponential moving average for stable prototypes\"\"\"\n",
    "    if not hasattr(refined_prototypes, 'proto_memory'):\n",
    "        refined_prototypes.proto_memory = {}\n",
    "    \n",
    "    classes = torch.unique(labels)\n",
    "    prototypes = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_mask = labels == cls\n",
    "        cls_embeddings = embeddings[cls_mask]\n",
    "        current_proto = cls_embeddings.mean(dim=0)\n",
    "        \n",
    "        cls_key = int(cls.item())\n",
    "        if cls_key in refined_prototypes.proto_memory:\n",
    "            refined_proto = (momentum * refined_prototypes.proto_memory[cls_key] + \n",
    "                           (1 - momentum) * current_proto)\n",
    "        else:\n",
    "            refined_proto = current_proto\n",
    "        \n",
    "        refined_prototypes.proto_memory[cls_key] = refined_proto.detach()\n",
    "        prototypes.append(refined_proto)\n",
    "    \n",
    "    return torch.stack(prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ff5f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZED_CONFIG = {\n",
    "    'input_dim': 200,\n",
    "    'hidden_dims': [512, 256, 128],\n",
    "    'embedding_dim': 128,\n",
    "    'dropout': 0.4,\n",
    "    'meta_lr': 5e-4,\n",
    "    'inner_lr': 5e-3,\n",
    "    'inner_steps': 10,\n",
    "    'meta_batch_size': 16,\n",
    "    'focal_alpha': 0.75,\n",
    "    'focal_gamma': 3.0,\n",
    "    'augment_minority': True,\n",
    "    'use_mixup': True,\n",
    "    'use_amp': True,\n",
    "    'use_cache': True,\n",
    "    'use_batched_inner': False,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "# Speed-optimized config for 8GB GPU\n",
    "SPEED_OPTIMIZED_CONFIG = {\n",
    "    'input_dim': 200,\n",
    "    'hidden_dims': [256, 128],\n",
    "    'embedding_dim': 64,\n",
    "    'dropout': 0.3,\n",
    "    'meta_lr': 5e-4,\n",
    "    'inner_lr': 5e-3,\n",
    "    'inner_steps': 8,\n",
    "    'meta_batch_size': 8,\n",
    "    'focal_alpha': 0.75,\n",
    "    'focal_gamma': 3.0,\n",
    "    'augment_minority': False,\n",
    "    'use_mixup': False,\n",
    "    'use_amp': True,\n",
    "    'use_cache': True,\n",
    "    'use_batched_inner': True,\n",
    "    'gradient_accumulation_steps': 2\n",
    "}\n",
    "\n",
    "input_dim = OPTIMIZED_CONFIG['input_dim']\n",
    "hidden_dims = OPTIMIZED_CONFIG['hidden_dims']\n",
    "embedding_dim = OPTIMIZED_CONFIG['embedding_dim']\n",
    "dropout = OPTIMIZED_CONFIG['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea6a3571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,331,778\n",
      "Encoder parameters: 164,736\n",
      "Classifier parameters: 8,386\n",
      "\n",
      "Meta-Learning Configuration:\n",
      "  Meta LR: 0.0005\n",
      "  Inner LR: 0.005\n",
      "  Inner steps: 10\n",
      "  Meta batch size: 16\n",
      "  Iterations: 1000\n",
      "  K-shot (minority): 5\n",
      "  K-shot (majority): 10\n",
      "  Query samples: 15\n",
      "  Early stopping patience: 100\n",
      "  LR scheduler: CosineAnnealingWarmRestarts\n"
     ]
    }
   ],
   "source": [
    "class ImprovedMetaLearner(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dims[0], dropout) for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.BatchNorm1d(hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims[1], embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.classifier = classifier_head(embedding_dim, num_classes, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        embeddings = self.encoder(x)\n",
    "        return embeddings\n",
    "    \n",
    "    def predict(self, x):\n",
    "        embeddings = self.forward(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim, dropout, num_classes):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.encoder = meta_network(input_dim, hidden_dims, embedding_dim, dropout)\n",
    "        self.classifier = classifier_head(embedding_dim, num_classes, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        return embeddings\n",
    "    \n",
    "    def predict(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "model = ImprovedMetaLearner(input_dim, hidden_dims, embedding_dim, dropout, num_classes).to(device)\n",
    "\n",
    "# Compile model for PyTorch 2.0+ speedup (disabled during training to avoid state_dict issues)\n",
    "# Will compile for inference after loading checkpoint\n",
    "compile_model = False\n",
    "if hasattr(torch, 'compile') and compile_model:\n",
    "    try:\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "        print(\"âœ“ Model compiled with torch.compile\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile not available: {e}\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in model.encoder.parameters()):,}\")\n",
    "print(f\"Classifier parameters: {sum(p.numel() for p in model.classifier.parameters()):,}\")\n",
    "\n",
    "QUICK_TEST = False\n",
    "\n",
    "if QUICK_TEST:\n",
    "    meta_lr = 5e-4\n",
    "    inner_lr = 5e-3\n",
    "    inner_steps = 5\n",
    "    meta_batch_size = 4\n",
    "    num_meta_iterations = 50\n",
    "    k_shot_minority = 3\n",
    "    k_shot_majority = 5\n",
    "    q_query = 10\n",
    "    early_stopping_patience = 20\n",
    "    min_delta = 1e-4\n",
    "    print(\"\\nQUICK TEST MODE ENABLED\")\n",
    "else:\n",
    "    meta_lr = OPTIMIZED_CONFIG['meta_lr']\n",
    "    inner_lr = OPTIMIZED_CONFIG['inner_lr']\n",
    "    inner_steps = OPTIMIZED_CONFIG['inner_steps']\n",
    "    meta_batch_size = OPTIMIZED_CONFIG['meta_batch_size']\n",
    "    num_meta_iterations = 1000\n",
    "    k_shot_minority = 5\n",
    "    k_shot_majority = 10\n",
    "    q_query = 15\n",
    "    early_stopping_patience = 100\n",
    "    min_delta = 1e-4\n",
    "\n",
    "meta_optimizer = Adam(model.parameters(), lr=meta_lr)\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "scheduler = CosineAnnealingWarmRestarts(meta_optimizer, T_0=100, T_mult=2, eta_min=1e-5)\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "# Note: GradScaler disabled for meta-training loop to avoid complexity\n",
    "# Mixed precision still used in inner loop via autocast\n",
    "grad_scaler = None\n",
    "\n",
    "# Initialize episode cache\n",
    "episode_cache = EpisodeCache(cache_size=500) if OPTIMIZED_CONFIG['use_cache'] else None\n",
    "\n",
    "print(f\"\\nMeta-Learning Configuration:\")\n",
    "print(f\"  Meta LR: {meta_lr}\")\n",
    "print(f\"  Inner LR: {inner_lr}\")\n",
    "print(f\"  Inner steps: {inner_steps}\")\n",
    "print(f\"  Meta batch size: {meta_batch_size}\")\n",
    "print(f\"  Iterations: {num_meta_iterations}\")\n",
    "print(f\"  K-shot (minority): {k_shot_minority}\")\n",
    "print(f\"  K-shot (majority): {k_shot_majority}\")\n",
    "print(f\"  Query samples: {q_query}\")\n",
    "print(f\"  Early stopping patience: {early_stopping_patience}\")\n",
    "print(f\"  LR scheduler: CosineAnnealingWarmRestarts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5db73542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training sources: 13\n",
      "Meta-train sources: 10\n",
      "Meta-val sources: 3\n",
      "Sources (sorted by imbalance): ['HPC_2k', 'Windows_2k', 'Hadoop_2k', 'Apache_2k', 'Zookeeper_2k', 'Mac_2k', 'Thunderbird_2k', 'BGL_2k', 'Proxifier_2k', 'Linux_2k', 'Android_2k', 'HealthApp_2k', 'Spark_2k']\n",
      "Imbalance ratios: [('HPC_2k', '1.2:1'), ('Windows_2k', '1.5:1'), ('Hadoop_2k', '1.9:1'), ('Apache_2k', '2.5:1'), ('Zookeeper_2k', '2.9:1')]\n",
      "  Android_2k: 2000 samples, imbalance 75.92:1, k-shot 5/10\n",
      "  Apache_2k: 2000 samples, imbalance 2.50:1, k-shot 5/10\n",
      "  BGL_2k: 2000 samples, imbalance 10.90:1, k-shot 5/10\n",
      "  Hadoop_2k: 2000 samples, imbalance 1.90:1, k-shot 5/10\n",
      "  HealthApp_2k: 2000 samples, imbalance 180.82:1, k-shot 2/5\n",
      "  HPC_2k: 2000 samples, imbalance 1.25:1, k-shot 5/10\n",
      "  Linux_2k: 2000 samples, imbalance 20.28:1, k-shot 5/10\n",
      "  Mac_2k: 2000 samples, imbalance 3.62:1, k-shot 5/10\n",
      "  Proxifier_2k: 2000 samples, imbalance 19.62:1, k-shot 5/10\n",
      "  Spark_2k: 2000 samples, imbalance 249.00:1, k-shot 2/4\n",
      "  Thunderbird_2k: 2000 samples, imbalance 9.26:1, k-shot 5/10\n",
      "  Windows_2k: 2000 samples, imbalance 1.53:1, k-shot 5/10\n",
      "  Zookeeper_2k: 2000 samples, imbalance 2.89:1, k-shot 5/10\n",
      "\n",
      "Prepared 13 sources for meta-training\n"
     ]
    }
   ],
   "source": [
    "train_sources = []\n",
    "source_imbalance_ratios = {}\n",
    "\n",
    "for source_name, source_data in data_dict.items():\n",
    "    if source_data['labels'] is not None:\n",
    "        labels = source_data['labels']\n",
    "        if len(np.unique(labels)) >= 2:\n",
    "            train_sources.append(source_name)\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            imb_ratio = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "            source_imbalance_ratios[source_name] = imb_ratio\n",
    "\n",
    "train_sources_sorted = sorted(train_sources, key=lambda x: source_imbalance_ratios[x])\n",
    "\n",
    "meta_train_sources = train_sources_sorted[:int(len(train_sources_sorted) * 0.8)]\n",
    "meta_val_sources = train_sources_sorted[int(len(train_sources_sorted) * 0.8):]\n",
    "\n",
    "print(f\"\\nTraining sources: {len(train_sources)}\")\n",
    "print(f\"Meta-train sources: {len(meta_train_sources)}\")\n",
    "print(f\"Meta-val sources: {len(meta_val_sources)}\")\n",
    "print(f\"Sources (sorted by imbalance): {train_sources_sorted}\")\n",
    "print(f\"Imbalance ratios: {[(s, f'{source_imbalance_ratios[s]:.1f}:1') for s in train_sources_sorted[:5]]}\")\n",
    "\n",
    "\n",
    "feat_variant = 'selected_imbalanced'\n",
    "\n",
    "source_features = {}\n",
    "source_labels = {}\n",
    "source_scalers = {}\n",
    "source_k_shots = {}\n",
    "\n",
    "for source_name in train_sources:\n",
    "    source_data = data_dict[source_name]\n",
    "    if feat_variant in source_data['feature_variants']:\n",
    "        X = source_data['feature_variants'][feat_variant]\n",
    "        y = source_data['labels']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        source_features[source_name] = X_scaled\n",
    "        source_labels[source_name] = y\n",
    "        source_scalers[source_name] = scaler\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        imb_ratio = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "        minority_count = counts.min()\n",
    "        \n",
    "        k_min = max(2, min(k_shot_minority, minority_count // 4))\n",
    "        k_maj = max(k_min, min(k_shot_majority, minority_count // 2))\n",
    "        source_k_shots[source_name] = {'minority': k_min, 'majority': k_maj}\n",
    "        \n",
    "        print(f\"  {source_name}: {len(y)} samples, imbalance {imb_ratio:.2f}:1, k-shot {k_min}/{k_maj}\")\n",
    "\n",
    "print(f\"\\nPrepared {len(source_features)} sources for meta-training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b099a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Meta-Training with speed optimizations...\n",
      "Speed optimizations enabled:\n",
      "  - Mixed Precision (AMP): True\n",
      "  - Episode Caching: True\n",
      "  - Batched Inner Loop: False\n",
      "  - Gradient Accumulation: 1x\n",
      "  - CUDNN Benchmark: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting Meta-Training with speed optimizations...\")\n",
    "\n",
    "best_meta_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "meta_losses = []\n",
    "val_losses = []\n",
    "patience_counter = 0\n",
    "curriculum_phase = 0\n",
    "curriculum_thresholds = [5, 20, 100]\n",
    "adaptive_lr_scheduler = TaskAdaptiveLR(base_lr=inner_lr)\n",
    "gradient_accumulation_steps = OPTIMIZED_CONFIG['gradient_accumulation_steps']\n",
    "\n",
    "print(f\"Speed optimizations enabled:\")\n",
    "print(f\"  - Mixed Precision (AMP): {OPTIMIZED_CONFIG['use_amp'] and torch.cuda.is_available()}\")\n",
    "print(f\"  - Episode Caching: {OPTIMIZED_CONFIG['use_cache']}\")\n",
    "print(f\"  - Batched Inner Loop: {OPTIMIZED_CONFIG['use_batched_inner']}\")\n",
    "print(f\"  - Gradient Accumulation: {gradient_accumulation_steps}x\")\n",
    "print(f\"  - CUDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "\n",
    "def get_curriculum_sources(phase, sources_sorted, imbalance_ratios):\n",
    "    if phase == 0:\n",
    "        return [s for s in sources_sorted if imbalance_ratios[s] <= curriculum_thresholds[0]]\n",
    "    elif phase == 1:\n",
    "        return [s for s in sources_sorted if imbalance_ratios[s] <= curriculum_thresholds[1]]\n",
    "    elif phase == 2:\n",
    "        return [s for s in sources_sorted if imbalance_ratios[s] <= curriculum_thresholds[2]]\n",
    "    else:\n",
    "        return sources_sorted\n",
    "\n",
    "def evaluate_meta_validation(model, val_sources, source_features, source_labels, source_k_shots, q_query):\n",
    "    \"\"\"Evaluate on meta-validation set\"\"\"\n",
    "    model.eval()\n",
    "    val_task_losses = []\n",
    "    \n",
    "    for source_name in val_sources:\n",
    "        if source_name not in source_features:\n",
    "            continue\n",
    "        \n",
    "        X_source = source_features[source_name]\n",
    "        y_source = source_labels[source_name]\n",
    "        k_shots = source_k_shots[source_name]\n",
    "        \n",
    "        episode = create_balanced_imbalanced_episode(X_source, y_source, q_query=q_query)\n",
    "        if episode[0] is None:\n",
    "            continue\n",
    "        \n",
    "        support_X, support_y, query_X, query_y = episode\n",
    "        \n",
    "        # Inner loop needs gradients for adaptation\n",
    "        adapted_model = maml_inner_loop(model, support_X, support_y, inner_lr, inner_steps, focal_loss)\n",
    "        \n",
    "        # Evaluate on query set without gradients\n",
    "        adapted_model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            query_X_tensor = torch.FloatTensor(query_X).to(device)\n",
    "            query_y_tensor = torch.LongTensor(query_y).to(device)\n",
    "            \n",
    "            query_embeddings = adapted_model(query_X_tensor)\n",
    "            query_logits = adapted_model.classifier(query_embeddings)\n",
    "            prototypes = refined_prototypes(query_embeddings, query_y_tensor)\n",
    "            \n",
    "            task_loss = combined_meta_loss(query_embeddings, query_logits, query_y_tensor, prototypes)\n",
    "            val_task_losses.append(task_loss.item())\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(val_task_losses) if val_task_losses else float('inf')\n",
    "\n",
    "curriculum_sources = get_curriculum_sources(curriculum_phase, meta_train_sources, source_imbalance_ratios)\n",
    "if not curriculum_sources:\n",
    "    curriculum_sources = meta_train_sources[:len(meta_train_sources)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14adbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with curriculum phase 0: 6 sources\n",
      "Iter 50/1000 - Loss: 1.1919, LR: 0.000255, Patience: 7/100\n",
      "Iter 100/1000 - Loss: 1.1940, Val: 0.9740, LR: 0.000500, Patience: 0/100\n",
      "Iter 150/1000 - Loss: 1.1941, Val: 0.9740, LR: 0.000428, Patience: 37/100\n",
      "Iter 200/1000 - Loss: 1.1941, Val: 0.9836, LR: 0.000255, Patience: 87/100, Cache hit rate: 15.22% (487/3200)\n",
      "\n",
      "Curriculum phase 1: 9 sources\n",
      "  Cache hit rate: 15.22% (487/3200)\n",
      "\n",
      "Early stopping at iteration 213\n",
      "\n",
      "Meta-training complete. Best meta loss: 1.1812\n",
      "Best validation loss: 0.9740\n",
      "Loaded best meta-model from iteration 112\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting with curriculum phase {curriculum_phase}: {len(curriculum_sources)} sources\")\n",
    "\n",
    "for iteration in range(num_meta_iterations):\n",
    "    if iteration > 0 and iteration % 200 == 0 and curriculum_phase < 3:\n",
    "        curriculum_phase += 1\n",
    "        curriculum_sources = get_curriculum_sources(curriculum_phase, meta_train_sources, source_imbalance_ratios)\n",
    "        if not curriculum_sources:\n",
    "            curriculum_sources = meta_train_sources\n",
    "        print(f\"\\nCurriculum phase {curriculum_phase}: {len(curriculum_sources)} sources\")\n",
    "        \n",
    "        if episode_cache:\n",
    "            print(f\"  {episode_cache.get_stats()}\")\n",
    "    \n",
    "    # Reset optimizer gradients\n",
    "    meta_optimizer.zero_grad()\n",
    "    \n",
    "    # Gradient accumulation loop\n",
    "    all_task_losses = []\n",
    "    total_valid_tasks = 0\n",
    "    \n",
    "    for accum_step in range(gradient_accumulation_steps):\n",
    "        task_losses = []\n",
    "        \n",
    "        for batch_idx in range(meta_batch_size):\n",
    "            available_sources = [s for s in curriculum_sources if s in source_features]\n",
    "            if not available_sources:\n",
    "                available_sources = list(source_features.keys())\n",
    "            \n",
    "            source_name = np.random.choice(available_sources)\n",
    "            X_source = source_features[source_name]\n",
    "            y_source = source_labels[source_name]\n",
    "            k_shots = source_k_shots[source_name]\n",
    "            imb_ratio = source_imbalance_ratios[source_name]\n",
    "            \n",
    "            adaptive_steps = adaptive_inner_steps(imb_ratio, inner_steps)\n",
    "            adaptive_lr = adaptive_lr_scheduler.get_lr(imb_ratio)\n",
    "            \n",
    "            # Use cached episodes if enabled\n",
    "            if episode_cache:\n",
    "                episode = episode_cache.get_episode(\n",
    "                    source_name, X_source, y_source,\n",
    "                    k_shots['minority'], k_shots['majority'], q_query\n",
    "                )\n",
    "            else:\n",
    "                episode = create_balanced_imbalanced_episode(X_source, y_source, q_query=q_query)\n",
    "            \n",
    "            if episode[0] is None:\n",
    "                continue\n",
    "            \n",
    "            support_X, support_y, query_X, query_y = episode\n",
    "            \n",
    "            if OPTIMIZED_CONFIG['augment_minority']:\n",
    "                support_X, support_y = augment_support_set(support_X, support_y, augment_factor=2)\n",
    "            \n",
    "            # Use batched inner loop if enabled\n",
    "            if OPTIMIZED_CONFIG['use_batched_inner']:\n",
    "                adapted_model = batched_maml_inner_loop(model, support_X, support_y, \n",
    "                                                       adaptive_lr, adaptive_steps, focal_loss)\n",
    "            else:\n",
    "                adapted_model = maml_inner_loop(model, support_X, support_y, \n",
    "                                               adaptive_lr, adaptive_steps, focal_loss,\n",
    "                                               use_amp=OPTIMIZED_CONFIG['use_amp'])\n",
    "            \n",
    "            query_X_tensor = torch.FloatTensor(query_X).to(device, non_blocking=True)\n",
    "            query_y_tensor = torch.LongTensor(query_y).to(device, non_blocking=True)\n",
    "            \n",
    "            # Use mixed precision for query\n",
    "            if OPTIMIZED_CONFIG['use_amp'] and torch.cuda.is_available():\n",
    "                device_type = 'cuda' if USE_NEW_AMP_API else None\n",
    "                with autocast(device_type=device_type) if USE_NEW_AMP_API else autocast():\n",
    "                    query_embeddings = adapted_model(query_X_tensor)\n",
    "                    query_logits = adapted_model.classifier(query_embeddings)\n",
    "                    prototypes = refined_prototypes(query_embeddings, query_y_tensor)\n",
    "                    task_loss = combined_meta_loss(query_embeddings, query_logits, query_y_tensor, prototypes)\n",
    "            else:\n",
    "                query_embeddings = adapted_model(query_X_tensor)\n",
    "                query_logits = adapted_model.classifier(query_embeddings)\n",
    "                prototypes = refined_prototypes(query_embeddings, query_y_tensor)\n",
    "                task_loss = combined_meta_loss(query_embeddings, query_logits, query_y_tensor, prototypes)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = task_loss / gradient_accumulation_steps\n",
    "            task_losses.append(scaled_loss)\n",
    "            total_valid_tasks += 1\n",
    "        \n",
    "        if task_losses:\n",
    "            meta_loss = torch.stack(task_losses).mean()\n",
    "            all_task_losses.append(meta_loss)\n",
    "            \n",
    "            # Backward pass (accumulate gradients)\n",
    "            meta_loss.backward()\n",
    "    \n",
    "    # Update after accumulation\n",
    "    if total_valid_tasks > 0 and all_task_losses:\n",
    "        # Compute average loss for logging\n",
    "        current_loss = torch.stack(all_task_losses).mean().item() * gradient_accumulation_steps\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Step optimizer\n",
    "        meta_optimizer.step()\n",
    "        scheduler.step()\n",
    "        meta_losses.append(current_loss)\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0 and meta_val_sources:\n",
    "            val_loss = evaluate_meta_validation(model, meta_val_sources, source_features, \n",
    "                                               source_labels, source_k_shots, q_query)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Clean state dict before saving\n",
    "                save_state_dict = model.state_dict()\n",
    "                if any(key.startswith('_orig_mod.') for key in save_state_dict.keys()):\n",
    "                    save_state_dict = {key.replace('_orig_mod.', ''): value for key, value in save_state_dict.items()}\n",
    "                \n",
    "                torch.save({\n",
    "                    'model': save_state_dict,\n",
    "                    'iteration': iteration,\n",
    "                    'meta_loss': current_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'curriculum_phase': curriculum_phase\n",
    "                }, MODELS_PATH / 'best_meta_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "        elif current_loss < best_meta_loss - min_delta:\n",
    "            best_meta_loss = current_loss\n",
    "            patience_counter = 0\n",
    "            # Clean state dict before saving\n",
    "            save_state_dict = model.state_dict()\n",
    "            if any(key.startswith('_orig_mod.') for key in save_state_dict.keys()):\n",
    "                save_state_dict = {key.replace('_orig_mod.', ''): value for key, value in save_state_dict.items()}\n",
    "            \n",
    "            torch.save({\n",
    "                'model': save_state_dict,\n",
    "                'iteration': iteration,\n",
    "                'meta_loss': best_meta_loss,\n",
    "                'curriculum_phase': curriculum_phase\n",
    "            }, MODELS_PATH / 'best_meta_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (iteration + 1) % 50 == 0:\n",
    "            avg_loss = np.mean(meta_losses[-50:])\n",
    "            current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "            val_info = f\", Val: {val_losses[-1]:.4f}\" if val_losses else \"\"\n",
    "            cache_info = f\", {episode_cache.get_stats()}\" if episode_cache and (iteration + 1) % 200 == 0 else \"\"\n",
    "            print(f\"Iter {iteration + 1}/{num_meta_iterations} - Loss: {avg_loss:.4f}{val_info}, LR: {current_lr:.6f}, Patience: {patience_counter}/{early_stopping_patience}{cache_info}\")\n",
    "        \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if torch.cuda.is_available() and (iteration + 1) % 100 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nMeta-training complete. Best meta loss: {best_meta_loss:.4f}\")\n",
    "if val_losses:\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "checkpoint = torch.load(MODELS_PATH / 'best_meta_model.pt')\n",
    "\n",
    "# Handle torch.compile state dict (removes _orig_mod. prefix)\n",
    "state_dict = checkpoint['model']\n",
    "if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
    "    state_dict = {key.replace('_orig_mod.', ''): value for key, value in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "print(f\"Loaded best meta-model from iteration {checkpoint['iteration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0c7a31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test sources...\n",
      "\n",
      "Android_2k:\n",
      "  F1-Macro: 0.4967\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.2482\n",
      "  MCC: 0.0000\n",
      "\n",
      "Apache_2k:\n",
      "  F1-Macro: 0.4167\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.6217\n",
      "  MCC: 0.0000\n",
      "\n",
      "BGL_2k:\n",
      "  F1-Macro: 0.4781\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.2732\n",
      "  MCC: 0.0000\n",
      "\n",
      "Hadoop_2k:\n",
      "  F1-Macro: 0.2565\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.4747\n",
      "  MCC: 0.0000\n",
      "\n",
      "HealthApp_2k:\n",
      "  F1-Macro: 0.4982\n",
      "  Balanced Acc: 0.4992\n",
      "  AUROC: 0.4245\n",
      "  MCC: -0.0029\n",
      "\n",
      "HPC_2k:\n",
      "  F1-Macro: 0.3569\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.5452\n",
      "  MCC: 0.0000\n",
      "\n",
      "Linux_2k:\n",
      "  F1-Macro: 0.0578\n",
      "  Balanced Acc: 0.5063\n",
      "  AUROC: 0.8692\n",
      "  MCC: 0.0245\n",
      "\n",
      "Mac_2k:\n",
      "  F1-Macro: 0.4393\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.4877\n",
      "  MCC: 0.0000\n",
      "\n",
      "Proxifier_2k:\n",
      "  F1-Macro: 0.5080\n",
      "  Balanced Acc: 0.5103\n",
      "  AUROC: 0.9972\n",
      "  MCC: 0.1401\n",
      "\n",
      "Spark_2k:\n",
      "  F1-Macro: 0.4987\n",
      "  Balanced Acc: 0.4995\n",
      "  AUROC: 0.5059\n",
      "  MCC: -0.0020\n",
      "\n",
      "Thunderbird_2k:\n",
      "  F1-Macro: 0.4744\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.1980\n",
      "  MCC: 0.0000\n",
      "\n",
      "Windows_2k:\n",
      "  F1-Macro: 0.3768\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.9833\n",
      "  MCC: 0.0000\n",
      "\n",
      "Zookeeper_2k:\n",
      "  F1-Macro: 0.4263\n",
      "  Balanced Acc: 0.5000\n",
      "  AUROC: 0.4685\n",
      "  MCC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "META-LEARNING EVALUATION SUMMARY\n",
      "================================================================================\n",
      "   test_source  f1_macro  balanced_acc    auroc       mcc  test_samples  support_samples\n",
      "  Proxifier_2k  0.508026      0.510309 0.997197  0.140136          2000               30\n",
      "      Spark_2k  0.498747      0.499498 0.505899 -0.002005          2000               30\n",
      "  HealthApp_2k  0.498244      0.499246 0.424517 -0.002882          2000               30\n",
      "    Android_2k  0.496729      0.500000 0.248227  0.000000          2000               30\n",
      "        BGL_2k  0.478079      0.500000 0.273208  0.000000          2000               30\n",
      "Thunderbird_2k  0.474376      0.500000 0.198031  0.000000          2000               30\n",
      "        Mac_2k  0.439305      0.500000 0.487683  0.000000          2000               30\n",
      "  Zookeeper_2k  0.426277      0.500000 0.468505  0.000000          2000               30\n",
      "     Apache_2k  0.416740      0.500000 0.621726  0.000000          2000               30\n",
      "    Windows_2k  0.376753      0.500000 0.983325  0.000000          2000               30\n",
      "        HPC_2k  0.356913      0.500000 0.545166  0.000000          2000               30\n",
      "     Hadoop_2k  0.256506      0.500000 0.474747  0.000000          2000               30\n",
      "      Linux_2k  0.057846      0.506296 0.869187  0.024475          2000               30\n",
      "\n",
      "============================================================\n",
      "AGGREGATE STATISTICS\n",
      "============================================================\n",
      "Sources evaluated: 13\n",
      "Average F1-Macro: 0.4065 Â± 0.1272\n",
      "Average Balanced Acc: 0.5012 Â± 0.0033\n",
      "Average AUROC: 0.5460 Â± 0.2614\n",
      "Average MCC: 0.0123 Â± 0.0390\n",
      "\n",
      "Results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\meta_learning_results_20251125_031033.csv\n",
      "Summary saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\meta_learning_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating on test sources...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "test_splits = splits[:3] if QUICK_TEST else splits\n",
    "\n",
    "for split in test_splits:\n",
    "    test_source = split['test_source']\n",
    "    train_sources_split = split['train_sources']\n",
    "    \n",
    "    if test_source not in data_dict:\n",
    "        continue\n",
    "    \n",
    "    test_data = data_dict[test_source]\n",
    "    if test_data['labels'] is None:\n",
    "        continue\n",
    "    \n",
    "    if feat_variant not in test_data['feature_variants']:\n",
    "        continue\n",
    "    \n",
    "    X_test = test_data['feature_variants'][feat_variant]\n",
    "    y_test = test_data['labels']\n",
    "    \n",
    "    if len(np.unique(y_test)) < 2:\n",
    "        continue\n",
    "    \n",
    "    scaler_test = StandardScaler()\n",
    "    X_test_scaled = scaler_test.fit_transform(X_test)\n",
    "    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    for src in train_sources_split:\n",
    "        if src in source_features:\n",
    "            X_train_list.append(source_features[src])\n",
    "            y_train_list.append(source_labels[src])\n",
    "    \n",
    "    if not X_train_list:\n",
    "        continue\n",
    "    \n",
    "    X_train_combined = np.vstack(X_train_list)\n",
    "    y_train_combined = np.concatenate(y_train_list)\n",
    "    \n",
    "    unique, counts = np.unique(y_train_combined, return_counts=True)\n",
    "    minority_count = counts.min()\n",
    "    \n",
    "    k_shot_adapt = min(k_shot_minority, minority_count // 2)\n",
    "    if k_shot_adapt < 2:\n",
    "        continue\n",
    "    \n",
    "    episode = create_imbalanced_episode(\n",
    "        X_train_combined, y_train_combined,\n",
    "        k_shot_adapt, k_shot_adapt * 2, 10\n",
    "    )\n",
    "    \n",
    "    if episode[0] is None:\n",
    "        continue\n",
    "    \n",
    "    support_X, support_y, _, _ = episode\n",
    "    \n",
    "    imb_ratio = source_imbalance_ratios.get(test_source, 10.0)\n",
    "    adaptive_steps = adaptive_inner_steps(imb_ratio, inner_steps)\n",
    "    adaptive_lr = adaptive_lr_scheduler.get_lr(imb_ratio)\n",
    "    \n",
    "    if OPTIMIZED_CONFIG['augment_minority']:\n",
    "        support_X, support_y = augment_support_set(support_X, support_y, augment_factor=2)\n",
    "    \n",
    "    adapted_model = maml_inner_loop(\n",
    "        model, support_X, support_y,\n",
    "        adaptive_lr, adaptive_steps, focal_loss\n",
    "    )\n",
    "    \n",
    "    adapted_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        test_logits = adapted_model.predict(X_test_tensor)\n",
    "        test_probs = F.softmax(test_logits, dim=1).cpu().numpy()\n",
    "        test_preds = torch.argmax(test_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, test_preds, test_probs)\n",
    "    \n",
    "    test_results.append({\n",
    "        'test_source': test_source,\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'balanced_acc': metrics['balanced_acc'],\n",
    "        'auroc': metrics['auroc'],\n",
    "        'mcc': metrics['mcc'],\n",
    "        'test_samples': len(y_test),\n",
    "        'support_samples': len(support_y)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{test_source}:\")\n",
    "    print(f\"  F1-Macro: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {metrics['balanced_acc']:.4f}\")\n",
    "    print(f\"  AUROC: {metrics['auroc']:.4f}\")\n",
    "    print(f\"  MCC: {metrics['mcc']:.4f}\")\n",
    "\n",
    "\n",
    "if test_results:\n",
    "    df_results = pd.DataFrame(test_results)\n",
    "    df_results = df_results.sort_values('f1_macro', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"META-LEARNING EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Sources evaluated: {len(test_results)}\")\n",
    "    print(f\"Average F1-Macro: {df_results['f1_macro'].mean():.4f} Â± {df_results['f1_macro'].std():.4f}\")\n",
    "    print(f\"Average Balanced Acc: {df_results['balanced_acc'].mean():.4f} Â± {df_results['balanced_acc'].std():.4f}\")\n",
    "    print(f\"Average AUROC: {df_results['auroc'].mean():.4f} Â± {df_results['auroc'].std():.4f}\")\n",
    "    print(f\"Average MCC: {df_results['mcc'].mean():.4f} Â± {df_results['mcc'].std():.4f}\")\n",
    "    \n",
    "    results_file = RESULTS_PATH / f\"meta_learning_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_results.to_csv(results_file, index=False)\n",
    "    print(f\"\\nResults saved to: {results_file}\")\n",
    "    \n",
    "    with open(RESULTS_PATH / 'meta_learning_summary.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': test_results,\n",
    "            'meta_losses': meta_losses,\n",
    "            'config': {\n",
    "                'meta_lr': meta_lr,\n",
    "                'inner_lr': inner_lr,\n",
    "                'inner_steps': inner_steps,\n",
    "                'meta_batch_size': meta_batch_size,\n",
    "                'num_meta_iterations': num_meta_iterations,\n",
    "                'k_shot_minority': k_shot_minority,\n",
    "                'k_shot_majority': k_shot_majority,\n",
    "                'q_query': q_query,\n",
    "                'input_dim': input_dim,\n",
    "                'hidden_dims': hidden_dims,\n",
    "                'embedding_dim': embedding_dim\n",
    "            },\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, f)\n",
    "    print(f\"Summary saved to: {RESULTS_PATH / 'meta_learning_summary.pkl'}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo test results generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b762dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prototypical Network Evaluation...\n",
      "\n",
      "Android_2k (Prototypical):\n",
      "  F1-Macro: 0.4056\n",
      "  Balanced Acc: 0.6046\n",
      "  AUROC: 0.6564\n",
      "\n",
      "Apache_2k (Prototypical):\n",
      "  F1-Macro: 0.9793\n",
      "  Balanced Acc: 0.9834\n",
      "  AUROC: 0.9787\n",
      "\n",
      "BGL_2k (Prototypical):\n",
      "  F1-Macro: 0.3397\n",
      "  Balanced Acc: 0.2808\n",
      "  AUROC: 0.1349\n",
      "\n",
      "Hadoop_2k (Prototypical):\n",
      "  F1-Macro: 0.4480\n",
      "  Balanced Acc: 0.5421\n",
      "  AUROC: 0.2930\n",
      "\n",
      "HealthApp_2k (Prototypical):\n",
      "  F1-Macro: 0.4832\n",
      "  Balanced Acc: 0.6629\n",
      "  AUROC: 0.7672\n",
      "\n",
      "HPC_2k (Prototypical):\n",
      "  F1-Macro: 0.2215\n",
      "  Balanced Acc: 0.2552\n",
      "  AUROC: 0.1528\n",
      "\n",
      "Linux_2k (Prototypical):\n",
      "  F1-Macro: 0.2813\n",
      "  Balanced Acc: 0.5841\n",
      "  AUROC: 0.7102\n",
      "\n",
      "Mac_2k (Prototypical):\n",
      "  F1-Macro: 0.4730\n",
      "  Balanced Acc: 0.5350\n",
      "  AUROC: 0.5774\n",
      "\n",
      "Proxifier_2k (Prototypical):\n",
      "  F1-Macro: 0.5653\n",
      "  Balanced Acc: 0.8361\n",
      "  AUROC: 0.7984\n",
      "\n",
      "Spark_2k (Prototypical):\n",
      "  F1-Macro: 0.4116\n",
      "  Balanced Acc: 0.8331\n",
      "  AUROC: 0.9584\n",
      "\n",
      "Thunderbird_2k (Prototypical):\n",
      "  F1-Macro: 0.6657\n",
      "  Balanced Acc: 0.7987\n",
      "  AUROC: 0.8642\n",
      "\n",
      "Windows_2k (Prototypical):\n",
      "  F1-Macro: 0.3789\n",
      "  Balanced Acc: 0.4936\n",
      "  AUROC: 0.3270\n",
      "\n",
      "Zookeeper_2k (Prototypical):\n",
      "  F1-Macro: 0.7731\n",
      "  Balanced Acc: 0.8094\n",
      "  AUROC: 0.9143\n",
      "\n",
      "================================================================================\n",
      "PROTOTYPICAL NETWORK EVALUATION SUMMARY\n",
      "================================================================================\n",
      "   test_source  f1_macro  balanced_acc    auroc       mcc  test_samples  prototypes\n",
      "     Apache_2k  0.979338      0.983372 0.978678  0.958860          2000           2\n",
      "  Zookeeper_2k  0.773073      0.809374 0.914297  0.565582          2000           2\n",
      "Thunderbird_2k  0.665700      0.798679 0.864192  0.411723          2000           2\n",
      "  Proxifier_2k  0.565335      0.836062 0.798446  0.322280          2000           2\n",
      "  HealthApp_2k  0.483243      0.662919 0.767174  0.071539          2000           2\n",
      "        Mac_2k  0.473000      0.534968 0.577356  0.057709          2000           2\n",
      "     Hadoop_2k  0.447951      0.542129 0.292966  0.094888          2000           2\n",
      "      Spark_2k  0.411567      0.833082 0.958396  0.088987          2000           2\n",
      "    Android_2k  0.405576      0.604571 0.656399  0.049055          2000           2\n",
      "    Windows_2k  0.378944      0.493649 0.327028 -0.052865          2000           2\n",
      "        BGL_2k  0.339716      0.280841 0.134910 -0.248029          2000           2\n",
      "      Linux_2k  0.281255      0.584124 0.710226  0.078634          2000           2\n",
      "        HPC_2k  0.221531      0.255178 0.152813 -0.545587          2000           2\n",
      "\n",
      "============================================================\n",
      "PROTOTYPICAL AGGREGATE STATISTICS\n",
      "============================================================\n",
      "Sources evaluated: 13\n",
      "Average F1-Macro: 0.4943 Â± 0.2085\n",
      "Average Balanced Acc: 0.6322 Â± 0.2181\n",
      "Average AUROC: 0.6256 Â± 0.3028\n",
      "\n",
      "Prototypical results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\prototypical_results_20251125_031034.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrototypical Network Evaluation...\")\n",
    "\n",
    "prototypical_results = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "proto_splits = splits[:3] if QUICK_TEST else splits\n",
    "\n",
    "for split in proto_splits:\n",
    "    test_source = split['test_source']\n",
    "    \n",
    "    if test_source not in data_dict:\n",
    "        continue\n",
    "    \n",
    "    test_data = data_dict[test_source]\n",
    "    if test_data['labels'] is None or feat_variant not in test_data['feature_variants']:\n",
    "        continue\n",
    "    \n",
    "    X_test = test_data['feature_variants'][feat_variant]\n",
    "    y_test = test_data['labels']\n",
    "    \n",
    "    if len(np.unique(y_test)) < 2:\n",
    "        continue\n",
    "    \n",
    "    scaler_test = StandardScaler()\n",
    "    X_test_scaled = scaler_test.fit_transform(X_test)\n",
    "    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    for src in split['train_sources']:\n",
    "        if src in source_features:\n",
    "            X_train_list.append(source_features[src])\n",
    "            y_train_list.append(source_labels[src])\n",
    "    \n",
    "    if not X_train_list:\n",
    "        continue\n",
    "    \n",
    "    X_train_combined = np.vstack(X_train_list)\n",
    "    y_train_combined = np.concatenate(y_train_list)\n",
    "    \n",
    "    unique, counts = np.unique(y_train_combined, return_counts=True)\n",
    "    minority_count = counts.min()\n",
    "    k_proto = min(20, minority_count // 2)\n",
    "    \n",
    "    if k_proto < 5:\n",
    "        continue\n",
    "    \n",
    "    episode = create_imbalanced_episode(\n",
    "        X_train_combined, y_train_combined,\n",
    "        k_proto, k_proto * 2, 0\n",
    "    )\n",
    "    \n",
    "    if episode[0] is None:\n",
    "        continue\n",
    "    \n",
    "    support_X, support_y, _, _ = episode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_X_tensor = torch.FloatTensor(support_X).to(device)\n",
    "        support_y_tensor = torch.LongTensor(support_y).to(device)\n",
    "        support_embeddings = model(support_X_tensor)\n",
    "        \n",
    "        prototypes, proto_classes = compute_prototypes(support_embeddings, support_y_tensor)\n",
    "        \n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        test_embeddings = model(X_test_tensor)\n",
    "        \n",
    "        distances = torch.cdist(test_embeddings, prototypes, p=2)\n",
    "        test_preds = torch.argmin(distances, dim=1).cpu().numpy()\n",
    "        \n",
    "        probs = F.softmax(-distances, dim=1).cpu().numpy()\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, test_preds, probs)\n",
    "    \n",
    "    prototypical_results.append({\n",
    "        'test_source': test_source,\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'balanced_acc': metrics['balanced_acc'],\n",
    "        'auroc': metrics['auroc'],\n",
    "        'mcc': metrics['mcc'],\n",
    "        'test_samples': len(y_test),\n",
    "        'prototypes': len(prototypes)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{test_source} (Prototypical):\")\n",
    "    print(f\"  F1-Macro: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {metrics['balanced_acc']:.4f}\")\n",
    "    print(f\"  AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "\n",
    "if prototypical_results:\n",
    "    df_proto = pd.DataFrame(prototypical_results)\n",
    "    df_proto = df_proto.sort_values('f1_macro', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROTOTYPICAL NETWORK EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_proto.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROTOTYPICAL AGGREGATE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Sources evaluated: {len(prototypical_results)}\")\n",
    "    print(f\"Average F1-Macro: {df_proto['f1_macro'].mean():.4f} Â± {df_proto['f1_macro'].std():.4f}\")\n",
    "    print(f\"Average Balanced Acc: {df_proto['balanced_acc'].mean():.4f} Â± {df_proto['balanced_acc'].std():.4f}\")\n",
    "    print(f\"Average AUROC: {df_proto['auroc'].mean():.4f} Â± {df_proto['auroc'].std():.4f}\")\n",
    "    \n",
    "    proto_file = RESULTS_PATH / f\"prototypical_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_proto.to_csv(proto_file, index=False)\n",
    "    print(f\"\\nPrototypical results saved to: {proto_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1220a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Transfer Learning Evaluation...\n",
      "\n",
      "Apache_2k (Transfer):\n",
      "  F1-Macro: 0.9913\n",
      "  Balanced Acc: 0.9950\n",
      "  AUROC: 1.0000\n",
      "\n",
      "BGL_2k (Transfer):\n",
      "  F1-Macro: 0.9819\n",
      "  Balanced Acc: 0.9930\n",
      "  AUROC: 0.9991\n",
      "\n",
      "Hadoop_2k (Transfer):\n",
      "  F1-Macro: 0.9627\n",
      "  Balanced Acc: 0.9602\n",
      "  AUROC: 0.9965\n",
      "\n",
      "HPC_2k (Transfer):\n",
      "  F1-Macro: 0.9870\n",
      "  Balanced Acc: 0.9876\n",
      "  AUROC: 0.9994\n",
      "\n",
      "Linux_2k (Transfer):\n",
      "  F1-Macro: 0.9494\n",
      "  Balanced Acc: 0.9948\n",
      "  AUROC: 0.9998\n",
      "\n",
      "Mac_2k (Transfer):\n",
      "  F1-Macro: 0.9283\n",
      "  Balanced Acc: 0.9217\n",
      "  AUROC: 0.9784\n",
      "\n",
      "Proxifier_2k (Transfer):\n",
      "  F1-Macro: 1.0000\n",
      "  Balanced Acc: 1.0000\n",
      "  AUROC: 1.0000\n",
      "\n",
      "Thunderbird_2k (Transfer):\n",
      "  F1-Macro: 0.9822\n",
      "  Balanced Acc: 0.9964\n",
      "  AUROC: 0.9999\n",
      "\n",
      "Windows_2k (Transfer):\n",
      "  F1-Macro: 0.9859\n",
      "  Balanced Acc: 0.9872\n",
      "  AUROC: 0.9997\n",
      "\n",
      "Zookeeper_2k (Transfer):\n",
      "  F1-Macro: 0.9376\n",
      "  Balanced Acc: 0.9202\n",
      "  AUROC: 0.9825\n",
      "\n",
      "================================================================================\n",
      "TRANSFER LEARNING EVALUATION SUMMARY\n",
      "================================================================================\n",
      "   test_source  f1_macro  balanced_acc    auroc      mcc  train_samples  test_samples\n",
      "  Proxifier_2k  1.000000      1.000000 1.000000 1.000000             30          1400\n",
      "     Apache_2k  0.991315      0.995000 1.000000 0.982779             30          1400\n",
      "        HPC_2k  0.987001      0.987622 0.999370 0.974069             30          1400\n",
      "    Windows_2k  0.985850      0.987213 0.999746 0.971789             30          1400\n",
      "Thunderbird_2k  0.982199      0.996440 0.999895 0.965014             30          1400\n",
      "        BGL_2k  0.981907      0.993033 0.999081 0.964172             30          1400\n",
      "     Hadoop_2k  0.962656      0.960165 0.996467 0.925458             30          1400\n",
      "      Linux_2k  0.949417      0.994753 0.999784 0.903516             30          1400\n",
      "  Zookeeper_2k  0.937597      0.920192 0.982489 0.878593             30          1400\n",
      "        Mac_2k  0.928329      0.921687 0.978387 0.857007             30          1400\n",
      "\n",
      "============================================================\n",
      "TRANSFER AGGREGATE STATISTICS\n",
      "============================================================\n",
      "Sources evaluated: 10\n",
      "Average F1-Macro: 0.9706 Â± 0.0246\n",
      "Average Balanced Acc: 0.9756 Â± 0.0309\n",
      "Average AUROC: 0.9955 Â± 0.0081\n",
      "\n",
      "Transfer results saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\transfer_results_20251125_031040.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFew-Shot Transfer Learning Evaluation...\")\n",
    "\n",
    "transfer_results = []\n",
    "\n",
    "transfer_splits = splits[:3] if QUICK_TEST else splits\n",
    "\n",
    "for split in transfer_splits:\n",
    "    test_source = split['test_source']\n",
    "    \n",
    "    if test_source not in data_dict:\n",
    "        continue\n",
    "    \n",
    "    test_data = data_dict[test_source]\n",
    "    if test_data['labels'] is None or feat_variant not in test_data['feature_variants']:\n",
    "        continue\n",
    "    \n",
    "    X_test_full = test_data['feature_variants'][feat_variant]\n",
    "    y_test_full = test_data['labels']\n",
    "    \n",
    "    if len(np.unique(y_test_full)) < 2:\n",
    "        continue\n",
    "    \n",
    "    scaler_transfer = StandardScaler()\n",
    "    X_test_scaled = scaler_transfer.fit_transform(X_test_full)\n",
    "    \n",
    "    unique, counts = np.unique(y_test_full, return_counts=True)\n",
    "    minority_count = counts.min()\n",
    "    k_transfer = min(10, minority_count // 3)\n",
    "    \n",
    "    if k_transfer < 3:\n",
    "        continue\n",
    "    \n",
    "    X_train_transfer, X_test_transfer, y_train_transfer, y_test_transfer = train_test_split(\n",
    "        X_test_scaled, y_test_full, test_size=0.7, random_state=SEED, stratify=y_test_full\n",
    "    )\n",
    "    \n",
    "    episode = create_imbalanced_episode(\n",
    "        X_train_transfer, y_train_transfer,\n",
    "        k_transfer, k_transfer * 2, 5\n",
    "    )\n",
    "    \n",
    "    if episode[0] is None:\n",
    "        continue\n",
    "    \n",
    "    support_X, support_y, _, _ = episode\n",
    "    \n",
    "    transfer_model = ImprovedMetaLearner(input_dim, hidden_dims, embedding_dim, dropout, num_classes).to(device)\n",
    "    \n",
    "    # Handle torch.compile state dict\n",
    "    state_dict = model.state_dict()\n",
    "    if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
    "        state_dict = {key.replace('_orig_mod.', ''): value for key, value in state_dict.items()}\n",
    "    \n",
    "    transfer_model.load_state_dict(state_dict)\n",
    "    \n",
    "    transfer_optimizer = Adam(transfer_model.parameters(), lr=1e-3)\n",
    "    \n",
    "    support_X_tensor = torch.FloatTensor(support_X).to(device)\n",
    "    support_y_tensor = torch.LongTensor(support_y).to(device)\n",
    "    \n",
    "    transfer_model.train()\n",
    "    for epoch in range(50):\n",
    "        transfer_optimizer.zero_grad()\n",
    "        logits = transfer_model.predict(support_X_tensor)\n",
    "        loss = focal_loss(logits, support_y_tensor)\n",
    "        loss.backward()\n",
    "        transfer_optimizer.step()\n",
    "    \n",
    "    transfer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test_transfer).to(device)\n",
    "        test_logits = transfer_model.predict(X_test_tensor)\n",
    "        test_probs = F.softmax(test_logits, dim=1).cpu().numpy()\n",
    "        test_preds = torch.argmax(test_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    metrics = calculate_metrics(y_test_transfer, test_preds, test_probs)\n",
    "    \n",
    "    transfer_results.append({\n",
    "        'test_source': test_source,\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'balanced_acc': metrics['balanced_acc'],\n",
    "        'auroc': metrics['auroc'],\n",
    "        'mcc': metrics['mcc'],\n",
    "        'train_samples': len(support_y),\n",
    "        'test_samples': len(y_test_transfer)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{test_source} (Transfer):\")\n",
    "    print(f\"  F1-Macro: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {metrics['balanced_acc']:.4f}\")\n",
    "    print(f\"  AUROC: {metrics['auroc']:.4f}\")\n",
    "    \n",
    "    del transfer_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if transfer_results:\n",
    "    df_transfer = pd.DataFrame(transfer_results)\n",
    "    df_transfer = df_transfer.sort_values('f1_macro', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSFER LEARNING EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_transfer.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSFER AGGREGATE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Sources evaluated: {len(transfer_results)}\")\n",
    "    print(f\"Average F1-Macro: {df_transfer['f1_macro'].mean():.4f} Â± {df_transfer['f1_macro'].std():.4f}\")\n",
    "    print(f\"Average Balanced Acc: {df_transfer['balanced_acc'].mean():.4f} Â± {df_transfer['balanced_acc'].std():.4f}\")\n",
    "    print(f\"Average AUROC: {df_transfer['auroc'].mean():.4f} Â± {df_transfer['auroc'].std():.4f}\")\n",
    "    \n",
    "    transfer_file = RESULTS_PATH / f\"transfer_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_transfer.to_csv(transfer_file, index=False)\n",
    "    print(f\"\\nTransfer results saved to: {transfer_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b14c1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Source  MAML F1  Proto F1  Transfer F1 Best Method\n",
      "  Proxifier_2k 0.508026  0.565335     1.000000    Transfer\n",
      "    Windows_2k 0.376753  0.378944     0.985850    Transfer\n",
      "  Zookeeper_2k 0.426277  0.773073     0.937597    Transfer\n",
      "        HPC_2k 0.356913  0.221531     0.987001    Transfer\n",
      "     Apache_2k 0.416740  0.979338     0.991315    Transfer\n",
      "        BGL_2k 0.478079  0.339716     0.981907    Transfer\n",
      "        Mac_2k 0.439305  0.473000     0.928329    Transfer\n",
      "     Hadoop_2k 0.256506  0.447951     0.962656    Transfer\n",
      "Thunderbird_2k 0.474376  0.665700     0.982199    Transfer\n",
      "      Linux_2k 0.057846  0.281255     0.949417    Transfer\n",
      "\n",
      "============================================================\n",
      "METHOD COMPARISON\n",
      "============================================================\n",
      "MAML Average F1: 0.3791\n",
      "Prototypical Average F1: 0.5126\n",
      "Transfer Average F1: 0.9706\n",
      "\n",
      "Best method frequency:\n",
      "  Transfer: 10 times (100.0%)\n",
      "\n",
      "Comparison saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\method_comparison_20251125_031040.csv\n"
     ]
    }
   ],
   "source": [
    "if test_results and prototypical_results and transfer_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    sources_all = set([r['test_source'] for r in test_results])\n",
    "    sources_proto = set([r['test_source'] for r in prototypical_results])\n",
    "    sources_transfer = set([r['test_source'] for r in transfer_results])\n",
    "    common_sources = sources_all & sources_proto & sources_transfer\n",
    "    \n",
    "    for source in common_sources:\n",
    "        maml_result = next((r for r in test_results if r['test_source'] == source), None)\n",
    "        proto_result = next((r for r in prototypical_results if r['test_source'] == source), None)\n",
    "        transfer_result = next((r for r in transfer_results if r['test_source'] == source), None)\n",
    "        \n",
    "        if maml_result and proto_result and transfer_result:\n",
    "            comparison_data.append({\n",
    "                'Source': source,\n",
    "                'MAML F1': maml_result['f1_macro'],\n",
    "                'Proto F1': proto_result['f1_macro'],\n",
    "                'Transfer F1': transfer_result['f1_macro'],\n",
    "                'Best Method': max([\n",
    "                    ('MAML', maml_result['f1_macro']),\n",
    "                    ('Proto', proto_result['f1_macro']),\n",
    "                    ('Transfer', transfer_result['f1_macro'])\n",
    "                ], key=lambda x: x[1])[0]\n",
    "            })\n",
    "    \n",
    "    if comparison_data:\n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n\" + df_comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"METHOD COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"MAML Average F1: {df_comparison['MAML F1'].mean():.4f}\")\n",
    "        print(f\"Prototypical Average F1: {df_comparison['Proto F1'].mean():.4f}\")\n",
    "        print(f\"Transfer Average F1: {df_comparison['Transfer F1'].mean():.4f}\")\n",
    "        \n",
    "        best_counts = df_comparison['Best Method'].value_counts()\n",
    "        print(f\"\\nBest method frequency:\")\n",
    "        for method, count in best_counts.items():\n",
    "            print(f\"  {method}: {count} times ({count/len(df_comparison)*100:.1f}%)\")\n",
    "        \n",
    "        comparison_file = RESULTS_PATH / f\"method_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df_comparison.to_csv(comparison_file, index=False)\n",
    "        print(f\"\\nComparison saved to: {comparison_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef75ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\models\\meta_learning\\final_meta_model.pt\n"
     ]
    }
   ],
   "source": [
    "final_state_dict = model.state_dict()\n",
    "if any(key.startswith('_orig_mod.') for key in final_state_dict.keys()):\n",
    "    final_state_dict = {key.replace('_orig_mod.', ''): value for key, value in final_state_dict.items()}\n",
    "\n",
    "torch.save({\n",
    "    'model': final_state_dict,\n",
    "    'config': {\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'dropout': dropout,\n",
    "        'num_classes': num_classes\n",
    "    },\n",
    "    'meta_config': {\n",
    "        'meta_lr': meta_lr,\n",
    "        'inner_lr': inner_lr,\n",
    "        'inner_steps': inner_steps,\n",
    "        'meta_batch_size': meta_batch_size,\n",
    "        'num_meta_iterations': num_meta_iterations,\n",
    "        'early_stopping_patience': early_stopping_patience,\n",
    "        'curriculum_learning': True\n",
    "    },\n",
    "    'training_info': {\n",
    "        'final_iteration': len(meta_losses),\n",
    "        'best_meta_loss': best_meta_loss,\n",
    "        'curriculum_phases': curriculum_phase + 1\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}, MODELS_PATH / 'final_meta_model.pt')\n",
    "\n",
    "print(f\"\\nFinal model saved to: {MODELS_PATH / 'final_meta_model.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e95efb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to: C:\\Computer Science\\AIMLDL\\log-anomaly-detection\\results\\meta_learning\\meta_learning_visualization.png\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(meta_losses)\n",
    "    axes[0, 0].set_title('Meta-Training Loss')\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    if test_results:\n",
    "        sources = [r['test_source'] for r in test_results]\n",
    "        f1_scores = [r['f1_macro'] for r in test_results]\n",
    "        axes[0, 1].barh(sources, f1_scores)\n",
    "        axes[0, 1].set_title('F1-Macro by Source (MAML)')\n",
    "        axes[0, 1].set_xlabel('F1-Macro')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    if comparison_data:\n",
    "        methods = ['MAML', 'Proto', 'Transfer']\n",
    "        avg_f1s = [\n",
    "            df_comparison['MAML F1'].mean(),\n",
    "            df_comparison['Proto F1'].mean(),\n",
    "            df_comparison['Transfer F1'].mean()\n",
    "        ]\n",
    "        axes[1, 0].bar(methods, avg_f1s)\n",
    "        axes[1, 0].set_title('Average F1-Macro by Method')\n",
    "        axes[1, 0].set_ylabel('F1-Macro')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    window = 50\n",
    "    if len(meta_losses) > window:\n",
    "        smoothed = np.convolve(meta_losses, np.ones(window)/window, mode='valid')\n",
    "        axes[1, 1].plot(smoothed)\n",
    "        axes[1, 1].set_title(f'Smoothed Meta-Training Loss (window={window})')\n",
    "        axes[1, 1].set_xlabel('Iteration')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'meta_learning_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to: {RESULTS_PATH / 'meta_learning_visualization.png'}\")\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization skipped: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
